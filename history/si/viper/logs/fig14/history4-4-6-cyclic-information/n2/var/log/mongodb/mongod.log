2020-05-08T12:15:36.243-0700 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2020-05-08T12:15:36.261-0700 W  ASIO     [main] No TransportLayer configured during NetworkInterface startup
2020-05-08T12:15:36.261-0700 I  CONTROL  [initandlisten] MongoDB starting : pid=652343 port=27019 dbpath=/var/lib/mongodb 64-bit host=n2
2020-05-08T12:15:36.261-0700 I  CONTROL  [initandlisten] db version v4.2.6
2020-05-08T12:15:36.261-0700 I  CONTROL  [initandlisten] git version: 20364840b8f1af16917e4c23c1b5f5efd8b352f8
2020-05-08T12:15:36.261-0700 I  CONTROL  [initandlisten] OpenSSL version: OpenSSL 1.1.0l  10 Sep 2019
2020-05-08T12:15:36.261-0700 I  CONTROL  [initandlisten] allocator: tcmalloc
2020-05-08T12:15:36.261-0700 I  CONTROL  [initandlisten] modules: none
2020-05-08T12:15:36.261-0700 I  CONTROL  [initandlisten] build environment:
2020-05-08T12:15:36.261-0700 I  CONTROL  [initandlisten]     distmod: debian92
2020-05-08T12:15:36.261-0700 I  CONTROL  [initandlisten]     distarch: x86_64
2020-05-08T12:15:36.261-0700 I  CONTROL  [initandlisten]     target_arch: x86_64
2020-05-08T12:15:36.261-0700 I  CONTROL  [initandlisten] options: { config: "/etc/mongod.conf", net: { bindIp: "0.0.0.0" }, processManagement: { timeZoneInfo: "/usr/share/zoneinfo" }, replication: { replSetName: "rs_config" }, sharding: { clusterRole: "configsvr" }, storage: { dbPath: "/var/lib/mongodb", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "/var/log/mongodb/mongod.log" } }
2020-05-08T12:15:36.262-0700 I  STORAGE  [initandlisten] 
2020-05-08T12:15:36.262-0700 I  STORAGE  [initandlisten] ** WARNING: Using the XFS filesystem is strongly recommended with the WiredTiger storage engine
2020-05-08T12:15:36.262-0700 I  STORAGE  [initandlisten] **          See http://dochub.mongodb.org/core/prodnotes-filesystem
2020-05-08T12:15:36.262-0700 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=63957M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000,close_scan_interval=10,close_handle_minimum=250),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2020-05-08T12:15:37.010-0700 I  STORAGE  [initandlisten] WiredTiger message [1588965337:10874][652343:0x7fd9ff2f1140], txn-recover: Set global recovery timestamp: (0, 0)
2020-05-08T12:15:37.071-0700 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
2020-05-08T12:15:37.116-0700 I  STORAGE  [initandlisten] Timestamp monitor starting
2020-05-08T12:15:37.140-0700 I  CONTROL  [initandlisten] 
2020-05-08T12:15:37.140-0700 I  CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2020-05-08T12:15:37.140-0700 I  CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2020-05-08T12:15:37.140-0700 I  CONTROL  [initandlisten] 
2020-05-08T12:15:37.141-0700 I  CONTROL  [initandlisten] 
2020-05-08T12:15:37.141-0700 I  CONTROL  [initandlisten] ** WARNING: You are running on a NUMA machine.
2020-05-08T12:15:37.141-0700 I  CONTROL  [initandlisten] **          We suggest launching mongod like this to avoid performance problems:
2020-05-08T12:15:37.141-0700 I  CONTROL  [initandlisten] **              numactl --interleave=all mongod [other options]
2020-05-08T12:15:37.142-0700 I  CONTROL  [initandlisten] 
2020-05-08T12:15:37.142-0700 I  CONTROL  [initandlisten] ** WARNING: /sys/kernel/mm/transparent_hugepage/enabled is 'always'.
2020-05-08T12:15:37.142-0700 I  CONTROL  [initandlisten] **        We suggest setting it to 'never'
2020-05-08T12:15:37.142-0700 I  CONTROL  [initandlisten] 
2020-05-08T12:15:37.143-0700 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2020-05-08T12:15:37.143-0700 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2020-05-08T12:15:37.143-0700 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2020-05-08T12:15:37.144-0700 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2020-05-08T12:15:37.144-0700 I  STORAGE  [initandlisten] createCollection: local.startup_log with generated UUID: d087ee7b-7eb4-439d-9ea3-5a929d717852 and options: { capped: true, size: 10485760 }
2020-05-08T12:15:37.195-0700 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.startup_log
2020-05-08T12:15:37.196-0700 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2020-05-08T12:15:37.196-0700 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/var/lib/mongodb/diagnostic.data'
2020-05-08T12:15:37.201-0700 I  SHARDING [thread1] creating distributed lock ping thread for process ConfigServer (sleeping for 30000ms)
2020-05-08T12:15:37.201-0700 I  SHARDING [shard-registry-reload] Periodic reload of shard registry failed  :: caused by :: ReadConcernMajorityNotAvailableYet: could not get updated shard list from config server :: caused by :: Read concern majority reads are currently not possible.; will retry after 30s
2020-05-08T12:15:37.202-0700 I  STORAGE  [initandlisten] createCollection: local.replset.oplogTruncateAfterPoint with generated UUID: 11c2af40-d351-4c7f-b2be-050693464e37 and options: {}
2020-05-08T12:15:37.252-0700 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.oplogTruncateAfterPoint
2020-05-08T12:15:37.252-0700 I  STORAGE  [initandlisten] createCollection: local.replset.minvalid with generated UUID: 30f5de36-6214-41b0-9782-9ab682f6e059 and options: {}
2020-05-08T12:15:37.308-0700 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.minvalid
2020-05-08T12:15:37.309-0700 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2020-05-08T12:15:37.309-0700 I  STORAGE  [initandlisten] createCollection: local.replset.election with generated UUID: 795e65d3-7d46-46aa-851a-ca044aefbbe0 and options: {}
2020-05-08T12:15:37.367-0700 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.election
2020-05-08T12:15:37.367-0700 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2020-05-08T12:15:37.368-0700 I  REPL     [initandlisten] Did not find local initialized voted for document at startup.
2020-05-08T12:15:37.368-0700 I  REPL     [initandlisten] Did not find local Rollback ID document at startup. Creating one.
2020-05-08T12:15:37.368-0700 I  STORAGE  [initandlisten] createCollection: local.system.rollback.id with generated UUID: 06d5900b-0070-482d-a9f2-4385a92d0f53 and options: {}
2020-05-08T12:15:37.421-0700 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.system.rollback.id
2020-05-08T12:15:37.422-0700 I  SHARDING [initandlisten] Marking collection local.system.rollback.id as collection version: <unsharded>
2020-05-08T12:15:37.422-0700 I  REPL     [initandlisten] Initialized the rollback ID to 1
2020-05-08T12:15:37.422-0700 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
2020-05-08T12:15:37.424-0700 I  SH_REFR  [ConfigServerCatalogCacheLoader-0] Refresh for database config from version {} to version { uuid: UUID("381222db-14fb-454d-98e7-8e3234f12462"), lastMod: 0 } took 0 ms
2020-05-08T12:15:37.424-0700 I  NETWORK  [listener] Listening on /tmp/mongodb-27019.sock
2020-05-08T12:15:37.424-0700 I  CONTROL  [LogicalSessionCacheRefresh] Failed to create config.system.sessions: Cannot create config.system.sessions until there are shards, will try again at the next refresh interval
2020-05-08T12:15:37.424-0700 I  NETWORK  [listener] Listening on 0.0.0.0
2020-05-08T12:15:37.424-0700 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Cannot create config.system.sessions until there are shards
2020-05-08T12:15:37.424-0700 I  NETWORK  [listener] waiting for connections on port 27019
2020-05-08T12:15:37.424-0700 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: Cannot use non-local read concern until replica set is finished initializing.
2020-05-08T12:15:38.000-0700 I  SHARDING [ftdc] Marking collection local.oplog.rs as collection version: <unsharded>
2020-05-08T12:15:38.294-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:51426 #1 (1 connection now open)
2020-05-08T12:15:38.295-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:51430 #2 (2 connections now open)
2020-05-08T12:15:38.295-0700 I  NETWORK  [conn1] received client metadata from 192.168.122.1:51426 conn1: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:15:38.295-0700 I  NETWORK  [conn2] received client metadata from 192.168.122.1:51430 conn2: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:15:38.299-0700 I  NETWORK  [conn1] end connection 192.168.122.1:51426 (1 connection now open)
2020-05-08T12:15:38.300-0700 I  NETWORK  [conn2] end connection 192.168.122.1:51430 (0 connections now open)
2020-05-08T12:15:38.333-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:53748 #3 (1 connection now open)
2020-05-08T12:15:38.334-0700 I  NETWORK  [conn3] end connection 192.168.122.11:53748 (0 connections now open)
2020-05-08T12:15:38.337-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:53752 #4 (1 connection now open)
2020-05-08T12:15:38.337-0700 I  NETWORK  [conn4] received client metadata from 192.168.122.11:53752 conn4: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:38.338-0700 I  CONNPOOL [Replication] Connecting to n1:27019
2020-05-08T12:15:38.877-0700 I  STORAGE  [replexec-0] createCollection: local.system.replset with generated UUID: 2a0c9d7f-8c77-4878-ad95-2b1b438af47b and options: {}
2020-05-08T12:15:38.878-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:43300 #8 (2 connections now open)
2020-05-08T12:15:38.878-0700 I  NETWORK  [conn8] end connection 192.168.122.13:43300 (1 connection now open)
2020-05-08T12:15:38.937-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:43308 #9 (2 connections now open)
2020-05-08T12:15:38.938-0700 I  NETWORK  [conn9] received client metadata from 192.168.122.13:43308 conn9: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:38.938-0700 I  CONNPOOL [Replication] Connecting to n3:27019
2020-05-08T12:15:38.940-0700 I  INDEX    [replexec-0] index build: done building index _id_ on ns local.system.replset
2020-05-08T12:15:38.941-0700 I  REPL     [replexec-0] New replica set config in use: { _id: "rs_config", version: 1, configsvr: true, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "n1:27019", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 3.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "n2:27019", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 2.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: "n3:27019", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 1, electionTimeoutMillis: 1000, catchUpTimeoutMillis: 1000, catchUpTakeoverDelayMillis: 3000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5eb5afdaa0224cfb413c716c') } }
2020-05-08T12:15:38.941-0700 I  REPL     [replexec-0] This node is n2:27019 in the config
2020-05-08T12:15:38.941-0700 I  REPL     [replexec-0] transition to STARTUP2 from STARTUP
2020-05-08T12:15:38.941-0700 I  REPL     [replexec-0] Starting replication storage threads
2020-05-08T12:15:38.941-0700 I  REPL     [replexec-1] Member n3:27019 is now in state STARTUP2
2020-05-08T12:15:38.942-0700 I  REPL     [replexec-2] Member n1:27019 is now in state SECONDARY
2020-05-08T12:15:38.948-0700 I  STORAGE  [replexec-0] createCollection: local.temp_oplog_buffer with generated UUID: 6ff781c1-dbdf-491e-b216-d935c6e30643 and options: { temp: true }
2020-05-08T12:15:39.006-0700 I  INDEX    [replexec-0] index build: done building index _id_ on ns local.temp_oplog_buffer
2020-05-08T12:15:39.006-0700 I  INITSYNC [replication-0] Starting initial sync (attempt 1 of 10)
2020-05-08T12:15:39.006-0700 I  STORAGE  [replication-0] Finishing collection drop for local.temp_oplog_buffer (6ff781c1-dbdf-491e-b216-d935c6e30643).
2020-05-08T12:15:39.015-0700 I  STORAGE  [replication-0] createCollection: local.temp_oplog_buffer with generated UUID: 329b170f-3973-459a-a084-df56a688bee1 and options: { temp: true }
2020-05-08T12:15:39.072-0700 I  INDEX    [replication-0] index build: done building index _id_ on ns local.temp_oplog_buffer
2020-05-08T12:15:39.072-0700 I  REPL     [replication-0] sync source candidate: n1:27019
2020-05-08T12:15:39.072-0700 I  INITSYNC [replication-0] Initial syncer oplog truncation finished in: 0ms
2020-05-08T12:15:39.072-0700 I  REPL     [replication-0] ******
2020-05-08T12:15:39.072-0700 I  REPL     [replication-0] creating replication oplog of size: 36643MB...
2020-05-08T12:15:39.072-0700 I  STORAGE  [replication-0] createCollection: local.oplog.rs with generated UUID: 275a5d1c-85e8-455d-8ede-b60aad3b08c2 and options: { capped: true, size: 38423989248.0, autoIndexId: false }
2020-05-08T12:15:39.134-0700 I  STORAGE  [replication-0] Starting OplogTruncaterThread local.oplog.rs
2020-05-08T12:15:39.134-0700 I  STORAGE  [replication-0] The size storer reports that the oplog contains 0 records totaling to 0 bytes
2020-05-08T12:15:39.134-0700 I  STORAGE  [replication-0] Scanning the oplog to determine where to place markers for truncation
2020-05-08T12:15:39.134-0700 I  STORAGE  [replication-0] WiredTiger record store oplog processing took 0ms
2020-05-08T12:15:39.467-0700 I  REPL     [replication-0] ******
2020-05-08T12:15:39.468-0700 I  REPL     [replication-0] dropReplicatedDatabases - dropping 1 databases
2020-05-08T12:15:39.468-0700 I  REPL     [replication-0] dropReplicatedDatabases - dropped 1 databases
2020-05-08T12:15:39.468-0700 I  CONNPOOL [RS] Connecting to n1:27019
2020-05-08T12:15:39.472-0700 I  SHARDING [replication-1] Marking collection local.temp_oplog_buffer as collection version: <unsharded>
2020-05-08T12:15:39.472-0700 I  INITSYNC [replication-0] CollectionCloner::start called, on ns:admin.system.version
2020-05-08T12:15:39.474-0700 I  STORAGE  [repl-writer-worker-15] createCollection: admin.system.version with provided UUID: 9776b654-fbe0-4ade-a717-5da68907cd8a and options: { uuid: UUID("9776b654-fbe0-4ade-a717-5da68907cd8a") }
2020-05-08T12:15:39.600-0700 I  INDEX    [repl-writer-worker-15] index build: starting on admin.system.version properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.version" } using method: Foreground
2020-05-08T12:15:39.600-0700 I  INDEX    [repl-writer-worker-15] build may temporarily use up to 200 megabytes of RAM
2020-05-08T12:15:39.603-0700 I  COMMAND  [repl-writer-worker-0] setting featureCompatibilityVersion to 4.2
2020-05-08T12:15:39.603-0700 I  NETWORK  [repl-writer-worker-0] Skip closing connection for connection # 9
2020-05-08T12:15:39.603-0700 I  NETWORK  [repl-writer-worker-0] Skip closing connection for connection # 4
2020-05-08T12:15:39.603-0700 I  INITSYNC [replication-0] CollectionCloner ns:admin.system.version finished cloning with status: OK
2020-05-08T12:15:39.604-0700 I  INDEX    [replication-0] index build: inserted 1 keys from external sorter into index in 0 seconds
2020-05-08T12:15:39.621-0700 I  INDEX    [replication-0] index build: done building index _id_ on ns admin.system.version
2020-05-08T12:15:39.622-0700 I  INITSYNC [replication-0] Finished cloning data: OK. Beginning oplog replay.
2020-05-08T12:15:39.623-0700 I  INITSYNC [replication-1] No need to apply operations. (currently at { : Timestamp(1588965338, 1) })
2020-05-08T12:15:39.624-0700 I  SHARDING [replication-0] Marking collection local.replset.oplogTruncateAfterPoint as collection version: <unsharded>
2020-05-08T12:15:39.624-0700 I  INITSYNC [replication-1] Finished fetching oplog during initial sync: CallbackCanceled: error in fetcher batch callback: oplog fetcher is shutting down. Last fetched optime: { ts: Timestamp(0, 0), t: -1 }
2020-05-08T12:15:39.624-0700 I  INITSYNC [replication-1] Initial sync attempt finishing up.
2020-05-08T12:15:39.624-0700 I  INITSYNC [replication-1] Initial Sync Attempt Statistics: { failedInitialSyncAttempts: 0, maxFailedInitialSyncAttempts: 10, initialSyncStart: new Date(1588965339006), initialSyncAttempts: [], fetchedMissingDocs: 0, appliedOps: 0, initialSyncOplogStart: Timestamp(1588965338, 1), initialSyncOplogEnd: Timestamp(1588965338, 1), databases: { databasesCloned: 1, admin: { collections: 1, clonedCollections: 1, start: new Date(1588965339472), end: new Date(1588965339623), elapsedMillis: 151, admin.system.version: { documentsToCopy: 1, documentsCopied: 1, indexes: 1, fetchedBatches: 1, start: new Date(1588965339472), end: new Date(1588965339623), elapsedMillis: 151, receivedBatches: 1 } } } }
2020-05-08T12:15:39.625-0700 I  CONNPOOL [RS] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:15:39.625-0700 I  STORAGE  [replication-0] Finishing collection drop for local.temp_oplog_buffer (329b170f-3973-459a-a084-df56a688bee1).
2020-05-08T12:15:39.649-0700 I  SHARDING [replication-0] Marking collection config.transactions as collection version: <unsharded>
2020-05-08T12:15:39.655-0700 I  INITSYNC [replication-0] initial sync done; took 0s.
2020-05-08T12:15:39.655-0700 I  REPL     [replication-0] transition to RECOVERING from STARTUP2
2020-05-08T12:15:39.655-0700 I  REPL     [replication-0] Starting replication fetcher thread
2020-05-08T12:15:39.655-0700 I  REPL     [replication-0] Starting replication applier thread
2020-05-08T12:15:39.655-0700 I  REPL     [replication-0] Starting replication reporter thread
2020-05-08T12:15:39.656-0700 I  REPL     [rsSync-0] Starting oplog application
2020-05-08T12:15:39.656-0700 I  REPL     [rsBackgroundSync] could not find member to sync from
2020-05-08T12:15:39.657-0700 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
2020-05-08T12:15:39.657-0700 I  REPL     [rsSync-0] Resetting sync source to empty, which was :27017
2020-05-08T12:15:39.676-0700 I  ELECTION [conn4] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 0, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965338, 1), t: -1 } }
2020-05-08T12:15:39.678-0700 I  ELECTION [conn4] Sending vote response: { term: 0, voteGranted: true, reason: "" }
2020-05-08T12:15:39.695-0700 I  ELECTION [conn4] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 1, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965338, 1), t: -1 } }
2020-05-08T12:15:39.695-0700 I  ELECTION [conn4] Sending vote response: { term: 1, voteGranted: true, reason: "" }
2020-05-08T12:15:40.158-0700 I  REPL     [replexec-4] Member n3:27019 is now in state SECONDARY
2020-05-08T12:15:40.158-0700 I  REPL     [replexec-1] Member n1:27019 is now in state PRIMARY
2020-05-08T12:15:40.602-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:51628 #14 (3 connections now open)
2020-05-08T12:15:40.603-0700 I  NETWORK  [conn14] received client metadata from 192.168.122.1:51628 conn14: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:15:40.605-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:51652 #15 (4 connections now open)
2020-05-08T12:15:40.605-0700 I  NETWORK  [conn15] received client metadata from 192.168.122.1:51652 conn15: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:15:40.612-0700 I  NETWORK  [conn14] end connection 192.168.122.1:51628 (3 connections now open)
2020-05-08T12:15:40.612-0700 I  NETWORK  [conn15] end connection 192.168.122.1:51652 (2 connections now open)
2020-05-08T12:15:40.656-0700 I  REPL     [rsBackgroundSync] sync source candidate: n1:27019
2020-05-08T12:15:41.268-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:43432 #16 (3 connections now open)
2020-05-08T12:15:41.268-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:53956 #17 (4 connections now open)
2020-05-08T12:15:41.269-0700 I  NETWORK  [conn16] received client metadata from 192.168.122.13:43432 conn16: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:41.269-0700 I  NETWORK  [conn17] received client metadata from 192.168.122.11:53956 conn17: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:41.269-0700 I  NETWORK  [listener] connection accepted from 192.168.122.16:35786 #18 (5 connections now open)
2020-05-08T12:15:41.269-0700 I  NETWORK  [conn18] received client metadata from 192.168.122.16:35786 conn18: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:41.270-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:43448 #19 (6 connections now open)
2020-05-08T12:15:41.270-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:53968 #20 (7 connections now open)
2020-05-08T12:15:41.270-0700 I  NETWORK  [conn19] received client metadata from 192.168.122.13:43448 conn19: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:41.270-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:53972 #21 (8 connections now open)
2020-05-08T12:15:41.270-0700 I  NETWORK  [conn20] received client metadata from 192.168.122.11:53968 conn20: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:41.271-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:43456 #22 (9 connections now open)
2020-05-08T12:15:41.271-0700 I  NETWORK  [conn21] received client metadata from 192.168.122.11:53972 conn21: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:41.272-0700 I  NETWORK  [listener] connection accepted from 192.168.122.16:35804 #23 (10 connections now open)
2020-05-08T12:15:41.272-0700 I  NETWORK  [conn22] received client metadata from 192.168.122.13:43456 conn22: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:41.272-0700 I  NETWORK  [listener] connection accepted from 192.168.122.16:35808 #24 (11 connections now open)
2020-05-08T12:15:41.272-0700 I  NETWORK  [conn23] received client metadata from 192.168.122.16:35804 conn23: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:41.272-0700 I  NETWORK  [conn24] received client metadata from 192.168.122.16:35808 conn24: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:41.289-0700 I  NETWORK  [listener] connection accepted from 192.168.122.15:60682 #25 (12 connections now open)
2020-05-08T12:15:41.289-0700 I  NETWORK  [conn25] received client metadata from 192.168.122.15:60682 conn25: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:41.336-0700 I  NETWORK  [listener] connection accepted from 192.168.122.14:48160 #26 (13 connections now open)
2020-05-08T12:15:41.337-0700 I  NETWORK  [listener] connection accepted from 192.168.122.19:56930 #27 (14 connections now open)
2020-05-08T12:15:41.337-0700 I  NETWORK  [conn26] received client metadata from 192.168.122.14:48160 conn26: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:41.337-0700 I  NETWORK  [listener] connection accepted from 192.168.122.18:38738 #28 (15 connections now open)
2020-05-08T12:15:41.337-0700 I  NETWORK  [conn27] received client metadata from 192.168.122.19:56930 conn27: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:41.337-0700 I  NETWORK  [listener] connection accepted from 192.168.122.17:44176 #29 (16 connections now open)
2020-05-08T12:15:41.337-0700 I  NETWORK  [conn28] received client metadata from 192.168.122.18:38738 conn28: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:41.338-0700 I  NETWORK  [listener] connection accepted from 192.168.122.12:45624 #30 (17 connections now open)
2020-05-08T12:15:41.338-0700 I  NETWORK  [conn29] received client metadata from 192.168.122.17:44176 conn29: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:41.338-0700 I  NETWORK  [conn30] received client metadata from 192.168.122.12:45624 conn30: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:41.409-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n1:27019
2020-05-08T12:15:41.410-0700 I  CONNPOOL [RS] Connecting to n1:27019
2020-05-08T12:15:41.413-0700 I  STORAGE  [repl-writer-worker-2] createCollection: config.transactions with provided UUID: 102592d2-904e-4bad-918e-733bcd99a988 and options: { uuid: UUID("102592d2-904e-4bad-918e-733bcd99a988") }
2020-05-08T12:15:41.459-0700 I  INDEX    [repl-writer-worker-2] index build: done building index _id_ on ns config.transactions
2020-05-08T12:15:41.462-0700 I  STORAGE  [repl-writer-worker-6] createCollection: config.chunks with provided UUID: 2d652394-987f-4aa4-a6e9-0606d5d5533d and options: { uuid: UUID("2d652394-987f-4aa4-a6e9-0606d5d5533d") }
2020-05-08T12:15:41.506-0700 I  INDEX    [repl-writer-worker-6] index build: done building index _id_ on ns config.chunks
2020-05-08T12:15:41.583-0700 I  INDEX    [repl-writer-worker-10] index build: starting on config.chunks properties: { v: 2, unique: true, key: { ns: 1, min: 1 }, name: "ns_1_min_1", ns: "config.chunks" } using method: Hybrid
2020-05-08T12:15:41.583-0700 I  INDEX    [repl-writer-worker-10] build may temporarily use up to 200 megabytes of RAM
2020-05-08T12:15:41.583-0700 I  STORAGE  [repl-writer-worker-10] Index build initialized: c3331626-ac5b-4b7c-92ea-50e4ff149eb5: config.chunks (2d652394-987f-4aa4-a6e9-0606d5d5533d ): indexes: 1
2020-05-08T12:15:41.584-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2020-05-08T12:15:41.585-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2020-05-08T12:15:41.599-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index ns_1_min_1 on ns config.chunks
2020-05-08T12:15:41.611-0700 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: c3331626-ac5b-4b7c-92ea-50e4ff149eb5: config.chunks ( 2d652394-987f-4aa4-a6e9-0606d5d5533d ). Index specs built: 1. Indexes in catalog before build: 1. Indexes in catalog after build: 2
2020-05-08T12:15:41.673-0700 I  INDEX    [repl-writer-worker-14] index build: starting on config.chunks properties: { v: 2, unique: true, key: { ns: 1, shard: 1, min: 1 }, name: "ns_1_shard_1_min_1", ns: "config.chunks" } using method: Hybrid
2020-05-08T12:15:41.673-0700 I  INDEX    [repl-writer-worker-14] build may temporarily use up to 200 megabytes of RAM
2020-05-08T12:15:41.673-0700 I  STORAGE  [repl-writer-worker-14] Index build initialized: 18ad6377-0da9-4c49-b501-75307ade0986: config.chunks (2d652394-987f-4aa4-a6e9-0606d5d5533d ): indexes: 1
2020-05-08T12:15:41.674-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2020-05-08T12:15:41.675-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2020-05-08T12:15:41.683-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index ns_1_shard_1_min_1 on ns config.chunks
2020-05-08T12:15:41.691-0700 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: 18ad6377-0da9-4c49-b501-75307ade0986: config.chunks ( 2d652394-987f-4aa4-a6e9-0606d5d5533d ). Index specs built: 1. Indexes in catalog before build: 2. Indexes in catalog after build: 3
2020-05-08T12:15:41.763-0700 I  INDEX    [repl-writer-worker-2] index build: starting on config.chunks properties: { v: 2, unique: true, key: { ns: 1, lastmod: 1 }, name: "ns_1_lastmod_1", ns: "config.chunks" } using method: Hybrid
2020-05-08T12:15:41.763-0700 I  INDEX    [repl-writer-worker-2] build may temporarily use up to 200 megabytes of RAM
2020-05-08T12:15:41.763-0700 I  STORAGE  [repl-writer-worker-2] Index build initialized: c14ca629-dff6-498d-8eea-f06c489582e4: config.chunks (2d652394-987f-4aa4-a6e9-0606d5d5533d ): indexes: 1
2020-05-08T12:15:41.763-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2020-05-08T12:15:41.765-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2020-05-08T12:15:41.766-0700 I  STORAGE  [repl-writer-worker-4] createCollection: config.migrations with provided UUID: 2a55a29f-6ce9-4f79-b893-278af01d545c and options: { uuid: UUID("2a55a29f-6ce9-4f79-b893-278af01d545c") }
2020-05-08T12:15:41.772-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index ns_1_lastmod_1 on ns config.chunks
2020-05-08T12:15:41.775-0700 I  STORAGE  [replication-0] Triggering the first stable checkpoint. Initial Data: Timestamp(1588965338, 1) PrevStable: Timestamp(0, 0) CurrStable: Timestamp(1588965340, 4)
2020-05-08T12:15:41.776-0700 I  SHARDING [conn21] Marking collection admin.system.keys as collection version: <unsharded>
2020-05-08T12:15:41.776-0700 I  SHARDING [conn20] Marking collection config.version as collection version: <unsharded>
2020-05-08T12:15:41.776-0700 I  SHARDING [conn22] Marking collection config.shards as collection version: <unsharded>
2020-05-08T12:15:41.836-0700 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: c14ca629-dff6-498d-8eea-f06c489582e4: config.chunks ( 2d652394-987f-4aa4-a6e9-0606d5d5533d ). Index specs built: 1. Indexes in catalog before build: 3. Indexes in catalog after build: 4
2020-05-08T12:15:41.837-0700 I  INDEX    [repl-writer-worker-4] index build: done building index _id_ on ns config.migrations
2020-05-08T12:15:41.837-0700 I  COMMAND  [conn22] command config.shards command: find { find: "shards", readConcern: { level: "majority", afterOpTime: { ts: Timestamp(0, 0), t: -1 } }, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(0, 0), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $configServerState: { opTime: { ts: Timestamp(0, 0), t: -1 } }, $db: "config" } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:549 locks:{ ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { r: 1 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 2 } }, Mutex: { acquireCount: { r: 1 } } } protocol:op_msg 504ms
2020-05-08T12:15:41.837-0700 I  COMMAND  [conn23] command config.version command: find { find: "version", readConcern: { level: "majority", afterOpTime: { ts: Timestamp(0, 0), t: -1 } }, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(0, 0), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $configServerState: { opTime: { ts: Timestamp(0, 0), t: -1 } }, $db: "config" } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:550 locks:{ ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { r: 1 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 2 } }, Mutex: { acquireCount: { r: 1 } } } protocol:op_msg 504ms
2020-05-08T12:15:41.837-0700 I  COMMAND  [conn24] command admin.system.keys command: find { find: "system.keys", filter: { purpose: "HMAC", expiresAt: { $gt: Timestamp(0, 0) } }, sort: { expiresAt: 1 }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp(0, 0), t: -1 } }, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(0, 0), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $configServerState: { opTime: { ts: Timestamp(0, 0), t: -1 } }, $db: "admin" } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:553 locks:{ ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { r: 1 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 2 } }, Mutex: { acquireCount: { r: 1 } } } protocol:op_msg 504ms
2020-05-08T12:15:41.837-0700 I  COMMAND  [conn21] command admin.system.keys command: find { find: "system.keys", filter: { purpose: "HMAC", expiresAt: { $gt: Timestamp(0, 0) } }, sort: { expiresAt: 1 }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp(0, 0), t: -1 } }, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(0, 0), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $configServerState: { opTime: { ts: Timestamp(0, 0), t: -1 } }, $db: "admin" } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:553 locks:{ ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { r: 1 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 2 } }, Mutex: { acquireCount: { r: 1 } } } protocol:op_msg 504ms
2020-05-08T12:15:41.837-0700 I  COMMAND  [conn20] command config.version command: find { find: "version", readConcern: { level: "majority", afterOpTime: { ts: Timestamp(0, 0), t: -1 } }, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(0, 0), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $configServerState: { opTime: { ts: Timestamp(0, 0), t: -1 } }, $db: "config" } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:550 locks:{ ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { r: 1 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 2 } }, Mutex: { acquireCount: { r: 1 } } } protocol:op_msg 504ms
2020-05-08T12:15:41.837-0700 I  COMMAND  [conn19] command config.version command: find { find: "version", readConcern: { level: "majority", afterOpTime: { ts: Timestamp(0, 0), t: -1 } }, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(0, 0), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $configServerState: { opTime: { ts: Timestamp(0, 0), t: -1 } }, $db: "config" } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:550 locks:{ ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { r: 1 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 2 } }, Mutex: { acquireCount: { r: 1 } } } protocol:op_msg 505ms
2020-05-08T12:15:41.924-0700 I  INDEX    [repl-writer-worker-8] index build: starting on config.migrations properties: { v: 2, unique: true, key: { ns: 1, min: 1 }, name: "ns_1_min_1", ns: "config.migrations" } using method: Hybrid
2020-05-08T12:15:41.924-0700 I  INDEX    [repl-writer-worker-8] build may temporarily use up to 200 megabytes of RAM
2020-05-08T12:15:41.925-0700 I  STORAGE  [repl-writer-worker-8] Index build initialized: 00cc87ac-985e-406c-9aeb-91dd13458cae: config.migrations (2a55a29f-6ce9-4f79-b893-278af01d545c ): indexes: 1
2020-05-08T12:15:41.925-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2020-05-08T12:15:41.926-0700 I  STORAGE  [repl-writer-worker-10] createCollection: config.shards with provided UUID: f0fccbd1-5de7-40cd-aa99-5999ae922b22 and options: { uuid: UUID("f0fccbd1-5de7-40cd-aa99-5999ae922b22") }
2020-05-08T12:15:41.926-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2020-05-08T12:15:41.957-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index ns_1_min_1 on ns config.migrations
2020-05-08T12:15:41.983-0700 I  INDEX    [repl-writer-worker-10] index build: done building index _id_ on ns config.shards
2020-05-08T12:15:41.997-0700 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: 00cc87ac-985e-406c-9aeb-91dd13458cae: config.migrations ( 2a55a29f-6ce9-4f79-b893-278af01d545c ). Index specs built: 1. Indexes in catalog before build: 1. Indexes in catalog after build: 2
2020-05-08T12:15:42.064-0700 I  INDEX    [repl-writer-worker-14] index build: starting on config.shards properties: { v: 2, unique: true, key: { host: 1 }, name: "host_1", ns: "config.shards" } using method: Hybrid
2020-05-08T12:15:42.064-0700 I  INDEX    [repl-writer-worker-14] build may temporarily use up to 200 megabytes of RAM
2020-05-08T12:15:42.064-0700 I  STORAGE  [repl-writer-worker-14] Index build initialized: b5391854-fb32-4897-bbd4-953a42349f32: config.shards (f0fccbd1-5de7-40cd-aa99-5999ae922b22 ): indexes: 1
2020-05-08T12:15:42.065-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2020-05-08T12:15:42.066-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2020-05-08T12:15:42.066-0700 I  STORAGE  [repl-writer-worker-0] createCollection: config.locks with provided UUID: f67e3cf8-f609-40d5-b10f-51cc8df9bd92 and options: { uuid: UUID("f67e3cf8-f609-40d5-b10f-51cc8df9bd92") }
2020-05-08T12:15:42.075-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index host_1 on ns config.shards
2020-05-08T12:15:42.121-0700 I  INDEX    [repl-writer-worker-0] index build: done building index _id_ on ns config.locks
2020-05-08T12:15:42.123-0700 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: b5391854-fb32-4897-bbd4-953a42349f32: config.shards ( f0fccbd1-5de7-40cd-aa99-5999ae922b22 ). Index specs built: 1. Indexes in catalog before build: 1. Indexes in catalog after build: 2
2020-05-08T12:15:42.170-0700 I  INDEX    [repl-writer-worker-4] index build: starting on config.locks properties: { v: 2, key: { ts: 1 }, name: "ts_1", ns: "config.locks" } using method: Hybrid
2020-05-08T12:15:42.170-0700 I  INDEX    [repl-writer-worker-4] build may temporarily use up to 200 megabytes of RAM
2020-05-08T12:15:42.170-0700 I  STORAGE  [repl-writer-worker-4] Index build initialized: 3c6c7ffb-f51d-4731-bbe3-fdd02cc1b54f: config.locks (f67e3cf8-f609-40d5-b10f-51cc8df9bd92 ): indexes: 1
2020-05-08T12:15:42.170-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2020-05-08T12:15:42.172-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2020-05-08T12:15:42.181-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index ts_1 on ns config.locks
2020-05-08T12:15:42.189-0700 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: 3c6c7ffb-f51d-4731-bbe3-fdd02cc1b54f: config.locks ( f67e3cf8-f609-40d5-b10f-51cc8df9bd92 ). Index specs built: 1. Indexes in catalog before build: 1. Indexes in catalog after build: 2
2020-05-08T12:15:42.238-0700 I  INDEX    [repl-writer-worker-8] index build: starting on config.locks properties: { v: 2, key: { state: 1, process: 1 }, name: "state_1_process_1", ns: "config.locks" } using method: Hybrid
2020-05-08T12:15:42.239-0700 I  INDEX    [repl-writer-worker-8] build may temporarily use up to 200 megabytes of RAM
2020-05-08T12:15:42.239-0700 I  STORAGE  [repl-writer-worker-8] Index build initialized: 6e2a5baa-87e0-4594-bf87-cadf41632a02: config.locks (f67e3cf8-f609-40d5-b10f-51cc8df9bd92 ): indexes: 1
2020-05-08T12:15:42.239-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2020-05-08T12:15:42.240-0700 I  STORAGE  [repl-writer-worker-10] createCollection: config.lockpings with provided UUID: 81acf486-601b-4e2b-8ee1-bbf74a1edd96 and options: { uuid: UUID("81acf486-601b-4e2b-8ee1-bbf74a1edd96") }
2020-05-08T12:15:42.241-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2020-05-08T12:15:42.277-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index state_1_process_1 on ns config.locks
2020-05-08T12:15:42.300-0700 I  INDEX    [repl-writer-worker-10] index build: done building index _id_ on ns config.lockpings
2020-05-08T12:15:42.302-0700 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: 6e2a5baa-87e0-4594-bf87-cadf41632a02: config.locks ( f67e3cf8-f609-40d5-b10f-51cc8df9bd92 ). Index specs built: 1. Indexes in catalog before build: 2. Indexes in catalog after build: 3
2020-05-08T12:15:42.346-0700 I  INDEX    [repl-writer-worker-14] index build: starting on config.lockpings properties: { v: 2, key: { ping: 1 }, name: "ping_1", ns: "config.lockpings" } using method: Hybrid
2020-05-08T12:15:42.346-0700 I  INDEX    [repl-writer-worker-14] build may temporarily use up to 200 megabytes of RAM
2020-05-08T12:15:42.346-0700 I  STORAGE  [repl-writer-worker-14] Index build initialized: ca78b4ee-efb4-4237-ab3e-8a199d84a96b: config.lockpings (81acf486-601b-4e2b-8ee1-bbf74a1edd96 ): indexes: 1
2020-05-08T12:15:42.346-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2020-05-08T12:15:42.347-0700 I  STORAGE  [repl-writer-worker-0] createCollection: config.tags with provided UUID: bc4858df-e110-456c-83bd-e519d850f168 and options: { uuid: UUID("bc4858df-e110-456c-83bd-e519d850f168") }
2020-05-08T12:15:42.347-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2020-05-08T12:15:42.355-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index ping_1 on ns config.lockpings
2020-05-08T12:15:42.377-0700 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: ca78b4ee-efb4-4237-ab3e-8a199d84a96b: config.lockpings ( 81acf486-601b-4e2b-8ee1-bbf74a1edd96 ). Index specs built: 1. Indexes in catalog before build: 1. Indexes in catalog after build: 2
2020-05-08T12:15:42.402-0700 I  INDEX    [repl-writer-worker-0] index build: done building index _id_ on ns config.tags
2020-05-08T12:15:42.444-0700 I  NETWORK  [listener] connection accepted from 192.168.122.15:60852 #32 (18 connections now open)
2020-05-08T12:15:42.444-0700 I  NETWORK  [conn32] received client metadata from 192.168.122.15:60852 conn32: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:42.475-0700 I  INDEX    [repl-writer-worker-4] index build: starting on config.tags properties: { v: 2, unique: true, key: { ns: 1, min: 1 }, name: "ns_1_min_1", ns: "config.tags" } using method: Hybrid
2020-05-08T12:15:42.475-0700 I  INDEX    [repl-writer-worker-4] build may temporarily use up to 200 megabytes of RAM
2020-05-08T12:15:42.475-0700 I  STORAGE  [repl-writer-worker-4] Index build initialized: 5725c24e-e91a-4f6a-af13-84ea917fe085: config.tags (bc4858df-e110-456c-83bd-e519d850f168 ): indexes: 1
2020-05-08T12:15:42.476-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2020-05-08T12:15:42.477-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2020-05-08T12:15:42.487-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index ns_1_min_1 on ns config.tags
2020-05-08T12:15:42.501-0700 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: 5725c24e-e91a-4f6a-af13-84ea917fe085: config.tags ( bc4858df-e110-456c-83bd-e519d850f168 ). Index specs built: 1. Indexes in catalog before build: 1. Indexes in catalog after build: 2
2020-05-08T12:15:42.547-0700 I  INDEX    [repl-writer-worker-8] index build: starting on config.tags properties: { v: 2, key: { ns: 1, tag: 1 }, name: "ns_1_tag_1", ns: "config.tags" } using method: Hybrid
2020-05-08T12:15:42.547-0700 I  INDEX    [repl-writer-worker-8] build may temporarily use up to 200 megabytes of RAM
2020-05-08T12:15:42.547-0700 I  STORAGE  [repl-writer-worker-8] Index build initialized: 33698308-1efd-4222-806e-ee6f17f517c9: config.tags (bc4858df-e110-456c-83bd-e519d850f168 ): indexes: 1
2020-05-08T12:15:42.547-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2020-05-08T12:15:42.549-0700 I  STORAGE  [repl-writer-worker-10] createCollection: config.version with provided UUID: bed99a80-dcc6-4152-9366-8222f3d1172a and options: { uuid: UUID("bed99a80-dcc6-4152-9366-8222f3d1172a") }
2020-05-08T12:15:42.549-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2020-05-08T12:15:42.579-0700 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index ns_1_tag_1 on ns config.tags
2020-05-08T12:15:42.602-0700 I  INDEX    [repl-writer-worker-10] index build: done building index _id_ on ns config.version
2020-05-08T12:15:42.604-0700 I  SHARDING [repl-writer-worker-12] Marking collection config.lockpings as collection version: <unsharded>
2020-05-08T12:15:42.611-0700 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: 33698308-1efd-4222-806e-ee6f17f517c9: config.tags ( bc4858df-e110-456c-83bd-e519d850f168 ). Index specs built: 1. Indexes in catalog before build: 2. Indexes in catalog after build: 3
2020-05-08T12:15:42.649-0700 I  STORAGE  [repl-writer-worker-4] createCollection: admin.system.keys with provided UUID: d89be7fe-6053-4310-be2b-4bff25fadfb8 and options: { uuid: UUID("d89be7fe-6053-4310-be2b-4bff25fadfb8") }
2020-05-08T12:15:42.693-0700 I  INDEX    [repl-writer-worker-4] index build: done building index _id_ on ns admin.system.keys
2020-05-08T12:15:42.898-0700 I  NETWORK  [listener] connection accepted from 192.168.122.12:45762 #33 (19 connections now open)
2020-05-08T12:15:42.899-0700 I  NETWORK  [conn33] received client metadata from 192.168.122.12:45762 conn33: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:43.599-0700 I  SHARDING [conn32] Marking collection config.settings as collection version: <unsharded>
2020-05-08T12:15:43.607-0700 I  SHARDING [conn33] Marking collection config.collections as collection version: <unsharded>
2020-05-08T12:15:43.653-0700 I  STORAGE  [repl-writer-worker-10] createCollection: config.mongos with provided UUID: ed1a62fa-1d96-460b-b917-f8098f56b82b and options: { uuid: UUID("ed1a62fa-1d96-460b-b917-f8098f56b82b") }
2020-05-08T12:15:43.703-0700 I  INDEX    [repl-writer-worker-10] index build: done building index _id_ on ns config.mongos
2020-05-08T12:15:43.705-0700 I  SHARDING [repl-writer-worker-15] Marking collection config.mongos as collection version: <unsharded>
2020-05-08T12:15:45.375-0700 I  NETWORK  [listener] connection accepted from 192.168.122.14:48452 #34 (20 connections now open)
2020-05-08T12:15:45.376-0700 I  NETWORK  [conn34] received client metadata from 192.168.122.14:48452 conn34: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:45.378-0700 I  NETWORK  [listener] connection accepted from 192.168.122.14:48474 #35 (21 connections now open)
2020-05-08T12:15:45.378-0700 I  NETWORK  [conn35] received client metadata from 192.168.122.14:48474 conn35: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:45.382-0700 I  NETWORK  [listener] connection accepted from 192.168.122.15:32780 #36 (22 connections now open)
2020-05-08T12:15:45.383-0700 I  NETWORK  [conn36] received client metadata from 192.168.122.15:32780 conn36: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:45.383-0700 I  NETWORK  [listener] connection accepted from 192.168.122.16:36150 #37 (23 connections now open)
2020-05-08T12:15:45.384-0700 I  NETWORK  [conn37] received client metadata from 192.168.122.16:36150 conn37: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:45.495-0700 I  STORAGE  [repl-writer-worker-12] createCollection: config.changelog with provided UUID: b5b04cc3-7329-463d-9284-d3319d7ed8dd and options: { uuid: UUID("b5b04cc3-7329-463d-9284-d3319d7ed8dd"), capped: true, size: 209715200 }
2020-05-08T12:15:45.542-0700 I  INDEX    [repl-writer-worker-12] index build: done building index _id_ on ns config.changelog
2020-05-08T12:15:45.546-0700 I  SHARDING [repl-writer-worker-14] Marking collection config.changelog as collection version: <unsharded>
2020-05-08T12:15:45.567-0700 I  NETWORK  [listener] connection accepted from 192.168.122.17:44518 #38 (24 connections now open)
2020-05-08T12:15:45.567-0700 I  NETWORK  [conn38] received client metadata from 192.168.122.17:44518 conn38: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:45.570-0700 I  NETWORK  [listener] connection accepted from 192.168.122.17:44524 #39 (25 connections now open)
2020-05-08T12:15:45.570-0700 I  NETWORK  [conn39] received client metadata from 192.168.122.17:44524 conn39: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:45.572-0700 I  NETWORK  [listener] connection accepted from 192.168.122.19:57302 #40 (26 connections now open)
2020-05-08T12:15:45.572-0700 I  NETWORK  [listener] connection accepted from 192.168.122.18:39106 #41 (27 connections now open)
2020-05-08T12:15:45.573-0700 I  NETWORK  [conn40] received client metadata from 192.168.122.19:57302 conn40: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:45.573-0700 I  NETWORK  [conn41] received client metadata from 192.168.122.18:39106 conn41: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:45.574-0700 I  NETWORK  [listener] connection accepted from 192.168.122.19:57312 #42 (28 connections now open)
2020-05-08T12:15:45.575-0700 I  NETWORK  [conn42] received client metadata from 192.168.122.19:57312 conn42: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:45.677-0700 I  SHARDING [repl-writer-worker-11] Marking collection config.locks as collection version: <unsharded>
2020-05-08T12:15:45.727-0700 I  STORAGE  [repl-writer-worker-1] createCollection: config.databases with provided UUID: ca3f968d-1e40-42a7-82a2-1fb918dba4dd and options: { uuid: UUID("ca3f968d-1e40-42a7-82a2-1fb918dba4dd") }
2020-05-08T12:15:45.771-0700 I  INDEX    [repl-writer-worker-1] index build: done building index _id_ on ns config.databases
2020-05-08T12:15:45.773-0700 I  SHARDING [repl-writer-worker-0] Marking collection config.databases as collection version: <unsharded>
2020-05-08T12:15:46.013-0700 I  SHARDING [conn35] Marking collection config.tags as collection version: <unsharded>
2020-05-08T12:15:46.179-0700 I  SHARDING [repl-writer-worker-3] Marking collection config.chunks as collection version: <unsharded>
2020-05-08T12:15:46.247-0700 I  STORAGE  [repl-writer-worker-9] createCollection: config.collections with provided UUID: a3749b3b-8edc-4244-a80b-c03e81de0177 and options: { uuid: UUID("a3749b3b-8edc-4244-a80b-c03e81de0177") }
2020-05-08T12:15:46.295-0700 I  INDEX    [repl-writer-worker-9] index build: done building index _id_ on ns config.collections
2020-05-08T12:15:50.781-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:52686 #43 (29 connections now open)
2020-05-08T12:15:50.782-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:52690 #44 (30 connections now open)
2020-05-08T12:15:50.782-0700 I  NETWORK  [conn43] received client metadata from 192.168.122.1:52686 conn43: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:15:50.782-0700 I  NETWORK  [conn44] received client metadata from 192.168.122.1:52690 conn44: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:15:50.784-0700 I  NETWORK  [conn43] end connection 192.168.122.1:52686 (29 connections now open)
2020-05-08T12:15:50.784-0700 I  NETWORK  [conn44] end connection 192.168.122.1:52690 (28 connections now open)
2020-05-08T12:15:51.404-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:52792 #45 (29 connections now open)
2020-05-08T12:15:51.404-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:52802 #46 (30 connections now open)
2020-05-08T12:15:51.404-0700 I  NETWORK  [conn45] received client metadata from 192.168.122.1:52792 conn45: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:15:51.404-0700 I  NETWORK  [conn46] received client metadata from 192.168.122.1:52802 conn46: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:15:51.408-0700 I  NETWORK  [conn45] end connection 192.168.122.1:52792 (29 connections now open)
2020-05-08T12:15:51.408-0700 I  NETWORK  [conn46] end connection 192.168.122.1:52802 (28 connections now open)
2020-05-08T12:15:52.462-0700 I  ELECTION [conn9] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 1, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965350, 10), t: 1 } }
2020-05-08T12:15:52.462-0700 I  ELECTION [conn9] Sending vote response: { term: 1, voteGranted: true, reason: "" }
2020-05-08T12:15:52.474-0700 I  ELECTION [conn9] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 2, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965350, 10), t: 1 } }
2020-05-08T12:15:52.474-0700 I  ELECTION [conn9] Sending vote response: { term: 2, voteGranted: true, reason: "" }
2020-05-08T12:15:52.665-0700 I  REPL     [replexec-4] Member n3:27019 is now in state PRIMARY
2020-05-08T12:15:52.665-0700 I  ELECTION [replexec-4] Scheduling priority takeover at 2020-05-08T12:15:54.778-0700
2020-05-08T12:15:53.664-0700 I  REPL     [replexec-2] Member n1:27019 is now in state RS_DOWN - Request 596 timed out, deadline was 2020-05-08T12:15:53.664-0700, op was RemoteCommand 596 -- target:[n1:27019] db:admin expDate:2020-05-08T12:15:53.664-0700 cmd:{ replSetHeartbeat: "rs_config", configVersion: 1, hbv: 1, from: "n2:27019", fromId: 1, term: 2 }
2020-05-08T12:15:53.664-0700 I  CONNPOOL [Replication] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-05-08T12:15:53.664-0700 I  CONNPOOL [Replication] Connecting to n1:27019
2020-05-08T12:15:53.765-0700 I  REPL     [replexec-3] Canceling priority takeover callback
2020-05-08T12:15:53.765-0700 I  ELECTION [replexec-3] Starting an election, since we've seen no PRIMARY in the past 1000ms
2020-05-08T12:15:53.769-0700 I  ELECTION [replexec-3] conducting a dry run election to see if we could be elected. current term: 2
2020-05-08T12:15:53.769-0700 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 597 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 2, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965350, 10), t: 1 } }
2020-05-08T12:15:53.769-0700 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 598 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 2, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965350, 10), t: 1 } }
2020-05-08T12:15:53.770-0700 I  ELECTION [replexec-1] VoteRequester(term 2 dry run) received a no vote from n3:27019 with reason "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965350, 10), t: 1 }, my last applied OpTime: { ts: Timestamp(1588965353, 1), t: 2 }"; response message: { term: 2, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965350, 10), t: 1 }, my last applied OpTime: { ts: Timestam...", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000002') }, lastCommittedOpTime: Timestamp(1588965350, 10), $clusterTime: { clusterTime: Timestamp(1588965353, 40), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965353, 1) }
2020-05-08T12:15:53.990-0700 I  NETWORK  [listener] connection accepted from 192.168.122.18:40036 #47 (29 connections now open)
2020-05-08T12:15:53.990-0700 I  NETWORK  [conn47] received client metadata from 192.168.122.18:40036 conn47: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:54.665-0700 I  REPL     [replexec-2] Member n3:27019 is now in state SECONDARY
2020-05-08T12:15:54.718-0700 I  ELECTION [conn9] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 2, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965353, 1), t: 2 } }
2020-05-08T12:15:54.719-0700 I  ELECTION [conn9] Sending vote response: { term: 2, voteGranted: true, reason: "" }
2020-05-08T12:15:54.728-0700 I  ELECTION [conn9] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 3, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965353, 1), t: 2 } }
2020-05-08T12:15:54.728-0700 I  ELECTION [conn9] Sending vote response: { term: 3, voteGranted: true, reason: "" }
2020-05-08T12:15:54.769-0700 I  ELECTION [replexec-3] VoteRequester(term 2 dry run) failed to receive response from n1:27019: NetworkInterfaceExceededTimeLimit: Couldn't get a connection within the time limit
2020-05-08T12:15:54.769-0700 I  ELECTION [replexec-3] not running for primary, we have been superseded already during dry run. original term: 2, current term: 3
2020-05-08T12:15:54.769-0700 I  ELECTION [replexec-3] Lost dry run election due to internal error
2020-05-08T12:15:55.826-0700 I  ELECTION [replexec-2] Starting an election, since we've seen no PRIMARY in the past 1000ms
2020-05-08T12:15:55.826-0700 I  ELECTION [replexec-2] conducting a dry run election to see if we could be elected. current term: 3
2020-05-08T12:15:55.826-0700 I  REPL     [replexec-2] Scheduling remote command request for vote request: RemoteCommand 601 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 3, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965350, 10), t: 1 } }
2020-05-08T12:15:55.826-0700 I  REPL     [replexec-2] Scheduling remote command request for vote request: RemoteCommand 602 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 3, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965350, 10), t: 1 } }
2020-05-08T12:15:55.826-0700 I  CONNPOOL [Replication] Connecting to n1:27019
2020-05-08T12:15:55.827-0700 I  ELECTION [replexec-0] VoteRequester(term 3 dry run) received a no vote from n3:27019 with reason "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965350, 10), t: 1 }, my last applied OpTime: { ts: Timestamp(1588965353, 1), t: 2 }"; response message: { term: 3, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965350, 10), t: 1 }, my last applied OpTime: { ts: Timestam...", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000003') }, lastCommittedOpTime: Timestamp(1588965350, 10), $clusterTime: { clusterTime: Timestamp(1588965355, 2), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965353, 1) }
2020-05-08T12:15:56.313-0700 I  NETWORK  [listener] connection accepted from 192.168.122.16:37194 #48 (30 connections now open)
2020-05-08T12:15:56.313-0700 I  NETWORK  [conn48] received client metadata from 192.168.122.16:37194 conn48: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:56.706-0700 I  ELECTION [replexec-2] VoteRequester(term 3 dry run) received a yes vote from n1:27019; response message: { term: 3, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000001') }, lastCommittedOpTime: Timestamp(1588965350, 10), $clusterTime: { clusterTime: Timestamp(1588965356, 2), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965350, 10) }
2020-05-08T12:15:56.706-0700 I  ELECTION [replexec-2] dry election run succeeded, running for election in term 4
2020-05-08T12:15:56.708-0700 I  REPL     [replexec-2] Scheduling remote command request for vote request: RemoteCommand 604 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 4, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965350, 10), t: 1 } }
2020-05-08T12:15:56.708-0700 I  REPL     [replexec-2] Scheduling remote command request for vote request: RemoteCommand 605 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 4, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965350, 10), t: 1 } }
2020-05-08T12:15:56.709-0700 I  ELECTION [replexec-4] VoteRequester(term 4) received a no vote from n3:27019 with reason "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965350, 10), t: 1 }, my last applied OpTime: { ts: Timestamp(1588965353, 1), t: 2 }"; response message: { term: 4, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965350, 10), t: 1 }, my last applied OpTime: { ts: Timestam...", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000003') }, lastCommittedOpTime: Timestamp(1588965350, 10), $clusterTime: { clusterTime: Timestamp(1588965356, 2), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965353, 1) }
2020-05-08T12:15:56.712-0700 I  ELECTION [replexec-0] VoteRequester(term 4) received a yes vote from n1:27019; response message: { term: 4, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000001') }, lastCommittedOpTime: Timestamp(1588965350, 10), $clusterTime: { clusterTime: Timestamp(1588965356, 2), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965350, 10) }
2020-05-08T12:15:56.712-0700 I  ELECTION [replexec-0] election succeeded, assuming primary role in term 4
2020-05-08T12:15:56.712-0700 I  REPL     [replexec-0] transition to PRIMARY from SECONDARY
2020-05-08T12:15:56.712-0700 I  REPL     [replexec-0] Resetting sync source to empty, which was n1:27019
2020-05-08T12:15:56.712-0700 I  REPL     [replexec-0] Entering primary catch-up mode.
2020-05-08T12:15:56.713-0700 I  REPL     [replexec-0] Member n1:27019 is now in state SECONDARY
2020-05-08T12:15:56.713-0700 I  REPL     [replexec-2] Heartbeats updated catchup target optime to { ts: Timestamp(1588965353, 1), t: 2 }
2020-05-08T12:15:56.713-0700 I  REPL     [replexec-2] Latest known optime per replica set member:
2020-05-08T12:15:56.713-0700 I  REPL     [replexec-2] Member ID: MemberId(0), latest known optime: { ts: Timestamp(1588965350, 10), t: 1 }
2020-05-08T12:15:56.713-0700 I  REPL     [replexec-2] Member ID: MemberId(1), latest known optime: unknown
2020-05-08T12:15:56.713-0700 I  REPL     [replexec-2] Member ID: MemberId(2), latest known optime: { ts: Timestamp(1588965353, 1), t: 2 }
2020-05-08T12:15:56.853-0700 I  REPL     [replication-1] Restarting oplog query due to error: NetworkInterfaceExceededTimeLimit: error in fetcher batch callback :: caused by :: Request 593 timed out, deadline was 2020-05-08T12:15:56.853-0700, op was RemoteCommand 593 -- target:[n1:27019] db:local expDate:2020-05-08T12:15:56.853-0700 cmd:{ getMore: 1040700054944964285, collection: "oplog.rs", batchSize: 13981010, maxTimeMS: 500, term: 1, lastKnownCommittedOpTime: { ts: Timestamp(1588965350, 10), t: 1 } }. Last fetched optime: { ts: Timestamp(1588965350, 10), t: 1 }. Restarts remaining: 1
2020-05-08T12:15:56.853-0700 I  CONNPOOL [RS] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:15:56.853-0700 I  CONNPOOL [RS] Connecting to n1:27019
2020-05-08T12:15:56.853-0700 I  REPL     [replication-1] Scheduled new oplog query Fetcher source: n1:27019 database: local query: { find: "oplog.rs", filter: { ts: { $gte: Timestamp(1588965350, 10) } }, tailable: true, oplogReplay: true, awaitData: true, maxTimeMS: 2000, batchSize: 13981010, term: 4, readConcern: { afterClusterTime: Timestamp(0, 1) } } query metadata: { $replData: 1, $oplogQueryData: 1, $readPreference: { mode: "secondaryPreferred" } } active: 1 findNetworkTimeout: 7000ms getMoreNetworkTimeout: 5500ms shutting down?: 0 first: 1 firstCommandScheduler: RemoteCommandRetryScheduler request: RemoteCommand 608 -- target:n1:27019 db:local cmd:{ find: "oplog.rs", filter: { ts: { $gte: Timestamp(1588965350, 10) } }, tailable: true, oplogReplay: true, awaitData: true, maxTimeMS: 2000, batchSize: 13981010, term: 4, readConcern: { afterClusterTime: Timestamp(0, 1) } } active: 1 callbackHandle.valid: 1 callbackHandle.cancelled: 0 attempt: 1 retryPolicy: {type: "NoRetryPolicy"}
2020-05-08T12:15:56.855-0700 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: Sync source must be ahead of me. My last fetched oplog optime: { ts: Timestamp(1588965350, 10), t: 1 }, latest oplog optime of sync source: { ts: Timestamp(1588965350, 10), t: 1 }
2020-05-08T12:15:56.855-0700 I  REPL     [rsBackgroundSync] sync source candidate: n3:27019
2020-05-08T12:15:56.855-0700 I  CONNPOOL [RS] Connecting to n3:27019
2020-05-08T12:15:56.858-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n3:27019
2020-05-08T12:15:56.861-0700 I  REPL     [rsSync-0] Caught up to the latest known optime successfully after becoming primary. Target optime: { ts: Timestamp(1588965353, 1), t: 2 }. My Last Applied: { ts: Timestamp(1588965353, 1), t: 2 }
2020-05-08T12:15:56.861-0700 I  REPL     [rsSync-0] Exited primary catch-up mode.
2020-05-08T12:15:56.861-0700 I  REPL     [rsSync-0] Stopping replication producer
2020-05-08T12:15:56.861-0700 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 4
2020-05-08T12:15:56.861-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:15:56.861-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:15:56.861-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 0 }
2020-05-08T12:15:56.861-0700 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2020-05-08T12:15:56.864-0700 I  SHARDING [rsSync-0] Marking collection config.migrations as collection version: <unsharded>
2020-05-08T12:15:56.864-0700 I  SHARDING [Balancer] CSRS balancer is starting
2020-05-08T12:15:56.864-0700 I  SHARDING [PeriodicShardedIndexConsistencyChecker] Checking consistency of sharded collection indexes across the cluster
2020-05-08T12:15:56.865-0700 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-05-08T12:15:56.865-0700 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-05-08T12:15:56.865-0700 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-05-08T12:15:56.865-0700 I  NETWORK  [PeriodicShardedIndexConsistencyChecker] Starting new replica set monitor for rs_shard1/n4:27018,n5:27018,n6:27018
2020-05-08T12:15:56.865-0700 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Connecting to n6:27018
2020-05-08T12:15:56.865-0700 I  NETWORK  [PeriodicShardedIndexConsistencyChecker] Starting new replica set monitor for rs_shard2/n7:27018,n8:27018,n9:27018
2020-05-08T12:15:56.866-0700 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Connecting to n5:27018
2020-05-08T12:15:56.866-0700 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Connecting to n4:27018
2020-05-08T12:15:56.866-0700 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Connecting to n7:27018
2020-05-08T12:15:56.866-0700 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Connecting to n9:27018
2020-05-08T12:15:56.866-0700 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Connecting to n8:27018
2020-05-08T12:15:56.866-0700 I  SH_REFR  [ConfigServerCatalogCacheLoader-0] Refresh for database jepsendb from version {} to version { uuid: UUID("d83d36a8-6185-46a3-a2bf-8393b7a71805"), lastMod: 1 } took 0 ms
2020-05-08T12:15:56.866-0700 I  NETWORK  [listener] connection accepted from 192.168.122.14:49626 #53 (31 connections now open)
2020-05-08T12:15:56.866-0700 I  NETWORK  [listener] connection accepted from 192.168.122.18:40192 #54 (32 connections now open)
2020-05-08T12:15:56.867-0700 I  NETWORK  [conn53] received client metadata from 192.168.122.14:49626 conn53: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:56.867-0700 I  NETWORK  [listener] connection accepted from 192.168.122.14:49632 #56 (33 connections now open)
2020-05-08T12:15:56.867-0700 I  NETWORK  [conn54] received client metadata from 192.168.122.18:40192 conn54: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:56.867-0700 I  NETWORK  [listener] connection accepted from 192.168.122.17:45622 #57 (34 connections now open)
2020-05-08T12:15:56.867-0700 I  NETWORK  [conn56] received client metadata from 192.168.122.14:49632 conn56: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:56.867-0700 I  NETWORK  [listener] connection accepted from 192.168.122.17:45630 #59 (35 connections now open)
2020-05-08T12:15:56.867-0700 I  NETWORK  [conn57] received client metadata from 192.168.122.17:45622 conn57: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:56.867-0700 I  NETWORK  [conn59] received client metadata from 192.168.122.17:45630 conn59: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:56.868-0700 I  SH_REFR  [ConfigServerCatalogCacheLoader-0] Refresh for collection jepsendb.jepsencoll to version 1|6||5eb5afe20a0e2b150583a3b5 took 1 ms
2020-05-08T12:15:56.869-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:15:56.869-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard2
2020-05-08T12:15:56.874-0700 I  SHARDING [PeriodicShardedIndexConsistencyChecker] Found 0 collections with inconsistent indexes
2020-05-08T12:15:57.187-0700 I  NETWORK  [listener] connection accepted from 192.168.122.12:47120 #70 (36 connections now open)
2020-05-08T12:15:57.187-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:55520 #71 (37 connections now open)
2020-05-08T12:15:57.187-0700 I  NETWORK  [conn70] received client metadata from 192.168.122.12:47120 conn70: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:57.187-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:55522 #72 (38 connections now open)
2020-05-08T12:15:57.188-0700 I  NETWORK  [conn71] received client metadata from 192.168.122.11:55520 conn71: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:57.188-0700 I  NETWORK  [conn72] received client metadata from 192.168.122.11:55522 conn72: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:57.213-0700 I  NETWORK  [listener] connection accepted from 192.168.122.19:58452 #73 (39 connections now open)
2020-05-08T12:15:57.213-0700 I  NETWORK  [listener] connection accepted from 192.168.122.19:58454 #74 (40 connections now open)
2020-05-08T12:15:57.214-0700 I  NETWORK  [conn73] received client metadata from 192.168.122.19:58452 conn73: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:57.214-0700 I  NETWORK  [listener] connection accepted from 192.168.122.15:33990 #75 (41 connections now open)
2020-05-08T12:15:57.214-0700 I  NETWORK  [conn74] received client metadata from 192.168.122.19:58454 conn74: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:57.214-0700 I  NETWORK  [listener] connection accepted from 192.168.122.15:33992 #76 (42 connections now open)
2020-05-08T12:15:57.214-0700 I  NETWORK  [conn75] received client metadata from 192.168.122.15:33990 conn75: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:57.215-0700 I  NETWORK  [conn76] received client metadata from 192.168.122.15:33992 conn76: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:57.315-0700 I  NETWORK  [listener] connection accepted from 192.168.122.12:47134 #77 (43 connections now open)
2020-05-08T12:15:57.315-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:45016 #78 (44 connections now open)
2020-05-08T12:15:57.315-0700 I  NETWORK  [conn77] received client metadata from 192.168.122.12:47134 conn77: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:57.316-0700 I  NETWORK  [conn78] received client metadata from 192.168.122.13:45016 conn78: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:57.367-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:15:57.369-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard2
2020-05-08T12:15:57.669-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:45048 #79 (45 connections now open)
2020-05-08T12:15:57.670-0700 I  NETWORK  [conn79] received client metadata from 192.168.122.13:45048 conn79: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:57.713-0700 I  REPL     [replexec-3] Member n1:27019 is now in state RS_DOWN - no response within election timeout period
2020-05-08T12:15:57.867-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:15:58.366-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:15:58.561-0700 I  NETWORK  [conn4] end connection 192.168.122.11:53752 (44 connections now open)
2020-05-08T12:15:58.563-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:45078 #80 (45 connections now open)
2020-05-08T12:15:58.563-0700 I  NETWORK  [conn80] received client metadata from 192.168.122.13:45078 conn80: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:15:58.565-0700 I  SHARDING [TransactionCoordinator] Marking collection config.transaction_coordinators as collection version: <unsharded>
2020-05-08T12:15:58.565-0700 I  COMMAND  [conn57] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965356, 2), signature: { hash: BinData(0, 9F925EB904E3A38FAD0A052ED1B3BB6BA45D722B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 1697ms
2020-05-08T12:15:58.565-0700 I  COMMAND  [conn59] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n7:27017" }, u: { $set: { _id: "n7:27017", ping: new Date(1588965353862), up: 10, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965356, 2), signature: { hash: BinData(0, 9F925EB904E3A38FAD0A052ED1B3BB6BA45D722B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1 } storage:{} protocol:op_msg 1697ms
2020-05-08T12:15:58.565-0700 I  COMMAND  [conn71] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965356, 2), signature: { hash: BinData(0, 9F925EB904E3A38FAD0A052ED1B3BB6BA45D722B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 1376ms
2020-05-08T12:15:58.565-0700 I  COMMAND  [conn78] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965356, 3), signature: { hash: BinData(0, 9F925EB904E3A38FAD0A052ED1B3BB6BA45D722B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 1249ms
2020-05-08T12:15:58.565-0700 I  COMMAND  [conn73] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n9:27017" }, u: { $set: { _id: "n9:27017", ping: new Date(1588965353709), up: 10, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965356, 2), signature: { hash: BinData(0, 9F925EB904E3A38FAD0A052ED1B3BB6BA45D722B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 1351ms
2020-05-08T12:15:58.566-0700 I  COMMAND  [conn56] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n4:27017" }, u: { $set: { _id: "n4:27017", ping: new Date(1588965353862), up: 10, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965354, 12), signature: { hash: BinData(0, 7499466E38233AF932612F5F00B68E2145F32CCD), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 1698ms
2020-05-08T12:15:58.566-0700 I  COMMAND  [conn53] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965354, 12), signature: { hash: BinData(0, 7499466E38233AF932612F5F00B68E2145F32CCD), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 1698ms
2020-05-08T12:15:58.566-0700 I  COMMAND  [conn20] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n1:27017" }, u: { $set: { _id: "n1:27017", ping: new Date(1588965353858), up: 10, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965356, 2), signature: { hash: BinData(0, 9F925EB904E3A38FAD0A052ED1B3BB6BA45D722B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1 } storage:{} protocol:op_msg 1379ms
2020-05-08T12:15:58.566-0700 I  COMMAND  [conn22] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n3:27017" }, u: { $set: { _id: "n3:27017", ping: new Date(1588965353857), up: 10, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965356, 3), signature: { hash: BinData(0, 9F925EB904E3A38FAD0A052ED1B3BB6BA45D722B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 1379ms
2020-05-08T12:15:58.566-0700 I  TXN      [TransactionCoordinator] Need to resume coordinating commit for 0 transactions
2020-05-08T12:15:58.566-0700 I  COMMAND  [conn19] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965356, 3), signature: { hash: BinData(0, 9F925EB904E3A38FAD0A052ED1B3BB6BA45D722B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 1539ms
2020-05-08T12:15:58.566-0700 I  COMMAND  [conn54] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n8:27017" }, u: { $set: { _id: "n8:27017", ping: new Date(1588965353862), up: 10, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965356, 2), signature: { hash: BinData(0, 9F925EB904E3A38FAD0A052ED1B3BB6BA45D722B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 1698ms
2020-05-08T12:15:58.566-0700 I  COMMAND  [conn21] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965356, 2), signature: { hash: BinData(0, 9F925EB904E3A38FAD0A052ED1B3BB6BA45D722B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 1379ms
2020-05-08T12:15:58.566-0700 I  COMMAND  [conn70] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n2:27017" }, u: { $set: { _id: "n2:27017", ping: new Date(1588965353709), up: 10, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965356, 3), signature: { hash: BinData(0, 9F925EB904E3A38FAD0A052ED1B3BB6BA45D722B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 1377ms
2020-05-08T12:15:58.566-0700 I  TXN      [TransactionCoordinator] Incoming coordinateCommit requests are now enabled
2020-05-08T12:15:58.566-0700 I  COMMAND  [conn32] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n5:27017" }, u: { $set: { _id: "n5:27017", ping: new Date(1588965353709), up: 10, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965356, 2), signature: { hash: BinData(0, 9F925EB904E3A38FAD0A052ED1B3BB6BA45D722B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 1352ms
2020-05-08T12:15:58.566-0700 I  COMMAND  [conn33] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965356, 3), signature: { hash: BinData(0, 9F925EB904E3A38FAD0A052ED1B3BB6BA45D722B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 1539ms
2020-05-08T12:15:58.566-0700 I  COMMAND  [conn77] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965356, 3), signature: { hash: BinData(0, 9F925EB904E3A38FAD0A052ED1B3BB6BA45D722B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 1250ms
2020-05-08T12:15:58.566-0700 I  COMMAND  [conn76] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965356, 2), signature: { hash: BinData(0, 9F925EB904E3A38FAD0A052ED1B3BB6BA45D722B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 1350ms
2020-05-08T12:15:58.566-0700 I  COMMAND  [conn24] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n6:27017" }, u: { $set: { _id: "n6:27017", ping: new Date(1588965353857), up: 10, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965354, 12), signature: { hash: BinData(0, 7499466E38233AF932612F5F00B68E2145F32CCD), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } storage:{} protocol:op_msg 1204ms
2020-05-08T12:15:58.566-0700 I  COMMAND  [conn75] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965356, 2), signature: { hash: BinData(0, 9F925EB904E3A38FAD0A052ED1B3BB6BA45D722B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 1350ms
2020-05-08T12:15:58.567-0700 I  COMMAND  [conn74] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965356, 2), signature: { hash: BinData(0, 9F925EB904E3A38FAD0A052ED1B3BB6BA45D722B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 1351ms
2020-05-08T12:15:58.566-0700 I  COMMAND  [conn72] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965356, 2), signature: { hash: BinData(0, 9F925EB904E3A38FAD0A052ED1B3BB6BA45D722B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965350, 10), t: 1 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 1377ms
2020-05-08T12:15:58.714-0700 I  REPL     [replexec-3] Member n1:27019 is now in state SECONDARY
2020-05-08T12:15:58.866-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:15:59.061-0700 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to n1:27019: InvalidSyncSource: Sync source was cleared. Was n1:27019
2020-05-08T12:15:59.366-0700 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs_shard1 is rs_shard1/n4:27018,n5:27018,n6:27018
2020-05-08T12:15:59.367-0700 I  CONNPOOL [ShardRegistry] Connecting to n4:27018
2020-05-08T12:15:59.370-0700 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs_shard2 is rs_shard2/n7:27018,n8:27018,n9:27018
2020-05-08T12:15:59.370-0700 I  CONNPOOL [ShardRegistry] Connecting to n8:27018
2020-05-08T12:15:59.836-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:53442 #83 (46 connections now open)
2020-05-08T12:15:59.836-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:53448 #84 (47 connections now open)
2020-05-08T12:15:59.837-0700 I  NETWORK  [conn83] received client metadata from 192.168.122.1:53442 conn83: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:15:59.837-0700 I  NETWORK  [conn84] received client metadata from 192.168.122.1:53448 conn84: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:15:59.840-0700 I  NETWORK  [conn83] end connection 192.168.122.1:53442 (46 connections now open)
2020-05-08T12:15:59.841-0700 I  NETWORK  [conn84] end connection 192.168.122.1:53448 (45 connections now open)
2020-05-08T12:15:59.841-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:55252 #85 (46 connections now open)
2020-05-08T12:15:59.841-0700 I  NETWORK  [conn85] received client metadata from 192.168.122.11:55252 conn85: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:00.663-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:53520 #86 (47 connections now open)
2020-05-08T12:16:00.664-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:53526 #87 (48 connections now open)
2020-05-08T12:16:00.664-0700 I  NETWORK  [conn86] received client metadata from 192.168.122.1:53520 conn86: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:00.664-0700 I  NETWORK  [conn87] received client metadata from 192.168.122.1:53526 conn87: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:00.667-0700 I  NETWORK  [conn86] end connection 192.168.122.1:53520 (47 connections now open)
2020-05-08T12:16:00.667-0700 I  NETWORK  [conn87] end connection 192.168.122.1:53526 (46 connections now open)
2020-05-08T12:16:01.481-0700 I  REPL     [replexec-2] Member n3:27019 is now in state RS_DOWN - no response within election timeout period
2020-05-08T12:16:01.553-0700 I  REPL     [replexec-3] Member n1:27019 is now in state RS_DOWN - no response within election timeout period
2020-05-08T12:16:01.553-0700 I  REPL     [replexec-3] can't see a majority of the set, relinquishing primary
2020-05-08T12:16:01.553-0700 I  REPL     [replexec-3] Stepping down from primary in response to heartbeat
2020-05-08T12:16:01.553-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:16:01.553-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:16:01.553-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 0, userOpsRunning: 0 }
2020-05-08T12:16:01.553-0700 I  REPL     [replexec-3] transition to SECONDARY from PRIMARY
2020-05-08T12:16:01.553-0700 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-05-08T12:16:01.554-0700 I  REPL     [rsBackgroundSync] could not find member to sync from
2020-05-08T12:16:01.554-0700 I  CONNPOOL [Replication] Connecting to n3:27019
2020-05-08T12:16:01.554-0700 I  CONNPOOL [Replication] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:16:01.555-0700 I  CONNPOOL [Replication] Ending connection to host n3:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:16:01.555-0700 I  REPL     [replexec-3] Member n1:27019 is now in state PRIMARY
2020-05-08T12:16:01.556-0700 I  REPL     [replexec-0] Member n3:27019 is now in state SECONDARY
2020-05-08T12:16:01.889-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:55828 #89 (47 connections now open)
2020-05-08T12:16:01.889-0700 I  NETWORK  [conn89] received client metadata from 192.168.122.11:55828 conn89: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:01.925-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:55830 #90 (48 connections now open)
2020-05-08T12:16:01.925-0700 I  NETWORK  [conn90] received client metadata from 192.168.122.11:55830 conn90: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:02.369-0700 I  ELECTION [conn85] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 4, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965357, 6), t: 4 } }
2020-05-08T12:16:02.369-0700 I  ELECTION [conn85] Sending vote response: { term: 5, voteGranted: false, reason: "candidate's term (4) is lower than mine (5)" }
2020-05-08T12:16:02.369-0700 I  NETWORK  [conn85] end connection 192.168.122.11:55252 (47 connections now open)
2020-05-08T12:16:02.555-0700 I  REPL     [rsBackgroundSync] sync source candidate: n1:27019
2020-05-08T12:16:02.557-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n1:27019
2020-05-08T12:16:06.420-0700 I  NETWORK  [listener] connection accepted from 192.168.122.15:34456 #91 (48 connections now open)
2020-05-08T12:16:06.421-0700 I  NETWORK  [conn91] received client metadata from 192.168.122.15:34456 conn91: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:06.714-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:53762 #92 (49 connections now open)
2020-05-08T12:16:06.715-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:53768 #93 (50 connections now open)
2020-05-08T12:16:06.715-0700 I  NETWORK  [conn92] received client metadata from 192.168.122.1:53762 conn92: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:06.715-0700 I  NETWORK  [conn93] received client metadata from 192.168.122.1:53768 conn93: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:06.719-0700 I  NETWORK  [conn92] end connection 192.168.122.1:53762 (49 connections now open)
2020-05-08T12:16:06.719-0700 I  NETWORK  [conn93] end connection 192.168.122.1:53768 (48 connections now open)
2020-05-08T12:16:07.618-0700 I  ELECTION [replexec-0] Starting an election, since we've seen no PRIMARY in the past 1000ms
2020-05-08T12:16:07.618-0700 I  ELECTION [replexec-0] conducting a dry run election to see if we could be elected. current term: 5
2020-05-08T12:16:07.618-0700 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 708 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 5, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965361, 1), t: 5 } }
2020-05-08T12:16:07.618-0700 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 709 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 5, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965361, 1), t: 5 } }
2020-05-08T12:16:07.619-0700 I  ELECTION [replexec-2] VoteRequester(term 5 dry run) received a yes vote from n3:27019; response message: { term: 5, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000003') }, lastCommittedOpTime: Timestamp(1588965361, 1), $clusterTime: { clusterTime: Timestamp(1588965366, 14), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965361, 1) }
2020-05-08T12:16:07.620-0700 I  ELECTION [replexec-2] dry election run succeeded, running for election in term 6
2020-05-08T12:16:07.620-0700 I  CONNPOOL [Replication] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-05-08T12:16:07.620-0700 I  CONNPOOL [Replication] Connecting to n1:27019
2020-05-08T12:16:07.632-0700 I  REPL     [replexec-2] Scheduling remote command request for vote request: RemoteCommand 710 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 6, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965361, 1), t: 5 } }
2020-05-08T12:16:07.632-0700 I  REPL     [replexec-2] Scheduling remote command request for vote request: RemoteCommand 711 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 6, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965361, 1), t: 5 } }
2020-05-08T12:16:07.655-0700 I  ELECTION [replexec-3] VoteRequester(term 6) received a yes vote from n3:27019; response message: { term: 6, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000003') }, lastCommittedOpTime: Timestamp(1588965361, 1), $clusterTime: { clusterTime: Timestamp(1588965366, 14), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965361, 1) }
2020-05-08T12:16:07.655-0700 I  ELECTION [replexec-3] election succeeded, assuming primary role in term 6
2020-05-08T12:16:07.655-0700 I  REPL     [replexec-3] transition to PRIMARY from SECONDARY
2020-05-08T12:16:07.655-0700 I  REPL     [replexec-3] Resetting sync source to empty, which was n1:27019
2020-05-08T12:16:07.655-0700 I  REPL     [replexec-3] Entering primary catch-up mode.
2020-05-08T12:16:08.655-0700 I  REPL     [replexec-0] Catchup timed out after becoming primary.
2020-05-08T12:16:08.655-0700 I  REPL     [replexec-0] Exited primary catch-up mode.
2020-05-08T12:16:08.655-0700 I  REPL     [replexec-0] Stopping replication producer
2020-05-08T12:16:08.655-0700 I  REPL     [replexec-5] Member n1:27019 is now in state RS_DOWN - Couldn't get a connection within the time limit
2020-05-08T12:16:08.655-0700 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2020-05-08T12:16:08.655-0700 I  CONNPOOL [RS] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:16:08.655-0700 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 6
2020-05-08T12:16:08.656-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:16:08.656-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:16:08.656-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 1 }
2020-05-08T12:16:08.658-0700 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-05-08T12:16:08.658-0700 I  SHARDING [Balancer] CSRS balancer is starting
2020-05-08T12:16:08.658-0700 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-05-08T12:16:08.659-0700 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-05-08T12:16:08.659-0700 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-05-08T12:16:08.662-0700 W  QUERY    [conn80] GetMore command executor error: FAILURE, status: CappedPositionLost: CollectionScan died due to failure to restore tailable cursor position. Last seen record id: RecordId(6824554290036604929), stats: { stage: "COLLSCAN", nReturned: 3, executionTimeMillisEstimate: 0, works: 74, advanced: 3, needTime: 35, needYield: 0, saveState: 35, restoreState: 35, isEOF: 0, direction: "forward", docsExamined: 3 }
2020-05-08T12:16:08.754-0700 I  NETWORK  [listener] connection accepted from 192.168.122.18:40890 #94 (49 connections now open)
2020-05-08T12:16:08.754-0700 I  NETWORK  [conn94] received client metadata from 192.168.122.18:40890 conn94: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:09.657-0700 I  COMMAND  [conn74] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965368, 123), signature: { hash: BinData(0, 0D77DEA65796E3FE85BB1DF41657937B4B5F3557), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965361, 1), t: 5 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } storage:{} protocol:op_msg 924ms
2020-05-08T12:16:09.657-0700 I  TXN      [TransactionCoordinator] Need to resume coordinating commit for 0 transactions
2020-05-08T12:16:09.657-0700 I  TXN      [TransactionCoordinator] Incoming coordinateCommit requests are now enabled
2020-05-08T12:16:09.691-0700 I  COMMAND  [conn23] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n6:27017" }, u: { $set: { _id: "n6:27017", ping: new Date(1588965368570), up: 24, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965369, 10), signature: { hash: BinData(0, CB35EDF272148C063FA23EA850884726E8CD8911), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965361, 1), t: 5 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1 } storage:{} protocol:op_msg 629ms
2020-05-08T12:16:09.692-0700 I  COMMAND  [conn54] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n8:27017" }, u: { $set: { _id: "n8:27017", ping: new Date(1588965368568), up: 24, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965368, 135), signature: { hash: BinData(0, 0D77DEA65796E3FE85BB1DF41657937B4B5F3557), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965361, 1), t: 5 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 938ms
2020-05-08T12:16:09.692-0700 I  COMMAND  [conn53] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965368, 142), signature: { hash: BinData(0, 0D77DEA65796E3FE85BB1DF41657937B4B5F3557), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965361, 1), t: 5 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } storage:{} protocol:op_msg 913ms
2020-05-08T12:16:09.692-0700 I  COMMAND  [conn57] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n7:27017" }, u: { $set: { _id: "n7:27017", ping: new Date(1588965368569), up: 24, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965369, 13), signature: { hash: BinData(0, CB35EDF272148C063FA23EA850884726E8CD8911), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965361, 1), t: 5 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 630ms
2020-05-08T12:16:09.692-0700 I  COMMAND  [conn75] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965369, 8), signature: { hash: BinData(0, CB35EDF272148C063FA23EA850884726E8CD8911), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965361, 1), t: 5 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 630ms
2020-05-08T12:16:09.692-0700 I  COMMAND  [conn32] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n5:27017" }, u: { $set: { _id: "n5:27017", ping: new Date(1588965368569), up: 24, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965369, 8), signature: { hash: BinData(0, CB35EDF272148C063FA23EA850884726E8CD8911), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965361, 1), t: 5 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1 } storage:{} protocol:op_msg 631ms
2020-05-08T12:16:09.692-0700 I  COMMAND  [conn24] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965369, 10), signature: { hash: BinData(0, CB35EDF272148C063FA23EA850884726E8CD8911), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965361, 1), t: 5 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 630ms
2020-05-08T12:16:09.692-0700 I  COMMAND  [conn73] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n9:27017" }, u: { $set: { _id: "n9:27017", ping: new Date(1588965368568), up: 24, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965368, 123), signature: { hash: BinData(0, 0D77DEA65796E3FE85BB1DF41657937B4B5F3557), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965361, 1), t: 5 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 959ms
2020-05-08T12:16:09.692-0700 I  COMMAND  [conn94] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965368, 135), signature: { hash: BinData(0, 0D77DEA65796E3FE85BB1DF41657937B4B5F3557), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965361, 1), t: 5 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 936ms
2020-05-08T12:16:09.692-0700 I  COMMAND  [conn56] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n4:27017" }, u: { $set: { _id: "n4:27017", ping: new Date(1588965368568), up: 24, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965368, 142), signature: { hash: BinData(0, 0D77DEA65796E3FE85BB1DF41657937B4B5F3557), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965361, 1), t: 5 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1 } storage:{} protocol:op_msg 914ms
2020-05-08T12:16:09.692-0700 I  COMMAND  [conn59] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965369, 13), signature: { hash: BinData(0, CB35EDF272148C063FA23EA850884726E8CD8911), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965361, 1), t: 5 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 631ms
2020-05-08T12:16:11.233-0700 I  NETWORK  [conn90] end connection 192.168.122.11:55830 (48 connections now open)
2020-05-08T12:16:11.425-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:56190 #95 (49 connections now open)
2020-05-08T12:16:11.425-0700 I  NETWORK  [conn95] received client metadata from 192.168.122.11:56190 conn95: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:11.563-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:56268 #96 (50 connections now open)
2020-05-08T12:16:11.563-0700 I  NETWORK  [conn96] received client metadata from 192.168.122.11:56268 conn96: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:11.571-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:56270 #97 (51 connections now open)
2020-05-08T12:16:11.571-0700 I  NETWORK  [conn97] received client metadata from 192.168.122.11:56270 conn97: { driver: { name: "MongoDB Internal Client", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:11.619-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54020 #98 (52 connections now open)
2020-05-08T12:16:11.619-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54028 #99 (53 connections now open)
2020-05-08T12:16:11.619-0700 I  NETWORK  [conn98] received client metadata from 192.168.122.1:54020 conn98: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:11.620-0700 I  NETWORK  [conn99] received client metadata from 192.168.122.1:54028 conn99: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:11.623-0700 I  NETWORK  [conn98] end connection 192.168.122.1:54020 (52 connections now open)
2020-05-08T12:16:11.623-0700 I  NETWORK  [conn99] end connection 192.168.122.1:54028 (51 connections now open)
2020-05-08T12:16:11.921-0700 I  NETWORK  [conn97] end connection 192.168.122.11:56270 (50 connections now open)
2020-05-08T12:16:12.011-0700 I  COMMAND  [conn19] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n3:27017" }, u: { $set: { _id: "n3:27017", ping: new Date(1588965368569), up: 24, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965371, 14), signature: { hash: BinData(0, B603E1231135BE53F998376580FB5F2BE6FDDC3D), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965369, 16), t: 6 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 103ms
2020-05-08T12:16:12.555-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54082 #100 (51 connections now open)
2020-05-08T12:16:12.555-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54088 #101 (52 connections now open)
2020-05-08T12:16:12.555-0700 I  NETWORK  [conn100] received client metadata from 192.168.122.1:54082 conn100: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:12.555-0700 I  NETWORK  [conn101] received client metadata from 192.168.122.1:54088 conn101: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:12.559-0700 I  NETWORK  [conn100] end connection 192.168.122.1:54082 (51 connections now open)
2020-05-08T12:16:12.559-0700 I  NETWORK  [conn101] end connection 192.168.122.1:54088 (50 connections now open)
2020-05-08T12:16:12.997-0700 I  ELECTION [conn95] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 6, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965371, 16), t: 6 } }
2020-05-08T12:16:12.998-0700 I  ELECTION [conn95] Sending vote response: { term: 6, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965371, 16), t: 6 }, my last applied OpTime: { ts: Timestam..." }
2020-05-08T12:16:13.042-0700 I  COMMAND  [conn23] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n6:27017:1588965341:-562014436095676681" }, update: { $set: { ping: new Date(1588965372609) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965371, 16), signature: { hash: BinData(0, B603E1231135BE53F998376580FB5F2BE6FDDC3D), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965371, 16), t: 6 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:627 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 432ms
2020-05-08T12:16:13.042-0700 I  COMMAND  [conn57] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n7:27017:1588965341:6871791939861018853" }, update: { $set: { ping: new Date(1588965372609) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965372, 17), signature: { hash: BinData(0, A1460814075450AB496B166EE03BF8D41CBA3AC0), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965371, 16), t: 6 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:627 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 432ms
2020-05-08T12:16:13.042-0700 I  COMMAND  [conn56] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n4:27017:1588965341:3187934225528271170" }, update: { $set: { ping: new Date(1588965372608) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965371, 16), signature: { hash: BinData(0, B603E1231135BE53F998376580FB5F2BE6FDDC3D), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965371, 16), t: 6 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:627 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } storage:{} protocol:op_msg 432ms
2020-05-08T12:16:13.051-0700 I  COMMAND  [conn53] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965372, 6), signature: { hash: BinData(0, A1460814075450AB496B166EE03BF8D41CBA3AC0), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965371, 16), t: 6 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 343ms
2020-05-08T12:16:13.051-0700 I  COMMAND  [conn75] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965372, 6), signature: { hash: BinData(0, A1460814075450AB496B166EE03BF8D41CBA3AC0), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965371, 16), t: 6 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 344ms
2020-05-08T12:16:13.051-0700 I  COMMAND  [conn54] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n8:27017:1588965341:-4138072281809841771" }, update: { $set: { ping: new Date(1588965372609) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965372, 18), signature: { hash: BinData(0, A1460814075450AB496B166EE03BF8D41CBA3AC0), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965371, 16), t: 6 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:628 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 440ms
2020-05-08T12:16:13.051-0700 I  COMMAND  [conn73] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n9:27017:1588965341:2106346409928220643" }, update: { $set: { ping: new Date(1588965372609) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965372, 18), signature: { hash: BinData(0, A1460814075450AB496B166EE03BF8D41CBA3AC0), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965371, 16), t: 6 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:627 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } storage:{} protocol:op_msg 440ms
2020-05-08T12:16:13.051-0700 I  COMMAND  [conn32] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n5:27017:1588965341:-4005854753316312821" }, update: { $set: { ping: new Date(1588965372609) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965372, 6), signature: { hash: BinData(0, A1460814075450AB496B166EE03BF8D41CBA3AC0), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965371, 16), t: 6 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:628 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } storage:{} protocol:op_msg 441ms
2020-05-08T12:16:13.051-0700 I  COMMAND  [conn70] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n2:27017:1588965341:-7408192429411933944" }, update: { $set: { ping: new Date(1588965372609) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965372, 8), signature: { hash: BinData(0, A1460814075450AB496B166EE03BF8D41CBA3AC0), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965371, 16), t: 6 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:628 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 441ms
2020-05-08T12:16:13.181-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54120 #102 (51 connections now open)
2020-05-08T12:16:13.182-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54126 #103 (52 connections now open)
2020-05-08T12:16:13.182-0700 I  NETWORK  [conn102] received client metadata from 192.168.122.1:54120 conn102: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:13.182-0700 I  NETWORK  [conn103] received client metadata from 192.168.122.1:54126 conn103: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:13.185-0700 I  NETWORK  [conn102] end connection 192.168.122.1:54120 (51 connections now open)
2020-05-08T12:16:13.185-0700 I  NETWORK  [conn103] end connection 192.168.122.1:54126 (50 connections now open)
2020-05-08T12:16:13.665-0700 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to n1:27019: InvalidSyncSource: Sync source was cleared. Was n1:27019
2020-05-08T12:16:13.698-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54160 #104 (51 connections now open)
2020-05-08T12:16:13.699-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54166 #105 (52 connections now open)
2020-05-08T12:16:13.699-0700 I  NETWORK  [conn104] received client metadata from 192.168.122.1:54160 conn104: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:13.699-0700 I  NETWORK  [conn105] received client metadata from 192.168.122.1:54166 conn105: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:13.702-0700 I  NETWORK  [conn104] end connection 192.168.122.1:54160 (51 connections now open)
2020-05-08T12:16:13.702-0700 I  NETWORK  [conn105] end connection 192.168.122.1:54166 (50 connections now open)
2020-05-08T12:16:14.326-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54266 #106 (51 connections now open)
2020-05-08T12:16:14.326-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54272 #107 (52 connections now open)
2020-05-08T12:16:14.326-0700 I  NETWORK  [conn106] received client metadata from 192.168.122.1:54266 conn106: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:14.327-0700 I  NETWORK  [conn107] received client metadata from 192.168.122.1:54272 conn107: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:14.330-0700 I  NETWORK  [conn106] end connection 192.168.122.1:54266 (51 connections now open)
2020-05-08T12:16:14.330-0700 I  NETWORK  [conn107] end connection 192.168.122.1:54272 (50 connections now open)
2020-05-08T12:16:15.052-0700 I  REPL     [replexec-1] Member n3:27019 is now in state RS_DOWN - no response within election timeout period
2020-05-08T12:16:15.052-0700 I  REPL     [replexec-1] can't see a majority of the set, relinquishing primary
2020-05-08T12:16:15.052-0700 I  REPL     [replexec-1] Stepping down from primary in response to heartbeat
2020-05-08T12:16:15.052-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:16:15.052-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:16:15.053-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 0, userOpsRunning: 0 }
2020-05-08T12:16:15.053-0700 I  REPL     [replexec-1] transition to SECONDARY from PRIMARY
2020-05-08T12:16:15.053-0700 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-05-08T12:16:15.649-0700 I  ELECTION [conn95] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 6, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965373, 1), t: 6 } }
2020-05-08T12:16:15.649-0700 I  ELECTION [conn95] Sending vote response: { term: 6, voteGranted: true, reason: "" }
2020-05-08T12:16:15.649-0700 I  NETWORK  [conn95] end connection 192.168.122.11:56190 (49 connections now open)
2020-05-08T12:16:15.809-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:56350 #108 (50 connections now open)
2020-05-08T12:16:15.809-0700 I  NETWORK  [conn108] received client metadata from 192.168.122.11:56350 conn108: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:15.871-0700 I  REPL     [replexec-0] Member n3:27019 is now in state SECONDARY
2020-05-08T12:16:16.030-0700 I  NETWORK  [conn96] end connection 192.168.122.11:56268 (49 connections now open)
2020-05-08T12:16:16.033-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:56540 #109 (50 connections now open)
2020-05-08T12:16:16.033-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:56538 #110 (51 connections now open)
2020-05-08T12:16:16.033-0700 I  NETWORK  [conn109] received client metadata from 192.168.122.11:56540 conn109: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:16.033-0700 I  NETWORK  [conn110] received client metadata from 192.168.122.11:56538 conn110: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:16.225-0700 I  NETWORK  [conn89] end connection 192.168.122.11:55828 (50 connections now open)
2020-05-08T12:16:17.011-0700 I  ELECTION [replexec-3] Starting an election, since we've seen no PRIMARY in the past 1000ms
2020-05-08T12:16:17.011-0700 I  ELECTION [replexec-3] conducting a dry run election to see if we could be elected. current term: 7
2020-05-08T12:16:17.011-0700 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 729 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 7, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965373, 1), t: 6 } }
2020-05-08T12:16:17.011-0700 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 730 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 7, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965373, 1), t: 6 } }
2020-05-08T12:16:17.012-0700 I  ELECTION [replexec-4] VoteRequester(term 7 dry run) received a yes vote from n3:27019; response message: { term: 7, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000003') }, lastCommittedOpTime: Timestamp(1588965373, 1), $clusterTime: { clusterTime: Timestamp(1588965376, 4), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965373, 1) }
2020-05-08T12:16:17.012-0700 I  ELECTION [replexec-1] dry election run succeeded, running for election in term 8
2020-05-08T12:16:17.017-0700 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 731 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 8, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965373, 1), t: 6 } }
2020-05-08T12:16:17.017-0700 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 732 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 8, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965373, 1), t: 6 } }
2020-05-08T12:16:17.020-0700 I  ELECTION [replexec-3] VoteRequester(term 8) received a yes vote from n3:27019; response message: { term: 8, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000003') }, lastCommittedOpTime: Timestamp(1588965373, 1), $clusterTime: { clusterTime: Timestamp(1588965376, 4), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965373, 1) }
2020-05-08T12:16:17.020-0700 I  ELECTION [replexec-3] election succeeded, assuming primary role in term 8
2020-05-08T12:16:17.020-0700 I  REPL     [replexec-3] transition to PRIMARY from SECONDARY
2020-05-08T12:16:17.021-0700 I  REPL     [replexec-3] Resetting sync source to empty, which was :27017
2020-05-08T12:16:17.021-0700 I  REPL     [replexec-3] Entering primary catch-up mode.
2020-05-08T12:16:17.534-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54352 #111 (51 connections now open)
2020-05-08T12:16:17.534-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54354 #112 (52 connections now open)
2020-05-08T12:16:17.535-0700 I  NETWORK  [conn111] received client metadata from 192.168.122.1:54352 conn111: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:17.535-0700 I  NETWORK  [conn112] received client metadata from 192.168.122.1:54354 conn112: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:17.538-0700 I  NETWORK  [conn111] end connection 192.168.122.1:54352 (51 connections now open)
2020-05-08T12:16:17.538-0700 I  NETWORK  [conn112] end connection 192.168.122.1:54354 (50 connections now open)
2020-05-08T12:16:17.655-0700 I  REPL     [replexec-3] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1588965373, 1), t: 6 }. My Last Applied: { ts: Timestamp(1588965373, 1), t: 6 }
2020-05-08T12:16:17.655-0700 I  REPL     [replexec-3] Exited primary catch-up mode.
2020-05-08T12:16:17.655-0700 I  REPL     [replexec-3] Stopping replication producer
2020-05-08T12:16:17.655-0700 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 8
2020-05-08T12:16:17.655-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:16:17.655-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:16:17.655-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 0 }
2020-05-08T12:16:17.657-0700 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-05-08T12:16:17.657-0700 I  SHARDING [Balancer] CSRS balancer is starting
2020-05-08T12:16:17.657-0700 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-05-08T12:16:17.658-0700 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-05-08T12:16:17.658-0700 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-05-08T12:16:17.778-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:56616 #113 (51 connections now open)
2020-05-08T12:16:17.778-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:56618 #114 (52 connections now open)
2020-05-08T12:16:17.778-0700 I  NETWORK  [conn113] received client metadata from 192.168.122.11:56616 conn113: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:17.778-0700 I  NETWORK  [conn114] received client metadata from 192.168.122.11:56618 conn114: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:18.116-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54384 #115 (53 connections now open)
2020-05-08T12:16:18.117-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54390 #116 (54 connections now open)
2020-05-08T12:16:18.117-0700 I  NETWORK  [conn115] received client metadata from 192.168.122.1:54384 conn115: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:18.117-0700 I  NETWORK  [conn116] received client metadata from 192.168.122.1:54390 conn116: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:18.120-0700 I  NETWORK  [conn115] end connection 192.168.122.1:54384 (53 connections now open)
2020-05-08T12:16:18.120-0700 I  NETWORK  [conn116] end connection 192.168.122.1:54390 (52 connections now open)
2020-05-08T12:16:18.728-0700 I  REPL     [replexec-4] Member n3:27019 is now in state RS_DOWN - no response within election timeout period
2020-05-08T12:16:18.728-0700 I  REPL     [replexec-4] can't see a majority of the set, relinquishing primary
2020-05-08T12:16:18.728-0700 I  REPL     [replexec-4] Stepping down from primary in response to heartbeat
2020-05-08T12:16:18.728-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:16:18.729-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:16:18.729-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 18, userOpsRunning: 0 }
2020-05-08T12:16:18.730-0700 W  COMMAND  [conn114] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:18.730-0700 I  COMMAND  [conn114] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965376, 1), signature: { hash: BinData(0, CF5F7BCE27ED3551F097F7D03B9C76B2D5A96785), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965373, 1), t: 6 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 950ms
2020-05-08T12:16:18.730-0700 W  COMMAND  [conn21] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:18.730-0700 I  COMMAND  [conn21] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n1:27017" }, u: { $set: { _id: "n1:27017", ping: new Date(1588965370372), up: 26, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965376, 1), signature: { hash: BinData(0, CF5F7BCE27ED3551F097F7D03B9C76B2D5A96785), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965373, 1), t: 6 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } protocol:op_msg 953ms
2020-05-08T12:16:18.731-0700 W  COMMAND  [conn39] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:18.731-0700 I  COMMAND  [conn39] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n7:27018:1588965345:4642009900937274884" }, update: { $set: { ping: new Date(1588965375583) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965378, 1), signature: { hash: BinData(0, DEFD6149DF6443D9DA8FED8D5728C5338B850ECE), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965373, 1), t: 6 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:745 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } protocol:op_msg 645ms
2020-05-08T12:16:18.731-0700 W  COMMAND  [conn53] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:18.731-0700 I  COMMAND  [conn53] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965376, 1), signature: { hash: BinData(0, CF5F7BCE27ED3551F097F7D03B9C76B2D5A96785), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965373, 1), t: 6 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 204ms
2020-05-08T12:16:18.731-0700 W  COMMAND  [conn47] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:18.731-0700 I  COMMAND  [conn47] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n8:27018:1588965345:-3830833260225672011" }, update: { $set: { ping: new Date(1588965375590) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965378, 1), signature: { hash: BinData(0, DEFD6149DF6443D9DA8FED8D5728C5338B850ECE), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965373, 1), t: 6 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:746 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } protocol:op_msg 639ms
2020-05-08T12:16:18.731-0700 W  COMMAND  [conn22] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:18.731-0700 I  COMMAND  [conn22] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965376, 4), signature: { hash: BinData(0, CF5F7BCE27ED3551F097F7D03B9C76B2D5A96785), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965373, 1), t: 6 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 925ms
2020-05-08T12:16:18.732-0700 W  COMMAND  [conn32] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:18.732-0700 I  COMMAND  [conn32] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965377, 2), signature: { hash: BinData(0, FB1AD7A2B10C571581820C222307125C53A9DC0E), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965373, 1), t: 6 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } protocol:op_msg 540ms
2020-05-08T12:16:18.732-0700 W  COMMAND  [conn70] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:18.732-0700 W  COMMAND  [conn48] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:18.732-0700 I  COMMAND  [conn70] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965376, 1), signature: { hash: BinData(0, CF5F7BCE27ED3551F097F7D03B9C76B2D5A96785), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965373, 1), t: 6 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 541ms
2020-05-08T12:16:18.732-0700 I  COMMAND  [conn48] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n6:27018:1588965345:-465166209935970491" }, update: { $set: { ping: new Date(1588965375397) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965377, 1), signature: { hash: BinData(0, FB1AD7A2B10C571581820C222307125C53A9DC0E), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965373, 1), t: 6 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:745 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 832ms
2020-05-08T12:16:18.732-0700 I  REPL     [replexec-4] transition to SECONDARY from PRIMARY
2020-05-08T12:16:18.732-0700 W  COMMAND  [conn20] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:18.732-0700 W  COMMAND  [conn23] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:18.732-0700 W  COMMAND  [conn73] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:18.732-0700 I  COMMAND  [conn23] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965376, 1), signature: { hash: BinData(0, CF5F7BCE27ED3551F097F7D03B9C76B2D5A96785), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965373, 1), t: 6 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 205ms
2020-05-08T12:16:18.732-0700 I  COMMAND  [conn20] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n1:27017:1588965341:7325128125104408104" }, update: { $set: { ping: new Date(1588965372609) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965376, 1), signature: { hash: BinData(0, CF5F7BCE27ED3551F097F7D03B9C76B2D5A96785), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965373, 1), t: 6 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:745 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 955ms
2020-05-08T12:16:18.733-0700 I  COMMAND  [conn73] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965376, 1), signature: { hash: BinData(0, CF5F7BCE27ED3551F097F7D03B9C76B2D5A96785), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965373, 1), t: 6 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 540ms
2020-05-08T12:16:18.733-0700 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-05-08T12:16:18.733-0700 W  COMMAND  [conn35] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:18.733-0700 W  COMMAND  [conn71] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:18.733-0700 W  COMMAND  [conn113] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:18.733-0700 I  COMMAND  [conn35] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n4:27018:1588965345:-5626666314136012223" }, update: { $set: { ping: new Date(1588965375389) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965377, 2), signature: { hash: BinData(0, FB1AD7A2B10C571581820C222307125C53A9DC0E), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965373, 1), t: 6 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:746 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 841ms
2020-05-08T12:16:18.733-0700 W  COMMAND  [conn42] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:18.733-0700 I  COMMAND  [conn71] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965376, 1), signature: { hash: BinData(0, CF5F7BCE27ED3551F097F7D03B9C76B2D5A96785), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965373, 1), t: 6 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 956ms
2020-05-08T12:16:18.733-0700 W  COMMAND  [conn91] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:18.733-0700 W  COMMAND  [conn72] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:18.733-0700 I  COMMAND  [conn113] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965376, 1), signature: { hash: BinData(0, CF5F7BCE27ED3551F097F7D03B9C76B2D5A96785), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965373, 1), t: 6 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 953ms
2020-05-08T12:16:18.733-0700 I  COMMAND  [conn42] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n9:27018:1588965345:-3626659247996741111" }, update: { $set: { ping: new Date(1588965375590) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965378, 1), signature: { hash: BinData(0, DEFD6149DF6443D9DA8FED8D5728C5338B850ECE), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965373, 1), t: 6 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:746 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 640ms
2020-05-08T12:16:18.733-0700 W  TXN      [TransactionCoordinator] Coordinator recovery failed and coordinateCommit requests will not be allowed :: caused by :: TransactionCoordinatorSteppingDown: operation was interrupted
2020-05-08T12:16:18.733-0700 I  COMMAND  [conn91] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n5:27018:1588965345:-3184880103931982133" }, update: { $set: { ping: new Date(1588965375397) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965377, 2), signature: { hash: BinData(0, FB1AD7A2B10C571581820C222307125C53A9DC0E), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965373, 1), t: 6 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:746 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 832ms
2020-05-08T12:16:18.733-0700 I  COMMAND  [conn72] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965376, 1), signature: { hash: BinData(0, CF5F7BCE27ED3551F097F7D03B9C76B2D5A96785), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965373, 1), t: 6 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 956ms
2020-05-08T12:16:19.870-0700 I  ELECTION [replexec-0] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:16:20.021-0700 I  CONNPOOL [Replication] Ending connection to host n3:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-05-08T12:16:20.021-0700 I  CONNPOOL [Replication] Connecting to n3:27019
2020-05-08T12:16:21.012-0700 I  ELECTION [replexec-3] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:16:22.077-0700 I  ELECTION [replexec-4] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:16:23.092-0700 I  ELECTION [replexec-0] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:16:24.232-0700 I  ELECTION [replexec-3] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:16:24.289-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:56850 #117 (53 connections now open)
2020-05-08T12:16:24.289-0700 I  NETWORK  [conn117] received client metadata from 192.168.122.11:56850 conn117: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:24.470-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54626 #118 (54 connections now open)
2020-05-08T12:16:24.471-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54632 #119 (55 connections now open)
2020-05-08T12:16:24.471-0700 I  NETWORK  [conn118] received client metadata from 192.168.122.1:54626 conn118: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:24.471-0700 I  NETWORK  [conn119] received client metadata from 192.168.122.1:54632 conn119: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:24.475-0700 I  NETWORK  [conn118] end connection 192.168.122.1:54626 (54 connections now open)
2020-05-08T12:16:24.475-0700 I  NETWORK  [conn119] end connection 192.168.122.1:54632 (53 connections now open)
2020-05-08T12:16:24.898-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54652 #120 (54 connections now open)
2020-05-08T12:16:24.899-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54658 #121 (55 connections now open)
2020-05-08T12:16:24.899-0700 I  NETWORK  [conn120] received client metadata from 192.168.122.1:54652 conn120: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:24.899-0700 I  NETWORK  [conn121] received client metadata from 192.168.122.1:54658 conn121: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:24.902-0700 I  NETWORK  [conn120] end connection 192.168.122.1:54652 (54 connections now open)
2020-05-08T12:16:24.903-0700 I  NETWORK  [conn121] end connection 192.168.122.1:54658 (53 connections now open)
2020-05-08T12:16:25.185-0700 I  NETWORK  [conn110] end connection 192.168.122.11:56538 (52 connections now open)
2020-05-08T12:16:25.287-0700 I  ELECTION [replexec-4] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:16:25.441-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:56648 #122 (53 connections now open)
2020-05-08T12:16:25.441-0700 I  NETWORK  [conn122] received client metadata from 192.168.122.11:56648 conn122: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:25.697-0700 I  ELECTION [conn109] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 8, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965376, 4), t: 7 } }
2020-05-08T12:16:25.697-0700 I  ELECTION [conn109] Sending vote response: { term: 8, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965376, 4), t: 7 }, my last applied OpTime: { ts: Timestamp..." }
2020-05-08T12:16:25.697-0700 I  NETWORK  [conn109] end connection 192.168.122.11:56540 (52 connections now open)
2020-05-08T12:16:26.209-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:56754 #123 (53 connections now open)
2020-05-08T12:16:26.209-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:56752 #124 (54 connections now open)
2020-05-08T12:16:26.209-0700 I  NETWORK  [conn123] received client metadata from 192.168.122.11:56754 conn123: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:26.210-0700 I  NETWORK  [conn124] received client metadata from 192.168.122.11:56752 conn124: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:26.353-0700 I  ELECTION [replexec-2] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:16:26.465-0700 I  NETWORK  [conn9] end connection 192.168.122.13:43308 (53 connections now open)
2020-05-08T12:16:26.977-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:56284 #125 (54 connections now open)
2020-05-08T12:16:26.977-0700 I  NETWORK  [conn125] received client metadata from 192.168.122.11:56284 conn125: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:27.235-0700 I  REPL     [replexec-3] Member n3:27019 is now in state SECONDARY
2020-05-08T12:16:27.655-0700 I  CONNPOOL [Replication] Connecting to n1:27019
2020-05-08T12:16:27.658-0700 I  REPL     [replexec-0] Member n1:27019 is now in state PRIMARY
2020-05-08T12:16:27.736-0700 I  REPL     [rsBackgroundSync] sync source candidate: n1:27019
2020-05-08T12:16:27.738-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n1:27019
2020-05-08T12:16:27.740-0700 I  REPL     [rsBackgroundSync] Starting rollback due to OplogStartMissing: Our last optime fetched: { ts: Timestamp(1588965378, 4), t: 8 }. source's GTE: { ts: Timestamp(1588965379, 2), t: 9 }
2020-05-08T12:16:27.740-0700 I  REPL     [rsBackgroundSync] Replication commit point: { ts: Timestamp(1588965373, 1), t: 6 }
2020-05-08T12:16:27.741-0700 I  REPL     [rsBackgroundSync] Rollback using 'recoverToStableTimestamp' method.
2020-05-08T12:16:27.741-0700 I  REPL     [rsBackgroundSync] Scheduling rollback (sync source: n1:27019)
2020-05-08T12:16:27.741-0700 I  ROLLBACK [rsBackgroundSync] transition to ROLLBACK
2020-05-08T12:16:27.741-0700 I  REPL     [rsBackgroundSync] State transition ops metrics: { lastStateTransition: "rollback", userOpsKilled: 4, userOpsRunning: 51 }
2020-05-08T12:16:27.741-0700 I  REPL     [rsBackgroundSync] transition to ROLLBACK from SECONDARY
2020-05-08T12:16:27.741-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 125
2020-05-08T12:16:27.741-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 124
2020-05-08T12:16:27.741-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 123
2020-05-08T12:16:27.741-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 122
2020-05-08T12:16:27.741-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 117
2020-05-08T12:16:27.741-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 114
2020-05-08T12:16:27.741-0700 I  COMMAND  [conn23] command config.$cmd command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp(1588965379, 197), t: 9 } }, limit: 1, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965379, 197), signature: { hash: BinData(0, 0F0E8F2DC5000EDA7C20D002E70E6B1B886733DE), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965379, 197), t: 9 } }, $db: "config" } numYields:0 ok:0 errMsg:"Error waiting for snapshot not less than { ts: Timestamp(1588965379, 197), t: 9 }, current relevant optime is { ts: Timestamp(1588965373, 1), t: 6 }. :: caused by :: operation was interrupted" errName:InterruptedDueToReplStateChange errCode:11602 reslen:765 locks:{} protocol:op_msg 8026ms
2020-05-08T12:16:27.741-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 113
2020-05-08T12:16:27.741-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 108
2020-05-08T12:16:27.741-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 94
2020-05-08T12:16:27.741-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 91
2020-05-08T12:16:27.741-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 80
2020-05-08T12:16:27.741-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 79
2020-05-08T12:16:27.741-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 78
2020-05-08T12:16:27.741-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 77
2020-05-08T12:16:27.741-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 76
2020-05-08T12:16:27.741-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 75
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 74
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 73
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 72
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 71
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 70
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 59
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 57
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 56
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 54
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 53
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 48
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 47
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 42
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 41
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 40
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 39
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 38
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 37
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 36
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 35
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 34
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 33
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 32
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 30
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 29
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 28
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 27
2020-05-08T12:16:27.742-0700 I  COMMAND  [conn70] command config.$cmd command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp(1588965384, 2), t: 9 } }, limit: 1, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965384, 2), signature: { hash: BinData(0, F4458C9035C064BC7AED66DE7AF404EC0A4344B5), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965384, 2), t: 9 } }, $db: "config" } numYields:0 ok:0 errMsg:"Error waiting for snapshot not less than { ts: Timestamp(1588965384, 2), t: 9 }, current relevant optime is { ts: Timestamp(0, 0), t: -1 }. :: caused by :: operation was interrupted" errName:InterruptedDueToReplStateChange errCode:11602 reslen:755 locks:{} protocol:op_msg 2976ms
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 26
2020-05-08T12:16:27.742-0700 I  COMMAND  [conn54] command config.$cmd command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp(1588965379, 197), t: 9 } }, limit: 1, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965379, 197), signature: { hash: BinData(0, 0F0E8F2DC5000EDA7C20D002E70E6B1B886733DE), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965379, 197), t: 9 } }, $db: "config" } numYields:0 ok:0 errMsg:"Error waiting for snapshot not less than { ts: Timestamp(1588965379, 197), t: 9 }, current relevant optime is { ts: Timestamp(0, 0), t: -1 }. :: caused by :: operation was interrupted" errName:InterruptedDueToReplStateChange errCode:11602 reslen:757 locks:{} protocol:op_msg 8025ms
2020-05-08T12:16:27.742-0700 I  COMMAND  [conn73] command config.$cmd command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp(1588965379, 197), t: 9 } }, limit: 1, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965379, 197), signature: { hash: BinData(0, 0F0E8F2DC5000EDA7C20D002E70E6B1B886733DE), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965379, 197), t: 9 } }, $db: "config" } numYields:0 ok:0 errMsg:"Error waiting for snapshot not less than { ts: Timestamp(1588965379, 197), t: 9 }, current relevant optime is { ts: Timestamp(0, 0), t: -1 }. :: caused by :: operation was interrupted" errName:InterruptedDueToReplStateChange errCode:11602 reslen:757 locks:{} protocol:op_msg 8026ms
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 25
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 24
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 23
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 22
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 21
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 20
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 19
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 18
2020-05-08T12:16:27.742-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 17
2020-05-08T12:16:27.743-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 16
2020-05-08T12:16:27.743-0700 I  ROLLBACK [rsBackgroundSync] Waiting for all background operations to complete before starting rollback
2020-05-08T12:16:27.743-0700 I  ROLLBACK [rsBackgroundSync] Finished waiting for background operations to complete before rollback
2020-05-08T12:16:27.743-0700 I  ROLLBACK [rsBackgroundSync] finding common point
2020-05-08T12:16:27.748-0700 I  ROLLBACK [rsBackgroundSync] Rollback common point is { ts: Timestamp(1588965373, 1), t: 6 }
2020-05-08T12:16:27.752-0700 I  REPL     [rsBackgroundSync] Incremented the rollback ID to 2
2020-05-08T12:16:27.752-0700 I  ROLLBACK [rsBackgroundSync] finding record store counts
2020-05-08T12:16:27.752-0700 I  ROLLBACK [rsBackgroundSync] Preparing to write deleted documents to a rollback file for collection config.mongos with uuid ed1a62fa-1d96-460b-b917-f8098f56b82b to /var/lib/mongodb/rollback/config.mongos/removed.2020-05-08T19-16-27.0.bson
2020-05-08T12:16:27.752-0700 I  ROLLBACK [rsBackgroundSync] Preparing to write deleted documents to a rollback file for collection config.lockpings with uuid 81acf486-601b-4e2b-8ee1-bbf74a1edd96 to /var/lib/mongodb/rollback/config.lockpings/removed.2020-05-08T19-16-27.1.bson
2020-05-08T12:16:27.753-0700 I  STORAGE  [rsBackgroundSync] closeCatalog: closing all databases
2020-05-08T12:16:27.753-0700 I  STORAGE  [rsBackgroundSync] closeCatalog: closing storage engine catalog
2020-05-08T12:16:27.753-0700 I  STORAGE  [rsBackgroundSync] Deregistering all the collections
2020-05-08T12:16:27.753-0700 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2020-05-08T12:16:27.821-0700 I  ROLLBACK [rsBackgroundSync] Rolling back to the stable timestamp. StableTimestamp: Timestamp(1588965373, 1) Initial Data Timestamp: Timestamp(1588965338, 1)
2020-05-08T12:16:27.822-0700 I  STORAGE  [rsBackgroundSync] openCatalog: loading storage engine catalog
2020-05-08T12:16:27.838-0700 I  STORAGE  [rsBackgroundSync] OplogTruncaterThread local.oplog.rs already started
2020-05-08T12:16:27.838-0700 I  STORAGE  [rsBackgroundSync] The size storer reports that the oplog contains 206 records totaling to 45674 bytes
2020-05-08T12:16:27.838-0700 I  STORAGE  [rsBackgroundSync] Scanning the oplog to determine where to place markers for truncation
2020-05-08T12:16:27.838-0700 I  STORAGE  [rsBackgroundSync] WiredTiger record store oplog processing took 0ms
2020-05-08T12:16:27.842-0700 I  STORAGE  [rsBackgroundSync] openCatalog: reconciling catalog and idents
2020-05-08T12:16:27.842-0700 I  STORAGE  [rsBackgroundSync] openCatalog: reopening all databases
2020-05-08T12:16:27.859-0700 I  STORAGE  [rsBackgroundSync] openCatalog: updating cached oplog pointer
2020-05-08T12:16:27.859-0700 I  STORAGE  [rsBackgroundSync] openCatalog: finished reloading collection catalog
2020-05-08T12:16:27.859-0700 I  STORAGE  [rsBackgroundSync] recoverToStableTimestamp successful. Stable Timestamp: Timestamp(1588965373, 1)
2020-05-08T12:16:27.859-0700 I  ROLLBACK [rsBackgroundSync] Rollback reverted 0 insert operations, 8 update operations and 0 delete operations.
2020-05-08T12:16:27.859-0700 I  ROLLBACK [rsBackgroundSync] Marking to truncate all oplog entries with timestamps greater than or equal to { ts: Timestamp(1588965377, 2), t: 8 }
2020-05-08T12:16:27.859-0700 I  REPL     [rsBackgroundSync] Removing unapplied entries starting at: { : Timestamp(1588965377, 2) }
2020-05-08T12:16:27.859-0700 I  REPL     [rsBackgroundSync] Replication recovery oplog truncation finished in: 0ms
2020-05-08T12:16:27.862-0700 I  REPL     [rsBackgroundSync] Recovering from stable timestamp: Timestamp(1588965373, 1) (top of oplog: { ts: Timestamp(1588965373, 1), t: 6 }, appliedThrough: { ts: Timestamp(0, 0), t: -1 }, TruncateAfter: Timestamp(0, 0))
2020-05-08T12:16:27.862-0700 I  REPL     [rsBackgroundSync] Starting recovery oplog application at the stable timestamp: Timestamp(1588965373, 1)
2020-05-08T12:16:27.862-0700 I  REPL     [rsBackgroundSync] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2020-05-08T12:16:27.862-0700 I  REPL     [rsBackgroundSync] Not updating committed snapshot because we are in rollback
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] Triggering the rollback op observer
2020-05-08T12:16:27.862-0700 I  REPL     [rsBackgroundSync] Resetting key manager cache
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] Rollback complete
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] Rollback summary:
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 	start time: 2020-05-08T12:16:27.741-0700
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 	end time: 2020-05-08T12:16:27.862-0700
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 	sync source: n1:27019
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 	rollback data file directory: /var/lib/mongodb/rollback/config.
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 	rollback id: 2
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 	last optime on branch of history rolled back: { ts: Timestamp(1588965378, 4), t: 8 }
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 	common point optime: { ts: Timestamp(1588965373, 1), t: 6 }
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 	last wall clock time on the branch of history rolled back: 2020-05-08T12:16:18.092-0700
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 	wall clock time of the first operation after the common point: 2020-05-08T12:16:13.033-0700
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 	difference in wall clock times: 5 second(s)
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 	truncate timestamp: Timestamp(1588965377, 2)
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 	stable timestamp: Timestamp(1588965373, 1)
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 	shard identity document rolled back: false
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 	config server config version document rolled back: false
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 	affected sessions: none
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 	affected namespaces: 
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 		config.lockpings
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 		config.mongos
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 	counts of interesting commands rolled back: 
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 		insert: 0
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 		delete: 0
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 		update: 8
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] 	total number of entries rolled back (including no-ops): 9
2020-05-08T12:16:27.862-0700 I  ROLLBACK [rsBackgroundSync] transition to SECONDARY
2020-05-08T12:16:27.862-0700 I  REPL     [rsBackgroundSync] transition to SECONDARY from ROLLBACK
2020-05-08T12:16:27.863-0700 I  REPL     [rsBackgroundSync] Resetting sync source to empty, which was n1:27019
2020-05-08T12:16:27.863-0700 I  REPL     [rsBackgroundSync] Rollback successful.
2020-05-08T12:16:27.863-0700 I  REPL     [rsBackgroundSync] Stopping replication producer
2020-05-08T12:16:27.863-0700 I  REPL     [rsBackgroundSync] Resetting last fetched optimes in bgsync
2020-05-08T12:16:27.863-0700 I  REPL     [rsBackgroundSync] sync source candidate: n1:27019
2020-05-08T12:16:27.864-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n1:27019
2020-05-08T12:16:28.001-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:56810 #130 (55 connections now open)
2020-05-08T12:16:28.001-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:46294 #131 (56 connections now open)
2020-05-08T12:16:28.001-0700 I  NETWORK  [conn130] received client metadata from 192.168.122.11:56810 conn130: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:28.001-0700 I  NETWORK  [conn131] received client metadata from 192.168.122.13:46294 conn131: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:29.861-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54894 #132 (57 connections now open)
2020-05-08T12:16:29.862-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54902 #133 (58 connections now open)
2020-05-08T12:16:29.862-0700 I  NETWORK  [conn132] received client metadata from 192.168.122.1:54894 conn132: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:29.862-0700 I  NETWORK  [conn133] received client metadata from 192.168.122.1:54902 conn133: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:29.865-0700 I  NETWORK  [conn132] end connection 192.168.122.1:54894 (57 connections now open)
2020-05-08T12:16:29.865-0700 I  NETWORK  [conn133] end connection 192.168.122.1:54902 (56 connections now open)
2020-05-08T12:16:30.173-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54912 #134 (57 connections now open)
2020-05-08T12:16:30.173-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54918 #135 (58 connections now open)
2020-05-08T12:16:30.174-0700 I  NETWORK  [conn134] received client metadata from 192.168.122.1:54912 conn134: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:30.174-0700 I  NETWORK  [conn135] received client metadata from 192.168.122.1:54918 conn135: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:30.176-0700 I  NETWORK  [conn134] end connection 192.168.122.1:54912 (57 connections now open)
2020-05-08T12:16:30.176-0700 I  NETWORK  [conn135] end connection 192.168.122.1:54918 (56 connections now open)
2020-05-08T12:16:30.594-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54940 #136 (57 connections now open)
2020-05-08T12:16:30.594-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:54946 #137 (58 connections now open)
2020-05-08T12:16:30.595-0700 I  NETWORK  [conn136] received client metadata from 192.168.122.1:54940 conn136: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:30.595-0700 I  NETWORK  [conn137] received client metadata from 192.168.122.1:54946 conn137: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:30.598-0700 I  NETWORK  [conn136] end connection 192.168.122.1:54940 (57 connections now open)
2020-05-08T12:16:30.599-0700 I  NETWORK  [conn137] end connection 192.168.122.1:54946 (56 connections now open)
2020-05-08T12:16:31.585-0700 I  NETWORK  [conn108] end connection 192.168.122.11:56350 (55 connections now open)
2020-05-08T12:16:32.999-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:55154 #138 (56 connections now open)
2020-05-08T12:16:33.000-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:55160 #139 (57 connections now open)
2020-05-08T12:16:33.000-0700 I  NETWORK  [conn139] received client metadata from 192.168.122.1:55160 conn139: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:33.000-0700 I  NETWORK  [conn138] received client metadata from 192.168.122.1:55154 conn138: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:33.003-0700 I  NETWORK  [conn138] end connection 192.168.122.1:55154 (56 connections now open)
2020-05-08T12:16:33.003-0700 I  NETWORK  [conn139] end connection 192.168.122.1:55160 (55 connections now open)
2020-05-08T12:16:34.003-0700 I  ELECTION [replexec-3] Starting an election, since we've seen no PRIMARY in the past 1000ms
2020-05-08T12:16:34.003-0700 I  ELECTION [replexec-3] conducting a dry run election to see if we could be elected. current term: 9
2020-05-08T12:16:34.003-0700 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 801 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 9, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965389, 3), t: 9 } }
2020-05-08T12:16:34.003-0700 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 802 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 9, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965389, 3), t: 9 } }
2020-05-08T12:16:34.004-0700 I  ELECTION [replexec-5] VoteRequester(term 9 dry run) received a yes vote from n3:27019; response message: { term: 9, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000003') }, lastCommittedOpTime: Timestamp(1588965389, 3), $clusterTime: { clusterTime: Timestamp(1588965392, 3), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965389, 3) }
2020-05-08T12:16:34.004-0700 I  ELECTION [replexec-4] dry election run succeeded, running for election in term 10
2020-05-08T12:16:34.004-0700 I  CONNPOOL [Replication] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-05-08T12:16:34.005-0700 I  CONNPOOL [Replication] Connecting to n1:27019
2020-05-08T12:16:34.010-0700 I  REPL     [replexec-4] Scheduling remote command request for vote request: RemoteCommand 803 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 10, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965389, 3), t: 9 } }
2020-05-08T12:16:34.010-0700 I  REPL     [replexec-4] Scheduling remote command request for vote request: RemoteCommand 804 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 10, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965389, 3), t: 9 } }
2020-05-08T12:16:34.013-0700 I  ELECTION [replexec-3] VoteRequester(term 10) received a yes vote from n3:27019; response message: { term: 10, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000003') }, lastCommittedOpTime: Timestamp(1588965389, 3), $clusterTime: { clusterTime: Timestamp(1588965392, 3), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965389, 3) }
2020-05-08T12:16:34.013-0700 I  ELECTION [replexec-5] election succeeded, assuming primary role in term 10
2020-05-08T12:16:34.013-0700 I  REPL     [replexec-5] transition to PRIMARY from SECONDARY
2020-05-08T12:16:34.013-0700 I  REPL     [replexec-5] Resetting sync source to empty, which was n1:27019
2020-05-08T12:16:34.013-0700 I  REPL     [replexec-5] Entering primary catch-up mode.
2020-05-08T12:16:34.535-0700 I  NETWORK  [conn124] end connection 192.168.122.11:56752 (54 connections now open)
2020-05-08T12:16:34.696-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:55230 #140 (55 connections now open)
2020-05-08T12:16:34.696-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:55236 #141 (56 connections now open)
2020-05-08T12:16:34.697-0700 I  NETWORK  [conn140] received client metadata from 192.168.122.1:55230 conn140: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:34.697-0700 I  NETWORK  [conn141] received client metadata from 192.168.122.1:55236 conn141: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:34.699-0700 I  NETWORK  [conn140] end connection 192.168.122.1:55230 (55 connections now open)
2020-05-08T12:16:34.699-0700 I  NETWORK  [conn141] end connection 192.168.122.1:55236 (54 connections now open)
2020-05-08T12:16:35.010-0700 I  REPL     [replexec-3] Member n1:27019 is now in state SECONDARY
2020-05-08T12:16:35.010-0700 I  REPL     [replexec-3] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1588965389, 3), t: 9 }. My Last Applied: { ts: Timestamp(1588965389, 3), t: 9 }
2020-05-08T12:16:35.010-0700 I  REPL     [replexec-3] Exited primary catch-up mode.
2020-05-08T12:16:35.010-0700 I  REPL     [replexec-3] Stopping replication producer
2020-05-08T12:16:35.010-0700 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 10
2020-05-08T12:16:35.010-0700 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2020-05-08T12:16:35.010-0700 I  CONNPOOL [RS] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:16:35.010-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:16:35.010-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:16:35.010-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 0 }
2020-05-08T12:16:35.012-0700 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-05-08T12:16:35.012-0700 I  SHARDING [Balancer] CSRS balancer is starting
2020-05-08T12:16:35.012-0700 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-05-08T12:16:35.013-0700 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-05-08T12:16:35.013-0700 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-05-08T12:16:35.013-0700 I  NETWORK  [Balancer] Starting new replica set monitor for rs_shard1/n4:27018,n5:27018,n6:27018
2020-05-08T12:16:35.013-0700 I  NETWORK  [Balancer] Starting new replica set monitor for rs_shard2/n7:27018,n8:27018,n9:27018
2020-05-08T12:16:35.014-0700 I  REPL     [replexec-1] Member n3:27019 is now in state RS_DOWN - no response within election timeout period
2020-05-08T12:16:35.014-0700 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs_shard1 is rs_shard1/n4:27018,n5:27018,n6:27018
2020-05-08T12:16:35.015-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard2
2020-05-08T12:16:35.015-0700 I  CONNPOOL [ShardRegistry] Connecting to n6:27018
2020-05-08T12:16:35.018-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard2
2020-05-08T12:16:35.517-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard2
2020-05-08T12:16:35.585-0700 I  NETWORK  [conn131] end connection 192.168.122.13:46294 (53 connections now open)
2020-05-08T12:16:35.605-0700 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to n1:27019: InvalidSyncSource: Sync source was cleared. Was n1:27019
2020-05-08T12:16:35.665-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:55266 #145 (54 connections now open)
2020-05-08T12:16:35.665-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:55272 #146 (55 connections now open)
2020-05-08T12:16:35.665-0700 I  NETWORK  [conn145] received client metadata from 192.168.122.1:55266 conn145: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:35.665-0700 I  NETWORK  [conn146] received client metadata from 192.168.122.1:55272 conn146: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:35.668-0700 I  NETWORK  [conn145] end connection 192.168.122.1:55266 (54 connections now open)
2020-05-08T12:16:35.668-0700 I  NETWORK  [conn146] end connection 192.168.122.1:55272 (53 connections now open)
2020-05-08T12:16:35.943-0700 I  REPL     [conn125] stepping down from primary, because a new term has begun: 11
2020-05-08T12:16:35.943-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:16:35.944-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:16:35.944-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 5, userOpsRunning: 2 }
2020-05-08T12:16:35.944-0700 W  COMMAND  [conn72] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:35.944-0700 I  COMMAND  [conn72] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n1:27017" }, u: { $set: { _id: "n1:27017", ping: new Date(1588965395470), up: 51, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965395, 2), signature: { hash: BinData(0, D1A6364B23E002829B6AB8203C84275E888AA2E2), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965389, 3), t: 9 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 472ms
2020-05-08T12:16:35.944-0700 W  COMMAND  [conn130] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:35.944-0700 I  COMMAND  [conn130] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965395, 2), signature: { hash: BinData(0, D1A6364B23E002829B6AB8203C84275E888AA2E2), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965389, 3), t: 9 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 559ms
2020-05-08T12:16:35.944-0700 W  COMMAND  [conn22] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:35.944-0700 I  COMMAND  [conn22] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n3:27017" }, u: { $set: { _id: "n3:27017", ping: new Date(1588965395360), up: 51, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965395, 2), signature: { hash: BinData(0, D1A6364B23E002829B6AB8203C84275E888AA2E2), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965389, 3), t: 9 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 374ms
2020-05-08T12:16:35.944-0700 I  REPL     [replexec-4] transition to SECONDARY from PRIMARY
2020-05-08T12:16:35.945-0700 W  COMMAND  [conn19] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:35.945-0700 W  COMMAND  [conn54] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:16:35.945-0700 I  COMMAND  [conn19] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965395, 2), signature: { hash: BinData(0, D1A6364B23E002829B6AB8203C84275E888AA2E2), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965389, 3), t: 9 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 375ms
2020-05-08T12:16:35.945-0700 I  COMMAND  [conn54] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965395, 2), signature: { hash: BinData(0, D1A6364B23E002829B6AB8203C84275E888AA2E2), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965389, 3), t: 9 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } protocol:op_msg 561ms
2020-05-08T12:16:35.945-0700 W  TXN      [TransactionCoordinator] Coordinator recovery failed and coordinateCommit requests will not be allowed :: caused by :: PrimarySteppedDown: Primary stepped down while waiting for replication
2020-05-08T12:16:35.945-0700 I  SHARDING [Balancer] caught exception while doing balance: operation was interrupted
2020-05-08T12:16:35.945-0700 I  SHARDING [Balancer] couldn't create config.actionlog collection: :: caused by :: InterruptedDueToReplStateChange: operation was interrupted
2020-05-08T12:16:35.945-0700 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-05-08T12:16:36.014-0700 I  REPL     [replexec-4] Member n3:27019 is now in state PRIMARY
2020-05-08T12:16:36.015-0700 I  ELECTION [replexec-4] Scheduling priority takeover at 2020-05-08T12:16:38.088-0700
2020-05-08T12:16:36.017-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard2
2020-05-08T12:16:36.069-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:46978 #147 (54 connections now open)
2020-05-08T12:16:36.069-0700 I  NETWORK  [conn147] received client metadata from 192.168.122.13:46978 conn147: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:36.101-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:46980 #148 (55 connections now open)
2020-05-08T12:16:36.101-0700 I  NETWORK  [conn148] received client metadata from 192.168.122.13:46980 conn148: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:36.517-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard2
2020-05-08T12:16:36.946-0700 I  REPL     [rsBackgroundSync] sync source candidate: n3:27019
2020-05-08T12:16:36.948-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n3:27019
2020-05-08T12:16:36.949-0700 I  CONNPOOL [RS] Connecting to n3:27019
2020-05-08T12:16:36.951-0700 I  REPL     [rsBackgroundSync] Starting rollback due to OplogStartMissing: Our last optime fetched: { ts: Timestamp(1588965395, 4), t: 10 }. source's GTE: { ts: Timestamp(1588965395, 4), t: 11 }
2020-05-08T12:16:36.951-0700 I  REPL     [rsBackgroundSync] Replication commit point: { ts: Timestamp(1588965389, 3), t: 9 }
2020-05-08T12:16:36.951-0700 I  REPL     [rsBackgroundSync] Rollback using 'recoverToStableTimestamp' method.
2020-05-08T12:16:36.951-0700 I  REPL     [rsBackgroundSync] Scheduling rollback (sync source: n3:27019)
2020-05-08T12:16:36.951-0700 I  ROLLBACK [rsBackgroundSync] transition to ROLLBACK
2020-05-08T12:16:36.951-0700 I  REPL     [rsBackgroundSync] State transition ops metrics: { lastStateTransition: "rollback", userOpsKilled: 0, userOpsRunning: 56 }
2020-05-08T12:16:36.951-0700 I  REPL     [rsBackgroundSync] Canceling priority takeover callback
2020-05-08T12:16:36.951-0700 I  REPL     [rsBackgroundSync] transition to ROLLBACK from SECONDARY
2020-05-08T12:16:36.951-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 148
2020-05-08T12:16:36.951-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 147
2020-05-08T12:16:36.951-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 130
2020-05-08T12:16:36.951-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 125
2020-05-08T12:16:36.951-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 123
2020-05-08T12:16:36.951-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 122
2020-05-08T12:16:36.951-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 117
2020-05-08T12:16:36.951-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 114
2020-05-08T12:16:36.951-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 113
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 94
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 91
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 80
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 79
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 78
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 77
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 76
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 75
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 74
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 73
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 72
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 71
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 70
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 59
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 57
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 56
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 54
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 53
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 48
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 47
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 42
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 41
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 40
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 39
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 38
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 37
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 36
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 35
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 34
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 33
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 32
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 30
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 29
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 28
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 27
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 26
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 25
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 24
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 23
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 22
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 21
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 20
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 19
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 18
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 17
2020-05-08T12:16:36.952-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 16
2020-05-08T12:16:36.952-0700 I  ROLLBACK [rsBackgroundSync] Waiting for all background operations to complete before starting rollback
2020-05-08T12:16:36.952-0700 I  ROLLBACK [rsBackgroundSync] Finished waiting for background operations to complete before rollback
2020-05-08T12:16:36.952-0700 I  ROLLBACK [rsBackgroundSync] finding common point
2020-05-08T12:16:36.958-0700 I  ROLLBACK [rsBackgroundSync] Rollback common point is { ts: Timestamp(1588965389, 3), t: 9 }
2020-05-08T12:16:36.961-0700 I  REPL     [rsBackgroundSync] Incremented the rollback ID to 3
2020-05-08T12:16:36.961-0700 I  ROLLBACK [rsBackgroundSync] finding record store counts
2020-05-08T12:16:36.961-0700 I  ROLLBACK [rsBackgroundSync] Preparing to write deleted documents to a rollback file for collection config.mongos with uuid ed1a62fa-1d96-460b-b917-f8098f56b82b to /var/lib/mongodb/rollback/config.mongos/removed.2020-05-08T19-16-36.2.bson
2020-05-08T12:16:36.961-0700 I  STORAGE  [rsBackgroundSync] closeCatalog: closing all databases
2020-05-08T12:16:36.961-0700 I  STORAGE  [rsBackgroundSync] closeCatalog: closing storage engine catalog
2020-05-08T12:16:36.961-0700 I  STORAGE  [rsBackgroundSync] Deregistering all the collections
2020-05-08T12:16:36.961-0700 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2020-05-08T12:16:37.010-0700 I  REPL     [replexec-0] Member n1:27019 is now in state ROLLBACK
2020-05-08T12:16:37.014-0700 I  ELECTION [replexec-5] Scheduling priority takeover at 2020-05-08T12:16:39.023-0700
2020-05-08T12:16:37.017-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard2
2020-05-08T12:16:37.040-0700 I  ROLLBACK [rsBackgroundSync] Rolling back to the stable timestamp. StableTimestamp: Timestamp(1588965389, 3) Initial Data Timestamp: Timestamp(1588965338, 1)
2020-05-08T12:16:37.040-0700 I  STORAGE  [rsBackgroundSync] openCatalog: loading storage engine catalog
2020-05-08T12:16:37.049-0700 I  STORAGE  [rsBackgroundSync] OplogTruncaterThread local.oplog.rs already started
2020-05-08T12:16:37.049-0700 I  STORAGE  [rsBackgroundSync] The size storer reports that the oplog contains 221 records totaling to 48451 bytes
2020-05-08T12:16:37.049-0700 I  STORAGE  [rsBackgroundSync] Scanning the oplog to determine where to place markers for truncation
2020-05-08T12:16:37.049-0700 I  STORAGE  [rsBackgroundSync] WiredTiger record store oplog processing took 0ms
2020-05-08T12:16:37.053-0700 I  STORAGE  [rsBackgroundSync] openCatalog: reconciling catalog and idents
2020-05-08T12:16:37.053-0700 I  STORAGE  [rsBackgroundSync] openCatalog: reopening all databases
2020-05-08T12:16:37.069-0700 I  STORAGE  [rsBackgroundSync] openCatalog: updating cached oplog pointer
2020-05-08T12:16:37.069-0700 I  STORAGE  [rsBackgroundSync] openCatalog: finished reloading collection catalog
2020-05-08T12:16:37.069-0700 I  STORAGE  [rsBackgroundSync] recoverToStableTimestamp successful. Stable Timestamp: Timestamp(1588965389, 3)
2020-05-08T12:16:37.070-0700 I  ROLLBACK [rsBackgroundSync] Rollback reverted 0 insert operations, 2 update operations and 0 delete operations.
2020-05-08T12:16:37.070-0700 I  ROLLBACK [rsBackgroundSync] Marking to truncate all oplog entries with timestamps greater than or equal to { ts: Timestamp(1588965395, 1), t: 10 }
2020-05-08T12:16:37.070-0700 I  REPL     [rsBackgroundSync] Removing unapplied entries starting at: { : Timestamp(1588965395, 1) }
2020-05-08T12:16:37.070-0700 I  REPL     [rsBackgroundSync] Replication recovery oplog truncation finished in: 0ms
2020-05-08T12:16:37.072-0700 I  REPL     [rsBackgroundSync] Recovering from stable timestamp: Timestamp(1588965389, 3) (top of oplog: { ts: Timestamp(1588965389, 3), t: 9 }, appliedThrough: { ts: Timestamp(0, 0), t: -1 }, TruncateAfter: Timestamp(0, 0))
2020-05-08T12:16:37.072-0700 I  REPL     [rsBackgroundSync] Starting recovery oplog application at the stable timestamp: Timestamp(1588965389, 3)
2020-05-08T12:16:37.072-0700 I  REPL     [rsBackgroundSync] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2020-05-08T12:16:37.073-0700 I  REPL     [rsBackgroundSync] Not updating committed snapshot because we are in rollback
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] Triggering the rollback op observer
2020-05-08T12:16:37.073-0700 I  REPL     [rsBackgroundSync] Resetting key manager cache
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] Rollback complete
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] Rollback summary:
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 	start time: 2020-05-08T12:16:36.951-0700
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 	end time: 2020-05-08T12:16:37.073-0700
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 	sync source: n3:27019
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 	rollback data file directory: /var/lib/mongodb/rollback/config.mongos
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 	rollback id: 3
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 	last optime on branch of history rolled back: { ts: Timestamp(1588965395, 4), t: 10 }
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 	common point optime: { ts: Timestamp(1588965389, 3), t: 9 }
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 	last wall clock time on the branch of history rolled back: 2020-05-08T12:16:35.569-0700
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 	wall clock time of the first operation after the common point: 2020-05-08T12:16:35.011-0700
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 	difference in wall clock times: 0 second(s)
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 	truncate timestamp: Timestamp(1588965395, 1)
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 	stable timestamp: Timestamp(1588965389, 3)
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 	shard identity document rolled back: false
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 	config server config version document rolled back: false
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 	affected sessions: none
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 	affected namespaces: 
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 		config.mongos
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 	counts of interesting commands rolled back: 
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 		insert: 0
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 		delete: 0
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 		update: 2
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] 	total number of entries rolled back (including no-ops): 3
2020-05-08T12:16:37.073-0700 I  ROLLBACK [rsBackgroundSync] transition to SECONDARY
2020-05-08T12:16:37.074-0700 I  REPL     [rsBackgroundSync] transition to SECONDARY from ROLLBACK
2020-05-08T12:16:37.074-0700 I  REPL     [rsBackgroundSync] Resetting sync source to empty, which was n3:27019
2020-05-08T12:16:37.074-0700 I  REPL     [rsBackgroundSync] Rollback successful.
2020-05-08T12:16:37.074-0700 I  REPL     [rsBackgroundSync] Stopping replication producer
2020-05-08T12:16:37.074-0700 I  REPL     [rsBackgroundSync] Resetting last fetched optimes in bgsync
2020-05-08T12:16:37.074-0700 I  REPL     [rsBackgroundSync] sync source candidate: n3:27019
2020-05-08T12:16:37.076-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n3:27019
2020-05-08T12:16:37.203-0700 I  NETWORK  [shard-registry-reload] Starting new replica set monitor for rs_shard1/n4:27018,n5:27018,n6:27018
2020-05-08T12:16:37.203-0700 I  NETWORK  [shard-registry-reload] Starting new replica set monitor for rs_shard2/n7:27018,n8:27018,n9:27018
2020-05-08T12:16:37.204-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:16:37.205-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard2
2020-05-08T12:16:37.467-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:55400 #151 (56 connections now open)
2020-05-08T12:16:37.468-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:55406 #152 (57 connections now open)
2020-05-08T12:16:37.468-0700 I  NETWORK  [conn151] received client metadata from 192.168.122.1:55400 conn151: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:37.468-0700 I  NETWORK  [conn152] received client metadata from 192.168.122.1:55406 conn152: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:37.472-0700 I  NETWORK  [conn151] end connection 192.168.122.1:55400 (56 connections now open)
2020-05-08T12:16:37.472-0700 I  NETWORK  [conn152] end connection 192.168.122.1:55406 (55 connections now open)
2020-05-08T12:16:39.010-0700 I  REPL     [replexec-0] Member n1:27019 is now in state SECONDARY
2020-05-08T12:16:39.023-0700 I  REPL     [replexec-4] Canceling priority takeover callback
2020-05-08T12:16:39.023-0700 I  ELECTION [replexec-4] Starting an election for a priority takeover
2020-05-08T12:16:39.023-0700 I  ELECTION [replexec-4] conducting a dry run election to see if we could be elected. current term: 11
2020-05-08T12:16:39.023-0700 I  REPL     [replexec-4] Scheduling remote command request for vote request: RemoteCommand 877 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 11, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965397, 5), t: 11 } }
2020-05-08T12:16:39.023-0700 I  REPL     [replexec-4] Scheduling remote command request for vote request: RemoteCommand 878 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 11, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965397, 5), t: 11 } }
2020-05-08T12:16:39.023-0700 I  ELECTION [replexec-0] VoteRequester(term 11 dry run) received a yes vote from n1:27019; response message: { term: 11, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000009') }, lastCommittedOpTime: Timestamp(1588965397, 5), $clusterTime: { clusterTime: Timestamp(1588965398, 2), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965397, 5) }
2020-05-08T12:16:39.024-0700 I  ELECTION [replexec-0] dry election run succeeded, running for election in term 12
2020-05-08T12:16:39.024-0700 I  CONNPOOL [Replication] Ending connection to host n3:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-05-08T12:16:39.024-0700 I  CONNPOOL [Replication] Connecting to n3:27019
2020-05-08T12:16:39.030-0700 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 879 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 12, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965397, 5), t: 11 } }
2020-05-08T12:16:39.030-0700 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 880 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 12, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965397, 5), t: 11 } }
2020-05-08T12:16:39.033-0700 I  ELECTION [replexec-4] VoteRequester(term 12) received a yes vote from n1:27019; response message: { term: 12, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000009') }, lastCommittedOpTime: Timestamp(1588965397, 5), $clusterTime: { clusterTime: Timestamp(1588965398, 2), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965397, 5) }
2020-05-08T12:16:39.033-0700 I  ELECTION [replexec-4] election succeeded, assuming primary role in term 12
2020-05-08T12:16:39.033-0700 I  REPL     [replexec-4] transition to PRIMARY from SECONDARY
2020-05-08T12:16:39.033-0700 I  CONNPOOL [Replication] Ending connection to host n3:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-05-08T12:16:39.033-0700 I  REPL     [replexec-4] Resetting sync source to empty, which was n3:27019
2020-05-08T12:16:39.034-0700 I  REPL     [replexec-4] Entering primary catch-up mode.
2020-05-08T12:16:39.036-0700 I  REPL     [replexec-4] Member n3:27019 is now in state SECONDARY
2020-05-08T12:16:39.036-0700 I  REPL     [replexec-4] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1588965397, 5), t: 11 }. My Last Applied: { ts: Timestamp(1588965397, 5), t: 11 }
2020-05-08T12:16:39.036-0700 I  REPL     [replexec-4] Exited primary catch-up mode.
2020-05-08T12:16:39.036-0700 I  REPL     [replexec-4] Stopping replication producer
2020-05-08T12:16:39.036-0700 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 12
2020-05-08T12:16:39.036-0700 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2020-05-08T12:16:39.036-0700 I  CONNPOOL [RS] Ending connection to host n3:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:16:39.036-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:16:39.036-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:16:39.036-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 0 }
2020-05-08T12:16:39.038-0700 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-05-08T12:16:39.038-0700 I  SHARDING [Balancer] CSRS balancer is starting
2020-05-08T12:16:39.038-0700 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-05-08T12:16:39.039-0700 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-05-08T12:16:39.039-0700 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-05-08T12:16:39.040-0700 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs_shard1 is rs_shard1/n4:27018,n5:27018,n6:27018
2020-05-08T12:16:39.041-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard2
2020-05-08T12:16:39.264-0700 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to n3:27019: InvalidSyncSource: Sync source was cleared. Was n3:27019
2020-05-08T12:16:39.541-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard2
2020-05-08T12:16:40.041-0700 I  COMMAND  [conn57] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n7:27017" }, u: { $set: { _id: "n7:27017", ping: new Date(1588965399732), up: 55, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965399, 2), signature: { hash: BinData(0, FA4D3AFD5793CA054CB0FB09FDE9207BEB6C7A05), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965397, 5), t: 11 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } storage:{} protocol:op_msg 304ms
2020-05-08T12:16:40.041-0700 I  COMMAND  [conn32] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n5:27017" }, u: { $set: { _id: "n5:27017", ping: new Date(1588965399728), up: 56, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965398, 2), signature: { hash: BinData(0, A61768D3F65105C345188C85E581B25F1C300613), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965397, 5), t: 11 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } storage:{} protocol:op_msg 311ms
2020-05-08T12:16:40.041-0700 I  COMMAND  [conn53] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n4:27017" }, u: { $set: { _id: "n4:27017", ping: new Date(1588965399728), up: 55, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965399, 2), signature: { hash: BinData(0, FA4D3AFD5793CA054CB0FB09FDE9207BEB6C7A05), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965397, 5), t: 11 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 308ms
2020-05-08T12:16:40.041-0700 I  TXN      [TransactionCoordinator] Need to resume coordinating commit for 0 transactions
2020-05-08T12:16:40.041-0700 I  TXN      [TransactionCoordinator] Incoming coordinateCommit requests are now enabled
2020-05-08T12:16:40.041-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard2
2020-05-08T12:16:40.296-0700 I  ELECTION [conn123] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 12, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965399, 5), t: 12 } }
2020-05-08T12:16:40.296-0700 I  ELECTION [conn123] Sending vote response: { term: 12, voteGranted: true, reason: "" }
2020-05-08T12:16:40.300-0700 I  REPL     [conn123] stepping down from primary, because a new term has begun: 13
2020-05-08T12:16:40.300-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:16:40.301-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:16:40.301-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 0, userOpsRunning: 3 }
2020-05-08T12:16:40.301-0700 I  REPL     [replexec-1] transition to SECONDARY from PRIMARY
2020-05-08T12:16:40.301-0700 I  SHARDING [Balancer] caught exception while doing balance: operation was interrupted
2020-05-08T12:16:40.301-0700 I  ELECTION [conn123] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 13, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965399, 5), t: 12 } }
2020-05-08T12:16:40.301-0700 I  SHARDING [Balancer] couldn't create config.actionlog collection: :: caused by :: InterruptedDueToReplStateChange: operation was interrupted
2020-05-08T12:16:40.302-0700 I  ELECTION [conn123] Sending vote response: { term: 13, voteGranted: true, reason: "" }
2020-05-08T12:16:40.302-0700 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-05-08T12:16:40.306-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:57742 #155 (56 connections now open)
2020-05-08T12:16:40.307-0700 I  NETWORK  [conn123] end connection 192.168.122.11:56754 (55 connections now open)
2020-05-08T12:16:40.307-0700 I  NETWORK  [conn155] received client metadata from 192.168.122.11:57742 conn155: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:40.528-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:55504 #156 (56 connections now open)
2020-05-08T12:16:40.528-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:55512 #157 (57 connections now open)
2020-05-08T12:16:40.529-0700 I  NETWORK  [conn156] received client metadata from 192.168.122.1:55504 conn156: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:40.529-0700 I  NETWORK  [conn157] received client metadata from 192.168.122.1:55512 conn157: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:40.530-0700 I  NETWORK  [conn156] end connection 192.168.122.1:55504 (56 connections now open)
2020-05-08T12:16:40.530-0700 I  NETWORK  [conn157] end connection 192.168.122.1:55512 (55 connections now open)
2020-05-08T12:16:40.541-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard2
2020-05-08T12:16:40.773-0700 I  NETWORK  [conn125] end connection 192.168.122.11:56284 (54 connections now open)
2020-05-08T12:16:41.035-0700 I  REPL     [replexec-4] Member n1:27019 is now in state PRIMARY
2020-05-08T12:16:41.041-0700 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs_shard2 is rs_shard2/n7:27018,n8:27018,n9:27018
2020-05-08T12:16:41.302-0700 I  REPL     [rsBackgroundSync] sync source candidate: n1:27019
2020-05-08T12:16:41.304-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n1:27019
2020-05-08T12:16:41.305-0700 I  CONNPOOL [RS] Connecting to n1:27019
2020-05-08T12:16:44.278-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:55790 #159 (55 connections now open)
2020-05-08T12:16:44.278-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:55796 #160 (56 connections now open)
2020-05-08T12:16:44.278-0700 I  NETWORK  [conn159] received client metadata from 192.168.122.1:55790 conn159: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:44.279-0700 I  NETWORK  [conn160] received client metadata from 192.168.122.1:55796 conn160: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:44.283-0700 I  NETWORK  [conn159] end connection 192.168.122.1:55790 (55 connections now open)
2020-05-08T12:16:44.283-0700 I  NETWORK  [conn160] end connection 192.168.122.1:55796 (54 connections now open)
2020-05-08T12:16:45.154-0700 I  ELECTION [replexec-5] Starting an election, since we've seen no PRIMARY in the past 1000ms
2020-05-08T12:16:45.154-0700 I  ELECTION [replexec-5] conducting a dry run election to see if we could be elected. current term: 13
2020-05-08T12:16:45.154-0700 I  REPL     [replexec-5] Scheduling remote command request for vote request: RemoteCommand 945 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 13, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965403, 8), t: 13 } }
2020-05-08T12:16:45.154-0700 I  REPL     [replexec-5] Scheduling remote command request for vote request: RemoteCommand 946 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 13, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965403, 8), t: 13 } }
2020-05-08T12:16:45.156-0700 I  ELECTION [conn148] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 13, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965403, 8), t: 13 } }
2020-05-08T12:16:45.156-0700 I  ELECTION [conn148] Sending vote response: { term: 13, voteGranted: true, reason: "" }
2020-05-08T12:16:45.156-0700 I  ELECTION [replexec-1] VoteRequester(term 13 dry run) received a yes vote from n3:27019; response message: { term: 13, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000000b') }, lastCommittedOpTime: Timestamp(1588965403, 8), $clusterTime: { clusterTime: Timestamp(1588965403, 10), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965403, 8) }
2020-05-08T12:16:45.156-0700 I  ELECTION [replexec-1] dry election run succeeded, running for election in term 14
2020-05-08T12:16:45.156-0700 I  CONNPOOL [Replication] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:16:45.158-0700 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 947 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 14, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965403, 8), t: 13 } }
2020-05-08T12:16:45.158-0700 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 948 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 14, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965403, 8), t: 13 } }
2020-05-08T12:16:45.159-0700 I  ELECTION [replexec-5] VoteRequester(term 14) received a no vote from n3:27019 with reason "already voted for another candidate (n3:27019) this term (14)"; response message: { term: 14, voteGranted: false, reason: "already voted for another candidate (n3:27019) this term (14)", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000000b') }, lastCommittedOpTime: Timestamp(1588965403, 8), $clusterTime: { clusterTime: Timestamp(1588965403, 10), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965403, 8) }
2020-05-08T12:16:45.160-0700 I  ELECTION [conn148] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 14, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965403, 8), t: 13 } }
2020-05-08T12:16:45.160-0700 I  ELECTION [conn148] Sending vote response: { term: 14, voteGranted: false, reason: "already voted for another candidate (n2:27019) this term (14)" }
2020-05-08T12:16:45.537-0700 I  CONNPOOL [Replication] Connecting to n1:27019
2020-05-08T12:16:45.903-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:55846 #161 (55 connections now open)
2020-05-08T12:16:45.903-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:55848 #162 (56 connections now open)
2020-05-08T12:16:45.904-0700 I  NETWORK  [conn161] received client metadata from 192.168.122.1:55846 conn161: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:45.904-0700 I  NETWORK  [conn162] received client metadata from 192.168.122.1:55848 conn162: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:45.907-0700 I  NETWORK  [conn162] end connection 192.168.122.1:55848 (55 connections now open)
2020-05-08T12:16:45.907-0700 I  NETWORK  [conn161] end connection 192.168.122.1:55846 (54 connections now open)
2020-05-08T12:16:46.156-0700 I  ELECTION [replexec-3] Not starting an election, since we are not electable due to: Not standing for election again; already candidate
2020-05-08T12:16:46.158-0700 I  ELECTION [replexec-1] VoteRequester(term 14) failed to receive response from n1:27019: NetworkInterfaceExceededTimeLimit: Request 947 timed out, deadline was 2020-05-08T12:16:46.158-0700, op was RemoteCommand 947 -- target:[n1:27019] db:admin expDate:2020-05-08T12:16:46.158-0700 cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 14, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965403, 8), t: 13 } }
2020-05-08T12:16:46.158-0700 I  CONNPOOL [Replication] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:16:46.158-0700 I  ELECTION [replexec-1] not becoming primary, we received insufficient votes
2020-05-08T12:16:46.158-0700 I  ELECTION [replexec-1] Lost election due to internal error
2020-05-08T12:16:46.337-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:58060 #163 (55 connections now open)
2020-05-08T12:16:46.337-0700 I  NETWORK  [conn163] received client metadata from 192.168.122.11:58060 conn163: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:46.537-0700 I  REPL     [replexec-0] Member n1:27019 is now in state RS_DOWN - Couldn't get a connection within the time limit
2020-05-08T12:16:46.731-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:55914 #165 (56 connections now open)
2020-05-08T12:16:46.732-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:55918 #166 (57 connections now open)
2020-05-08T12:16:46.732-0700 I  NETWORK  [conn165] received client metadata from 192.168.122.1:55914 conn165: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:46.732-0700 I  NETWORK  [conn166] received client metadata from 192.168.122.1:55918 conn166: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:46.735-0700 I  NETWORK  [conn165] end connection 192.168.122.1:55914 (56 connections now open)
2020-05-08T12:16:46.736-0700 I  NETWORK  [conn166] end connection 192.168.122.1:55918 (55 connections now open)
2020-05-08T12:16:46.741-0700 I  REPL     [replication-0] Choosing new sync source. Our current sync source is not primary and does not have a sync source, so we require that it is ahead of us. Current sync source: n1:27019, my last fetched oplog optime: { ts: Timestamp(1588965403, 8), t: 13 }, latest oplog optime of sync source: { ts: Timestamp(1588965403, 8), t: 13 } (n3:27019 is)
2020-05-08T12:16:46.741-0700 I  REPL     [replication-0] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: n1:27019, OpTime { ts: Timestamp(1588965403, 8), t: 13 }, its sync source index:-1
2020-05-08T12:16:46.741-0700 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source n1:27019 (config version: 1; last applied optime: { ts: Timestamp(1588965403, 8), t: 13 }; sync source index: -1; primary index: 2) is no longer valid
2020-05-08T12:16:46.742-0700 I  REPL     [rsBackgroundSync] Clearing sync source n1:27019 to choose a new one.
2020-05-08T12:16:46.742-0700 I  REPL     [rsBackgroundSync] could not find member to sync from
2020-05-08T12:16:46.743-0700 I  REPL     [replexec-0] Member n1:27019 is now in state SECONDARY
2020-05-08T12:16:46.743-0700 I  REPL     [replexec-2] Member n3:27019 is now in state PRIMARY
2020-05-08T12:16:46.743-0700 I  ELECTION [replexec-2] Scheduling priority takeover at 2020-05-08T12:16:48.882-0700
2020-05-08T12:16:47.242-0700 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to n1:27019: InvalidSyncSource: Sync source was cleared. Was n1:27019
2020-05-08T12:16:47.456-0700 I  ELECTION [conn163] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 14, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965407, 1), t: 14 } }
2020-05-08T12:16:47.456-0700 I  ELECTION [conn163] Sending vote response: { term: 14, voteGranted: true, reason: "" }
2020-05-08T12:16:47.460-0700 I  REPL     [conn163] Canceling priority takeover callback
2020-05-08T12:16:47.460-0700 I  ELECTION [conn163] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 15, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965407, 1), t: 14 } }
2020-05-08T12:16:47.460-0700 I  ELECTION [conn163] Sending vote response: { term: 15, voteGranted: true, reason: "" }
2020-05-08T12:16:47.463-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:55964 #167 (56 connections now open)
2020-05-08T12:16:47.464-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:55970 #168 (57 connections now open)
2020-05-08T12:16:47.464-0700 I  NETWORK  [conn167] received client metadata from 192.168.122.1:55964 conn167: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:47.464-0700 I  NETWORK  [conn168] received client metadata from 192.168.122.1:55970 conn168: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:47.467-0700 I  NETWORK  [conn167] end connection 192.168.122.1:55964 (56 connections now open)
2020-05-08T12:16:47.468-0700 I  NETWORK  [conn168] end connection 192.168.122.1:55970 (55 connections now open)
2020-05-08T12:16:47.649-0700 I  NETWORK  [conn155] end connection 192.168.122.11:57742 (54 connections now open)
2020-05-08T12:16:47.742-0700 I  REPL     [rsBackgroundSync] sync source candidate: n3:27019
2020-05-08T12:16:47.744-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n3:27019
2020-05-08T12:16:47.744-0700 I  REPL     [replexec-3] Member n1:27019 is now in state PRIMARY
2020-05-08T12:16:47.745-0700 I  CONNPOOL [RS] Connecting to n3:27019
2020-05-08T12:16:47.750-0700 I  REPL     [replication-0] Choosing new sync source. Our current sync source is not primary and does not have a sync source, so we require that it is ahead of us. Current sync source: n3:27019, my last fetched oplog optime: { ts: Timestamp(1588965407, 1), t: 14 }, latest oplog optime of sync source: { ts: Timestamp(1588965407, 1), t: 14 } (sync source does not know the primary)
2020-05-08T12:16:47.750-0700 I  REPL     [replication-0] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: n3:27019, OpTime { ts: Timestamp(1588965407, 1), t: 14 }, its sync source index:-1
2020-05-08T12:16:47.750-0700 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source n3:27019 (config version: 1; last applied optime: { ts: Timestamp(1588965407, 1), t: 14 }; sync source index: -1; primary index: -1) is no longer valid
2020-05-08T12:16:47.750-0700 I  REPL     [rsBackgroundSync] Clearing sync source n3:27019 to choose a new one.
2020-05-08T12:16:47.750-0700 I  REPL     [rsBackgroundSync] sync source candidate: n1:27019
2020-05-08T12:16:47.752-0700 I  REPL     [rsBackgroundSync] Changed sync source from n3:27019 to n1:27019
2020-05-08T12:16:47.752-0700 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to n3:27019: InvalidSyncSource: Sync source changed from n3:27019 to n1:27019
2020-05-08T12:16:47.950-0700 I  REPL     [replexec-2] Member n3:27019 is now in state SECONDARY
2020-05-08T12:16:51.110-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:56164 #170 (55 connections now open)
2020-05-08T12:16:51.111-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:56170 #171 (56 connections now open)
2020-05-08T12:16:51.111-0700 I  NETWORK  [conn170] received client metadata from 192.168.122.1:56164 conn170: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:51.111-0700 I  NETWORK  [conn171] received client metadata from 192.168.122.1:56170 conn171: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:51.114-0700 I  NETWORK  [conn170] end connection 192.168.122.1:56164 (55 connections now open)
2020-05-08T12:16:51.114-0700 I  NETWORK  [conn171] end connection 192.168.122.1:56170 (54 connections now open)
2020-05-08T12:16:51.884-0700 I  ELECTION [replexec-4] Starting an election, since we've seen no PRIMARY in the past 1000ms
2020-05-08T12:16:51.884-0700 I  ELECTION [replexec-4] conducting a dry run election to see if we could be elected. current term: 15
2020-05-08T12:16:51.884-0700 I  REPL     [replexec-4] Scheduling remote command request for vote request: RemoteCommand 1040 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 15, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965410, 7), t: 15 } }
2020-05-08T12:16:51.884-0700 I  REPL     [replexec-4] Scheduling remote command request for vote request: RemoteCommand 1041 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 15, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965410, 7), t: 15 } }
2020-05-08T12:16:51.884-0700 I  CONNPOOL [Replication] Connecting to n1:27019
2020-05-08T12:16:51.885-0700 I  ELECTION [replexec-0] VoteRequester(term 15 dry run) received a yes vote from n3:27019; response message: { term: 15, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000000e') }, lastCommittedOpTime: Timestamp(1588965410, 7), $clusterTime: { clusterTime: Timestamp(1588965411, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965410, 7) }
2020-05-08T12:16:51.886-0700 I  ELECTION [replexec-0] dry election run succeeded, running for election in term 16
2020-05-08T12:16:51.890-0700 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 1042 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 16, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965410, 7), t: 15 } }
2020-05-08T12:16:51.890-0700 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 1043 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 16, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965410, 7), t: 15 } }
2020-05-08T12:16:51.895-0700 I  ELECTION [replexec-4] VoteRequester(term 16) received a yes vote from n3:27019; response message: { term: 16, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000000e') }, lastCommittedOpTime: Timestamp(1588965410, 7), $clusterTime: { clusterTime: Timestamp(1588965411, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965410, 7) }
2020-05-08T12:16:51.895-0700 I  ELECTION [replexec-4] election succeeded, assuming primary role in term 16
2020-05-08T12:16:51.895-0700 I  REPL     [replexec-4] transition to PRIMARY from SECONDARY
2020-05-08T12:16:51.895-0700 I  REPL     [replexec-4] Resetting sync source to empty, which was n1:27019
2020-05-08T12:16:51.896-0700 I  REPL     [replexec-4] Entering primary catch-up mode.
2020-05-08T12:16:51.896-0700 I  CONNPOOL [Replication] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 2 connections to that host remain open
2020-05-08T12:16:52.745-0700 I  REPL     [replexec-4] Member n1:27019 is now in state RS_DOWN - Couldn't get a connection within the time limit
2020-05-08T12:16:52.745-0700 I  REPL     [replexec-4] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1588965410, 7), t: 15 }. My Last Applied: { ts: Timestamp(1588965410, 7), t: 15 }
2020-05-08T12:16:52.745-0700 I  REPL     [replexec-4] Exited primary catch-up mode.
2020-05-08T12:16:52.745-0700 I  REPL     [replexec-4] Stopping replication producer
2020-05-08T12:16:52.745-0700 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 16
2020-05-08T12:16:52.745-0700 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2020-05-08T12:16:52.745-0700 I  CONNPOOL [RS] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:16:52.746-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:16:52.746-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:16:52.746-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 1 }
2020-05-08T12:16:52.748-0700 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-05-08T12:16:52.748-0700 I  SHARDING [Balancer] CSRS balancer is starting
2020-05-08T12:16:52.748-0700 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-05-08T12:16:52.749-0700 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-05-08T12:16:52.749-0700 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-05-08T12:16:52.750-0700 I  CONNPOOL [ShardRegistry] Connecting to n9:27018
2020-05-08T12:16:52.755-0700 I  TXN      [TransactionCoordinator] Need to resume coordinating commit for 0 transactions
2020-05-08T12:16:52.755-0700 I  TXN      [TransactionCoordinator] Incoming coordinateCommit requests are now enabled
2020-05-08T12:16:52.961-0700 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to n1:27019: InvalidSyncSource: Sync source was cleared. Was n1:27019
2020-05-08T12:16:53.020-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:56268 #175 (55 connections now open)
2020-05-08T12:16:53.021-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:56274 #176 (56 connections now open)
2020-05-08T12:16:53.021-0700 I  NETWORK  [conn175] received client metadata from 192.168.122.1:56268 conn175: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:53.021-0700 I  NETWORK  [conn176] received client metadata from 192.168.122.1:56274 conn176: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:53.023-0700 I  NETWORK  [conn175] end connection 192.168.122.1:56268 (55 connections now open)
2020-05-08T12:16:53.024-0700 I  NETWORK  [conn176] end connection 192.168.122.1:56274 (54 connections now open)
2020-05-08T12:16:53.473-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:58474 #177 (55 connections now open)
2020-05-08T12:16:53.473-0700 I  NETWORK  [conn177] received client metadata from 192.168.122.11:58474 conn177: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:16:53.649-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:56300 #178 (56 connections now open)
2020-05-08T12:16:53.650-0700 I  NETWORK  [conn178] received client metadata from 192.168.122.1:56300 conn178: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:53.650-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:56306 #179 (57 connections now open)
2020-05-08T12:16:53.650-0700 I  NETWORK  [conn179] received client metadata from 192.168.122.1:56306 conn179: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:53.653-0700 I  NETWORK  [conn178] end connection 192.168.122.1:56300 (56 connections now open)
2020-05-08T12:16:53.654-0700 I  NETWORK  [conn179] end connection 192.168.122.1:56306 (55 connections now open)
2020-05-08T12:16:54.485-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:56346 #180 (56 connections now open)
2020-05-08T12:16:54.485-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:56350 #181 (57 connections now open)
2020-05-08T12:16:54.485-0700 I  NETWORK  [conn180] received client metadata from 192.168.122.1:56346 conn180: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:54.485-0700 I  NETWORK  [conn181] received client metadata from 192.168.122.1:56350 conn181: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:54.489-0700 I  NETWORK  [conn180] end connection 192.168.122.1:56346 (56 connections now open)
2020-05-08T12:16:54.489-0700 I  NETWORK  [conn181] end connection 192.168.122.1:56350 (55 connections now open)
2020-05-08T12:16:54.746-0700 I  REPL     [replexec-1] stepping down from primary, because a new term has begun: 17
2020-05-08T12:16:54.746-0700 I  REPL     [replexec-1] Member n1:27019 is now in state PRIMARY
2020-05-08T12:16:54.746-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:16:54.746-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:16:54.746-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 0, userOpsRunning: 1 }
2020-05-08T12:16:54.747-0700 I  REPL     [replexec-2] transition to SECONDARY from PRIMARY
2020-05-08T12:16:54.747-0700 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-05-08T12:16:54.749-0700 I  ELECTION [conn177] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 16, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965412, 1), t: 16 } }
2020-05-08T12:16:54.749-0700 I  ELECTION [conn177] Sending vote response: { term: 17, voteGranted: false, reason: "candidate's term (16) is lower than mine (17)" }
2020-05-08T12:16:54.749-0700 I  NETWORK  [conn177] end connection 192.168.122.11:58474 (54 connections now open)
2020-05-08T12:16:54.817-0700 I  NETWORK  [conn163] end connection 192.168.122.11:58060 (53 connections now open)
2020-05-08T12:16:54.909-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:56386 #182 (54 connections now open)
2020-05-08T12:16:54.909-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:56392 #183 (55 connections now open)
2020-05-08T12:16:54.909-0700 I  NETWORK  [conn182] received client metadata from 192.168.122.1:56386 conn182: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:54.910-0700 I  NETWORK  [conn183] received client metadata from 192.168.122.1:56392 conn183: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:16:54.913-0700 I  NETWORK  [conn182] end connection 192.168.122.1:56386 (54 connections now open)
2020-05-08T12:16:54.913-0700 I  NETWORK  [conn183] end connection 192.168.122.1:56392 (53 connections now open)
2020-05-08T12:16:55.817-0700 I  ELECTION [replexec-1] Starting an election, since we've seen no PRIMARY in the past 1000ms
2020-05-08T12:16:55.817-0700 I  ELECTION [replexec-1] conducting a dry run election to see if we could be elected. current term: 17
2020-05-08T12:16:55.817-0700 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 1054 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 17, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965412, 1), t: 16 } }
2020-05-08T12:16:55.817-0700 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 1055 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 17, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965412, 1), t: 16 } }
2020-05-08T12:16:55.898-0700 I  CONNPOOL [Replication] Connecting to n3:27019
2020-05-08T12:16:56.817-0700 I  ELECTION [replexec-0] VoteRequester(term 17 dry run) failed to receive response from n3:27019: NetworkInterfaceExceededTimeLimit: Request 1055 timed out, deadline was 2020-05-08T12:16:56.817-0700, op was RemoteCommand 1055 -- target:[n3:27019] db:admin expDate:2020-05-08T12:16:56.817-0700 cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 17, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965412, 1), t: 16 } }
2020-05-08T12:16:56.817-0700 I  ELECTION [replexec-3] VoteRequester(term 17 dry run) failed to receive response from n1:27019: NetworkInterfaceExceededTimeLimit: Request 1054 timed out, deadline was 2020-05-08T12:16:56.817-0700, op was RemoteCommand 1054 -- target:[n1:27019] db:admin expDate:2020-05-08T12:16:56.817-0700 cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 17, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965412, 1), t: 16 } }
2020-05-08T12:16:56.817-0700 I  CONNPOOL [Replication] Ending connection to host n3:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:16:56.817-0700 I  ELECTION [replexec-3] not running for primary, we received insufficient votes
2020-05-08T12:16:56.817-0700 I  ELECTION [replexec-3] Lost dry run election due to internal error
2020-05-08T12:16:56.817-0700 I  CONNPOOL [Replication] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:16:56.871-0700 I  ELECTION [replexec-4] Starting an election, since we've seen no PRIMARY in the past 1000ms
2020-05-08T12:16:56.871-0700 I  ELECTION [replexec-4] conducting a dry run election to see if we could be elected. current term: 17
2020-05-08T12:16:56.871-0700 I  REPL     [replexec-4] Scheduling remote command request for vote request: RemoteCommand 1058 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 17, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965412, 1), t: 16 } }
2020-05-08T12:16:56.871-0700 I  REPL     [replexec-4] Scheduling remote command request for vote request: RemoteCommand 1059 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 17, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965412, 1), t: 16 } }
2020-05-08T12:16:56.871-0700 I  CONNPOOL [Replication] Connecting to n1:27019
2020-05-08T12:16:56.898-0700 I  REPL     [replexec-2] Member n3:27019 is now in state RS_DOWN - Couldn't get a connection within the time limit
2020-05-08T12:16:57.746-0700 I  REPL     [replexec-0] Member n1:27019 is now in state RS_DOWN - Request 1057 timed out, deadline was 2020-05-08T12:16:57.746-0700, op was RemoteCommand 1057 -- target:[n1:27019] db:admin expDate:2020-05-08T12:16:57.746-0700 cmd:{ replSetHeartbeat: "rs_config", configVersion: 1, hbv: 1, from: "n2:27019", fromId: 1, term: 17 }
2020-05-08T12:16:57.746-0700 I  CONNPOOL [Replication] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:16:57.871-0700 I  ELECTION [replexec-1] VoteRequester(term 17 dry run) failed to receive response from n1:27019: NetworkInterfaceExceededTimeLimit: Couldn't get a connection within the time limit
2020-05-08T12:16:57.871-0700 I  ELECTION [replexec-3] VoteRequester(term 17 dry run) failed to receive response from n3:27019: NetworkInterfaceExceededTimeLimit: Couldn't get a connection within the time limit
2020-05-08T12:16:57.871-0700 I  ELECTION [replexec-3] not running for primary, we received insufficient votes
2020-05-08T12:16:57.871-0700 I  ELECTION [replexec-3] Lost dry run election due to internal error
2020-05-08T12:16:57.884-0700 I  ELECTION [replexec-2] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:16:58.567-0700 I  NETWORK  [conn76] end connection 192.168.122.15:33992 (52 connections now open)
2020-05-08T12:16:58.567-0700 I  NETWORK  [conn77] end connection 192.168.122.12:47134 (51 connections now open)
2020-05-08T12:16:58.567-0700 I  NETWORK  [conn33] end connection 192.168.122.12:45762 (50 connections now open)
2020-05-08T12:16:59.013-0700 I  ELECTION [replexec-4] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:17:00.117-0700 I  ELECTION [replexec-0] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:17:01.212-0700 I  ELECTION [replexec-3] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:17:02.246-0700 I  ELECTION [replexec-1] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:17:03.009-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:48360 #184 (51 connections now open)
2020-05-08T12:17:03.009-0700 I  NETWORK  [conn184] received client metadata from 192.168.122.13:48360 conn184: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:03.012-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:48498 #185 (52 connections now open)
2020-05-08T12:17:03.012-0700 I  NETWORK  [conn185] received client metadata from 192.168.122.13:48498 conn185: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:03.276-0700 I  ELECTION [replexec-5] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:17:03.297-0700 I  NETWORK  [conn80] end connection 192.168.122.13:45078 (51 connections now open)
2020-05-08T12:17:03.304-0700 I  ELECTION [conn185] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 17, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965412, 1), t: 16 } }
2020-05-08T12:17:03.304-0700 I  ELECTION [conn185] Sending vote response: { term: 17, voteGranted: true, reason: "" }
2020-05-08T12:17:03.308-0700 I  ELECTION [conn185] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 18, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965412, 1), t: 16 } }
2020-05-08T12:17:03.308-0700 I  ELECTION [conn185] Sending vote response: { term: 18, voteGranted: true, reason: "" }
2020-05-08T12:17:03.399-0700 I  REPL     [replexec-4] Member n3:27019 is now in state PRIMARY
2020-05-08T12:17:03.399-0700 I  ELECTION [replexec-4] Scheduling priority takeover at 2020-05-08T12:17:05.410-0700
2020-05-08T12:17:03.585-0700 I  NETWORK  [conn147] end connection 192.168.122.13:46978 (50 connections now open)
2020-05-08T12:17:04.097-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:48224 #187 (51 connections now open)
2020-05-08T12:17:04.097-0700 I  NETWORK  [conn187] received client metadata from 192.168.122.13:48224 conn187: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:04.247-0700 I  REPL     [replexec-4] Member n1:27019 is now in state SECONDARY
2020-05-08T12:17:04.751-0700 I  REPL     [rsBackgroundSync] sync source candidate: n3:27019
2020-05-08T12:17:04.753-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n3:27019
2020-05-08T12:17:05.410-0700 I  REPL     [replexec-1] Canceling priority takeover callback
2020-05-08T12:17:05.410-0700 I  ELECTION [replexec-1] Starting an election for a priority takeover
2020-05-08T12:17:05.410-0700 I  ELECTION [replexec-1] conducting a dry run election to see if we could be elected. current term: 18
2020-05-08T12:17:05.410-0700 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 1091 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 18, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965424, 57), t: 18 } }
2020-05-08T12:17:05.410-0700 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 1092 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 18, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965424, 57), t: 18 } }
2020-05-08T12:17:05.411-0700 I  ELECTION [replexec-5] VoteRequester(term 18 dry run) received a yes vote from n1:27019; response message: { term: 18, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000011') }, lastCommittedOpTime: Timestamp(1588965424, 57), $clusterTime: { clusterTime: Timestamp(1588965424, 57), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965424, 57) }
2020-05-08T12:17:05.411-0700 I  ELECTION [replexec-5] dry election run succeeded, running for election in term 19
2020-05-08T12:17:05.413-0700 I  REPL     [replexec-5] Scheduling remote command request for vote request: RemoteCommand 1093 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 19, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965424, 57), t: 18 } }
2020-05-08T12:17:05.414-0700 I  REPL     [replexec-5] Scheduling remote command request for vote request: RemoteCommand 1094 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 19, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965424, 57), t: 18 } }
2020-05-08T12:17:05.416-0700 I  ELECTION [replexec-1] VoteRequester(term 19) received a yes vote from n1:27019; response message: { term: 19, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000011') }, lastCommittedOpTime: Timestamp(1588965424, 57), $clusterTime: { clusterTime: Timestamp(1588965424, 57), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965424, 57) }
2020-05-08T12:17:05.417-0700 I  ELECTION [replexec-1] election succeeded, assuming primary role in term 19
2020-05-08T12:17:05.417-0700 I  REPL     [replexec-1] transition to PRIMARY from SECONDARY
2020-05-08T12:17:05.417-0700 I  REPL     [replexec-1] Resetting sync source to empty, which was n3:27019
2020-05-08T12:17:05.417-0700 I  CONNPOOL [Replication] Ending connection to host n3:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:17:05.417-0700 I  REPL     [replexec-1] Entering primary catch-up mode.
2020-05-08T12:17:05.418-0700 I  REPL     [replexec-1] Member n3:27019 is now in state SECONDARY
2020-05-08T12:17:05.418-0700 I  REPL     [replexec-1] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1588965424, 57), t: 18 }. My Last Applied: { ts: Timestamp(1588965424, 57), t: 18 }
2020-05-08T12:17:05.418-0700 I  REPL     [replexec-1] Exited primary catch-up mode.
2020-05-08T12:17:05.418-0700 I  REPL     [replexec-1] Stopping replication producer
2020-05-08T12:17:05.418-0700 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2020-05-08T12:17:05.418-0700 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 19
2020-05-08T12:17:05.418-0700 I  CONNPOOL [RS] Ending connection to host n3:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:17:05.419-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:17:05.419-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:17:05.419-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 0 }
2020-05-08T12:17:05.421-0700 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-05-08T12:17:05.421-0700 I  SHARDING [Balancer] CSRS balancer is starting
2020-05-08T12:17:05.422-0700 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-05-08T12:17:05.422-0700 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-05-08T12:17:05.422-0700 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-05-08T12:17:05.633-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:48332 #190 (52 connections now open)
2020-05-08T12:17:05.633-0700 I  NETWORK  [conn190] received client metadata from 192.168.122.13:48332 conn190: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:05.764-0700 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to n3:27019: InvalidSyncSource: Sync source was cleared. Was n3:27019
2020-05-08T12:17:06.014-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:56948 #191 (53 connections now open)
2020-05-08T12:17:06.014-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:56954 #192 (54 connections now open)
2020-05-08T12:17:06.014-0700 I  NETWORK  [conn191] received client metadata from 192.168.122.1:56948 conn191: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:06.014-0700 I  NETWORK  [conn192] received client metadata from 192.168.122.1:56954 conn192: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:06.016-0700 I  NETWORK  [conn191] end connection 192.168.122.1:56948 (53 connections now open)
2020-05-08T12:17:06.016-0700 I  NETWORK  [conn192] end connection 192.168.122.1:56954 (52 connections now open)
2020-05-08T12:17:06.324-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:56962 #193 (53 connections now open)
2020-05-08T12:17:06.324-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:56968 #194 (54 connections now open)
2020-05-08T12:17:06.324-0700 I  NETWORK  [conn193] received client metadata from 192.168.122.1:56962 conn193: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:06.324-0700 I  NETWORK  [conn194] received client metadata from 192.168.122.1:56968 conn194: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:06.328-0700 I  NETWORK  [conn193] end connection 192.168.122.1:56962 (53 connections now open)
2020-05-08T12:17:06.328-0700 I  NETWORK  [conn194] end connection 192.168.122.1:56968 (52 connections now open)
2020-05-08T12:17:06.418-0700 I  REPL     [replexec-0] Member n1:27019 is now in state RS_DOWN - no response within election timeout period
2020-05-08T12:17:06.418-0700 I  REPL     [replexec-0] Member n3:27019 is now in state RS_DOWN - no response within election timeout period
2020-05-08T12:17:06.418-0700 I  REPL     [replexec-0] can't see a majority of the set, relinquishing primary
2020-05-08T12:17:06.418-0700 I  REPL     [replexec-0] Stepping down from primary in response to heartbeat
2020-05-08T12:17:06.418-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:17:06.418-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:17:06.418-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 6, userOpsRunning: 0 }
2020-05-08T12:17:06.419-0700 W  COMMAND  [conn73] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:06.419-0700 I  COMMAND  [conn73] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965425, 5), signature: { hash: BinData(0, 890A476C1C2458024EA1E648B4ACC54C1A01838B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965424, 57), t: 18 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 456ms
2020-05-08T12:17:06.419-0700 W  COMMAND  [conn70] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:06.419-0700 I  COMMAND  [conn70] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965425, 5), signature: { hash: BinData(0, 890A476C1C2458024EA1E648B4ACC54C1A01838B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965424, 57), t: 18 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 457ms
2020-05-08T12:17:06.420-0700 W  COMMAND  [conn57] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:06.420-0700 I  REPL     [replexec-0] transition to SECONDARY from PRIMARY
2020-05-08T12:17:06.420-0700 W  COMMAND  [conn190] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:06.420-0700 I  COMMAND  [conn57] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965425, 4), signature: { hash: BinData(0, 890A476C1C2458024EA1E648B4ACC54C1A01838B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965424, 57), t: 18 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 460ms
2020-05-08T12:17:06.420-0700 I  COMMAND  [conn190] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965425, 5), signature: { hash: BinData(0, 890A476C1C2458024EA1E648B4ACC54C1A01838B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965424, 57), t: 18 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 459ms
2020-05-08T12:17:06.420-0700 W  COMMAND  [conn54] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:06.420-0700 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-05-08T12:17:06.420-0700 I  COMMAND  [conn54] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965425, 4), signature: { hash: BinData(0, 890A476C1C2458024EA1E648B4ACC54C1A01838B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965424, 57), t: 18 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 414ms
2020-05-08T12:17:06.420-0700 W  COMMAND  [conn23] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:06.420-0700 I  COMMAND  [conn23] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/n4:27018,n5:27018,n6:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965425, 5), signature: { hash: BinData(0, 890A476C1C2458024EA1E648B4ACC54C1A01838B), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965424, 57), t: 18 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } protocol:op_msg 459ms
2020-05-08T12:17:06.421-0700 W  TXN      [TransactionCoordinator] Coordinator recovery failed and coordinateCommit requests will not be allowed :: caused by :: TransactionCoordinatorSteppingDown: operation was interrupted
2020-05-08T12:17:07.543-0700 I  ELECTION [replexec-1] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:17:08.418-0700 I  CONNPOOL [Replication] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-05-08T12:17:08.418-0700 I  CONNPOOL [Replication] Ending connection to host n3:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-05-08T12:17:08.418-0700 I  CONNPOOL [Replication] Connecting to n1:27019
2020-05-08T12:17:08.418-0700 I  CONNPOOL [Replication] Connecting to n3:27019
2020-05-08T12:17:08.573-0700 I  ELECTION [replexec-0] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:17:09.539-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:09.574-0700 I  ELECTION [replexec-1] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:17:09.658-0700 I  NETWORK  [conn74] end connection 192.168.122.19:58454 (51 connections now open)
2020-05-08T12:17:09.692-0700 I  NETWORK  [conn24] end connection 192.168.122.16:35808 (50 connections now open)
2020-05-08T12:17:09.693-0700 I  NETWORK  [conn59] end connection 192.168.122.17:45630 (49 connections now open)
2020-05-08T12:17:09.693-0700 I  NETWORK  [conn94] end connection 192.168.122.18:40890 (48 connections now open)
2020-05-08T12:17:10.689-0700 I  ELECTION [replexec-4] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:17:11.201-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:48846 #195 (49 connections now open)
2020-05-08T12:17:11.201-0700 I  NETWORK  [conn195] received client metadata from 192.168.122.13:48846 conn195: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:11.460-0700 I  NETWORK  [conn16] end connection 192.168.122.13:43432 (48 connections now open)
2020-05-08T12:17:11.461-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:48874 #198 (49 connections now open)
2020-05-08T12:17:11.462-0700 I  NETWORK  [conn198] received client metadata from 192.168.122.13:48874 conn198: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:11.541-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard2
2020-05-08T12:17:11.771-0700 I  ELECTION [replexec-5] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:17:11.874-0700 I  NETWORK  [conn117] end connection 192.168.122.11:56850 (48 connections now open)
2020-05-08T12:17:11.875-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:59394 #199 (49 connections now open)
2020-05-08T12:17:11.876-0700 I  NETWORK  [conn199] received client metadata from 192.168.122.11:59394 conn199: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:11.919-0700 I  REPL     [replexec-4] Member n1:27019 is now in state SECONDARY
2020-05-08T12:17:11.919-0700 I  REPL     [replexec-1] Member n3:27019 is now in state PRIMARY
2020-05-08T12:17:11.919-0700 I  ELECTION [replexec-1] Scheduling priority takeover at 2020-05-08T12:17:14.065-0700
2020-05-08T12:17:12.423-0700 I  REPL     [rsBackgroundSync] sync source candidate: n3:27019
2020-05-08T12:17:12.903-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n3:27019
2020-05-08T12:17:12.904-0700 I  CONNPOOL [RS] Connecting to n3:27019
2020-05-08T12:17:12.906-0700 I  REPL     [rsBackgroundSync] Starting rollback due to OplogStartMissing: Our last optime fetched: { ts: Timestamp(1588965425, 2), t: 19 }. source's GTE: { ts: Timestamp(1588965427, 1), t: 20 }
2020-05-08T12:17:12.906-0700 I  REPL     [rsBackgroundSync] Replication commit point: { ts: Timestamp(1588965424, 57), t: 18 }
2020-05-08T12:17:12.906-0700 I  REPL     [rsBackgroundSync] Rollback using 'recoverToStableTimestamp' method.
2020-05-08T12:17:12.906-0700 I  REPL     [rsBackgroundSync] Scheduling rollback (sync source: n3:27019)
2020-05-08T12:17:12.906-0700 I  ROLLBACK [rsBackgroundSync] transition to ROLLBACK
2020-05-08T12:17:12.906-0700 I  REPL     [rsBackgroundSync] State transition ops metrics: { lastStateTransition: "rollback", userOpsKilled: 0, userOpsRunning: 50 }
2020-05-08T12:17:12.906-0700 I  REPL     [rsBackgroundSync] Canceling priority takeover callback
2020-05-08T12:17:12.906-0700 I  REPL     [rsBackgroundSync] transition to ROLLBACK from SECONDARY
2020-05-08T12:17:12.906-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 199
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 198
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 195
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 190
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 187
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 185
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 184
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 148
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 130
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 122
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 114
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 113
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 91
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 79
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 78
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 75
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 73
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 72
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 71
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 70
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 57
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 56
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 54
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 53
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 48
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 47
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 42
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 41
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 40
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 39
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 38
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 37
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 36
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 35
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 34
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 32
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 30
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 29
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 28
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 27
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 26
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 25
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 23
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 22
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 21
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 20
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 19
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 18
2020-05-08T12:17:12.907-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 17
2020-05-08T12:17:12.907-0700 I  ROLLBACK [rsBackgroundSync] Waiting for all background operations to complete before starting rollback
2020-05-08T12:17:12.908-0700 I  ROLLBACK [rsBackgroundSync] Finished waiting for background operations to complete before rollback
2020-05-08T12:17:12.908-0700 I  ROLLBACK [rsBackgroundSync] finding common point
2020-05-08T12:17:12.914-0700 I  ROLLBACK [rsBackgroundSync] Rollback common point is { ts: Timestamp(1588965424, 57), t: 18 }
2020-05-08T12:17:12.921-0700 I  ELECTION [replexec-5] Scheduling priority takeover at 2020-05-08T12:17:15.063-0700
2020-05-08T12:17:12.926-0700 I  REPL     [rsBackgroundSync] Incremented the rollback ID to 4
2020-05-08T12:17:12.926-0700 I  ROLLBACK [rsBackgroundSync] finding record store counts
2020-05-08T12:17:12.927-0700 I  STORAGE  [rsBackgroundSync] closeCatalog: closing all databases
2020-05-08T12:17:12.927-0700 I  STORAGE  [rsBackgroundSync] closeCatalog: closing storage engine catalog
2020-05-08T12:17:12.927-0700 I  STORAGE  [rsBackgroundSync] Deregistering all the collections
2020-05-08T12:17:12.927-0700 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2020-05-08T12:17:13.027-0700 I  ROLLBACK [rsBackgroundSync] Rolling back to the stable timestamp. StableTimestamp: Timestamp(1588965424, 57) Initial Data Timestamp: Timestamp(1588965338, 1)
2020-05-08T12:17:13.028-0700 I  STORAGE  [rsBackgroundSync] openCatalog: loading storage engine catalog
2020-05-08T12:17:13.039-0700 I  STORAGE  [rsBackgroundSync] OplogTruncaterThread local.oplog.rs already started
2020-05-08T12:17:13.039-0700 I  STORAGE  [rsBackgroundSync] The size storer reports that the oplog contains 268 records totaling to 57370 bytes
2020-05-08T12:17:13.039-0700 I  STORAGE  [rsBackgroundSync] Scanning the oplog to determine where to place markers for truncation
2020-05-08T12:17:13.039-0700 I  STORAGE  [rsBackgroundSync] WiredTiger record store oplog processing took 0ms
2020-05-08T12:17:13.042-0700 I  NETWORK  [conn56] end connection 192.168.122.14:49632 (48 connections now open)
2020-05-08T12:17:13.043-0700 I  STORAGE  [rsBackgroundSync] openCatalog: reconciling catalog and idents
2020-05-08T12:17:13.043-0700 I  STORAGE  [rsBackgroundSync] openCatalog: reopening all databases
2020-05-08T12:17:13.051-0700 I  NETWORK  [conn75] end connection 192.168.122.15:33990 (47 connections now open)
2020-05-08T12:17:13.057-0700 I  ELECTION [conn185] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 20, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965424, 57), t: 18 } }
2020-05-08T12:17:13.060-0700 I  ELECTION [conn185] Sending vote response: { term: 22, voteGranted: false, reason: "candidate's term (20) is lower than mine (22)" }
2020-05-08T12:17:13.059-0700 I  STORAGE  [rsBackgroundSync] openCatalog: updating cached oplog pointer
2020-05-08T12:17:13.060-0700 I  STORAGE  [rsBackgroundSync] openCatalog: finished reloading collection catalog
2020-05-08T12:17:13.060-0700 I  STORAGE  [rsBackgroundSync] recoverToStableTimestamp successful. Stable Timestamp: Timestamp(1588965424, 57)
2020-05-08T12:17:13.060-0700 I  ELECTION [conn187] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 19, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965424, 57), t: 18 } }
2020-05-08T12:17:13.060-0700 I  ROLLBACK [rsBackgroundSync] Rollback reverted 0 insert operations, 0 update operations and 0 delete operations.
2020-05-08T12:17:13.060-0700 I  ELECTION [conn187] Sending vote response: { term: 22, voteGranted: false, reason: "candidate's term (19) is lower than mine (22)" }
2020-05-08T12:17:13.060-0700 I  ROLLBACK [rsBackgroundSync] Marking to truncate all oplog entries with timestamps greater than or equal to { ts: Timestamp(1588965425, 2), t: 19 }
2020-05-08T12:17:13.060-0700 I  NETWORK  [conn185] end connection 192.168.122.13:48498 (46 connections now open)
2020-05-08T12:17:13.060-0700 I  NETWORK  [conn187] end connection 192.168.122.13:48224 (45 connections now open)
2020-05-08T12:17:13.060-0700 I  REPL     [rsBackgroundSync] Removing unapplied entries starting at: { : Timestamp(1588965425, 2) }
2020-05-08T12:17:13.060-0700 I  REPL     [rsBackgroundSync] Replication recovery oplog truncation finished in: 0ms
2020-05-08T12:17:13.083-0700 I  REPL     [rsBackgroundSync] Recovering from stable timestamp: Timestamp(1588965424, 57) (top of oplog: { ts: Timestamp(1588965424, 57), t: 18 }, appliedThrough: { ts: Timestamp(0, 0), t: -1 }, TruncateAfter: Timestamp(0, 0))
2020-05-08T12:17:13.083-0700 I  REPL     [rsBackgroundSync] Starting recovery oplog application at the stable timestamp: Timestamp(1588965424, 57)
2020-05-08T12:17:13.083-0700 I  REPL     [rsBackgroundSync] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2020-05-08T12:17:13.083-0700 I  REPL     [rsBackgroundSync] Not updating committed snapshot because we are in rollback
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] Triggering the rollback op observer
2020-05-08T12:17:13.084-0700 I  REPL     [rsBackgroundSync] Resetting key manager cache
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] Rollback complete
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] Rollback summary:
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 	start time: 2020-05-08T12:17:12.906-0700
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 	end time: 2020-05-08T12:17:13.084-0700
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 	sync source: n3:27019
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 	rollback data file directory: none; no files written
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 	rollback id: 4
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 	last optime on branch of history rolled back: { ts: Timestamp(1588965425, 2), t: 19 }
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 	common point optime: { ts: Timestamp(1588965424, 57), t: 18 }
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 	last wall clock time on the branch of history rolled back: 2020-05-08T12:17:05.420-0700
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 	wall clock time of the first operation after the common point: 2020-05-08T12:17:05.420-0700
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 	difference in wall clock times: 0 second(s)
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 	truncate timestamp: Timestamp(1588965425, 2)
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 	stable timestamp: Timestamp(1588965424, 57)
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 	shard identity document rolled back: false
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 	config server config version document rolled back: false
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 	affected sessions: none
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 	affected namespaces: none
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 	counts of interesting commands rolled back: 
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 		insert: 0
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 		delete: 0
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 		update: 0
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] 	total number of entries rolled back (including no-ops): 1
2020-05-08T12:17:13.084-0700 I  ROLLBACK [rsBackgroundSync] transition to SECONDARY
2020-05-08T12:17:13.084-0700 I  REPL     [rsBackgroundSync] transition to SECONDARY from ROLLBACK
2020-05-08T12:17:13.084-0700 I  REPL     [rsBackgroundSync] Resetting sync source to empty, which was n3:27019
2020-05-08T12:17:13.084-0700 I  REPL     [rsBackgroundSync] Rollback successful.
2020-05-08T12:17:13.084-0700 I  REPL     [rsBackgroundSync] Stopping replication producer
2020-05-08T12:17:13.084-0700 I  REPL     [rsBackgroundSync] Resetting last fetched optimes in bgsync
2020-05-08T12:17:13.084-0700 I  REPL     [rsBackgroundSync] sync source candidate: n1:27019
2020-05-08T12:17:13.085-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n1:27019
2020-05-08T12:17:13.085-0700 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to n3:27019: InvalidSyncSource: Sync source changed from n3:27019 to n1:27019
2020-05-08T12:17:13.085-0700 I  CONNPOOL [RS] Connecting to n1:27019
2020-05-08T12:17:13.569-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:48718 #203 (46 connections now open)
2020-05-08T12:17:13.569-0700 I  NETWORK  [conn203] received client metadata from 192.168.122.13:48718 conn203: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:14.534-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:59532 #204 (47 connections now open)
2020-05-08T12:17:14.534-0700 I  NETWORK  [conn204] received client metadata from 192.168.122.11:59532 conn204: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:14.607-0700 I  ELECTION [conn204] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 22, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965433, 8), t: 22 } }
2020-05-08T12:17:14.607-0700 I  ELECTION [conn204] Sending vote response: { term: 22, voteGranted: true, reason: "" }
2020-05-08T12:17:14.616-0700 I  REPL     [conn204] Canceling priority takeover callback
2020-05-08T12:17:14.616-0700 I  ELECTION [conn204] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 23, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965433, 8), t: 22 } }
2020-05-08T12:17:14.616-0700 I  ELECTION [conn204] Sending vote response: { term: 23, voteGranted: true, reason: "" }
2020-05-08T12:17:14.630-0700 I  REPL     [replication-1] Restarting oplog query due to error: CappedPositionLost: error in fetcher batch callback :: caused by :: CollectionScan died due to failure to restore tailable cursor position. Last seen record id: RecordId(6824554573504446466). Last fetched optime: { ts: Timestamp(1588965434, 2), t: 23 }. Restarts remaining: 1
2020-05-08T12:17:14.630-0700 I  REPL     [replication-1] Scheduled new oplog query Fetcher source: n1:27019 database: local query: { find: "oplog.rs", filter: { ts: { $gte: Timestamp(1588965434, 2) } }, tailable: true, oplogReplay: true, awaitData: true, maxTimeMS: 2000, batchSize: 13981010, term: 23, readConcern: { afterClusterTime: Timestamp(0, 1) } } query metadata: { $replData: 1, $oplogQueryData: 1, $readPreference: { mode: "secondaryPreferred" } } active: 1 findNetworkTimeout: 7000ms getMoreNetworkTimeout: 5500ms shutting down?: 0 first: 1 firstCommandScheduler: RemoteCommandRetryScheduler request: RemoteCommand 1146 -- target:n1:27019 db:local cmd:{ find: "oplog.rs", filter: { ts: { $gte: Timestamp(1588965434, 2) } }, tailable: true, oplogReplay: true, awaitData: true, maxTimeMS: 2000, batchSize: 13981010, term: 23, readConcern: { afterClusterTime: Timestamp(0, 1) } } active: 1 callbackHandle.valid: 1 callbackHandle.cancelled: 0 attempt: 1 retryPolicy: {type: "NoRetryPolicy"}
2020-05-08T12:17:14.633-0700 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: Sync source must be ahead of me. My last fetched oplog optime: { ts: Timestamp(1588965434, 2), t: 23 }, latest oplog optime of sync source: { ts: Timestamp(1588965434, 2), t: 23 }
2020-05-08T12:17:14.633-0700 I  REPL     [rsBackgroundSync] Clearing sync source n1:27019 to choose a new one.
2020-05-08T12:17:14.633-0700 I  REPL     [rsBackgroundSync] could not find member to sync from
2020-05-08T12:17:14.634-0700 I  REPL     [replexec-3] Member n3:27019 is now in state SECONDARY
2020-05-08T12:17:14.634-0700 I  REPL     [replexec-4] Member n1:27019 is now in state PRIMARY
2020-05-08T12:17:14.634-0700 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to n1:27019: InvalidSyncSource: Sync source was cleared. Was n1:27019
2020-05-08T12:17:15.105-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:48358 #205 (48 connections now open)
2020-05-08T12:17:15.105-0700 I  NETWORK  [conn205] received client metadata from 192.168.122.13:48358 conn205: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:15.395-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:57292 #206 (49 connections now open)
2020-05-08T12:17:15.396-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:57296 #207 (50 connections now open)
2020-05-08T12:17:15.396-0700 I  NETWORK  [conn206] received client metadata from 192.168.122.1:57292 conn206: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:15.396-0700 I  NETWORK  [conn207] received client metadata from 192.168.122.1:57296 conn207: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:15.399-0700 I  NETWORK  [conn206] end connection 192.168.122.1:57292 (49 connections now open)
2020-05-08T12:17:15.399-0700 I  NETWORK  [conn207] end connection 192.168.122.1:57296 (48 connections now open)
2020-05-08T12:17:15.634-0700 I  REPL     [rsBackgroundSync] sync source candidate: n1:27019
2020-05-08T12:17:15.642-0700 I  ELECTION [conn203] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 23, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965433, 8), t: 22 } }
2020-05-08T12:17:15.642-0700 I  ELECTION [conn203] Sending vote response: { term: 23, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965433, 8), t: 22 }, my last applied OpTime: { ts: Timestam..." }
2020-05-08T12:17:15.846-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n1:27019
2020-05-08T12:17:17.915-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:57370 #208 (49 connections now open)
2020-05-08T12:17:17.916-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:57372 #209 (50 connections now open)
2020-05-08T12:17:17.916-0700 I  NETWORK  [conn208] received client metadata from 192.168.122.1:57370 conn208: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:17.916-0700 I  NETWORK  [conn209] received client metadata from 192.168.122.1:57372 conn209: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:17.919-0700 I  NETWORK  [conn208] end connection 192.168.122.1:57370 (49 connections now open)
2020-05-08T12:17:17.919-0700 I  NETWORK  [conn209] end connection 192.168.122.1:57372 (48 connections now open)
2020-05-08T12:17:18.995-0700 I  ELECTION [replexec-1] Starting an election, since we've seen no PRIMARY in the past 1000ms
2020-05-08T12:17:18.995-0700 I  ELECTION [replexec-1] conducting a dry run election to see if we could be elected. current term: 23
2020-05-08T12:17:18.995-0700 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 1175 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 23, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965434, 11), t: 23 } }
2020-05-08T12:17:18.995-0700 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 1176 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 23, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965434, 11), t: 23 } }
2020-05-08T12:17:18.996-0700 I  ELECTION [replexec-4] VoteRequester(term 23 dry run) received a yes vote from n3:27019; response message: { term: 23, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000016') }, lastCommittedOpTime: Timestamp(1588965434, 11), $clusterTime: { clusterTime: Timestamp(1588965438, 123), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965434, 11) }
2020-05-08T12:17:18.997-0700 I  ELECTION [replexec-4] dry election run succeeded, running for election in term 24
2020-05-08T12:17:18.997-0700 I  CONNPOOL [Replication] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-05-08T12:17:18.997-0700 I  CONNPOOL [Replication] Connecting to n1:27019
2020-05-08T12:17:19.001-0700 I  REPL     [replexec-4] Scheduling remote command request for vote request: RemoteCommand 1177 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 24, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965434, 11), t: 23 } }
2020-05-08T12:17:19.002-0700 I  REPL     [replexec-4] Scheduling remote command request for vote request: RemoteCommand 1178 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 24, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965434, 11), t: 23 } }
2020-05-08T12:17:19.006-0700 I  ELECTION [replexec-1] VoteRequester(term 24) received a yes vote from n3:27019; response message: { term: 24, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000016') }, lastCommittedOpTime: Timestamp(1588965434, 11), $clusterTime: { clusterTime: Timestamp(1588965438, 123), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965434, 11) }
2020-05-08T12:17:19.009-0700 I  ELECTION [replexec-5] election succeeded, assuming primary role in term 24
2020-05-08T12:17:19.009-0700 I  REPL     [replexec-5] transition to PRIMARY from SECONDARY
2020-05-08T12:17:19.009-0700 I  REPL     [replexec-5] Resetting sync source to empty, which was n1:27019
2020-05-08T12:17:19.009-0700 I  REPL     [replexec-5] Entering primary catch-up mode.
2020-05-08T12:17:20.009-0700 I  REPL     [replexec-2] Catchup timed out after becoming primary.
2020-05-08T12:17:20.009-0700 I  REPL     [replexec-2] Exited primary catch-up mode.
2020-05-08T12:17:20.009-0700 I  REPL     [replexec-2] Stopping replication producer
2020-05-08T12:17:20.009-0700 I  REPL     [replexec-5] Member n1:27019 is now in state RS_DOWN - no response within election timeout period
2020-05-08T12:17:20.009-0700 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2020-05-08T12:17:20.009-0700 I  REPL     [replexec-5] Member n3:27019 is now in state RS_DOWN - no response within election timeout period
2020-05-08T12:17:20.009-0700 I  CONNPOOL [RS] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:17:20.009-0700 I  REPL     [replexec-5] can't see a majority of the set, relinquishing primary
2020-05-08T12:17:20.009-0700 I  REPL     [replexec-5] Stepping down from primary in response to heartbeat
2020-05-08T12:17:20.009-0700 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 24
2020-05-08T12:17:20.009-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:17:20.010-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:17:20.010-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:17:20.010-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 0, userOpsRunning: 1 }
2020-05-08T12:17:20.010-0700 I  REPL     [replexec-5] transition to SECONDARY from PRIMARY
2020-05-08T12:17:20.010-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:17:20.010-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 1 }
2020-05-08T12:17:20.690-0700 I  ELECTION [conn203] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 24, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965434, 11), t: 23 } }
2020-05-08T12:17:20.690-0700 I  ELECTION [conn203] Sending vote response: { term: 24, voteGranted: true, reason: "" }
2020-05-08T12:17:20.707-0700 I  ELECTION [conn203] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 25, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965434, 11), t: 23 } }
2020-05-08T12:17:20.707-0700 I  ELECTION [conn203] Sending vote response: { term: 25, voteGranted: true, reason: "" }
2020-05-08T12:17:21.010-0700 I  REPL     [replexec-2] Member n3:27019 is now in state PRIMARY
2020-05-08T12:17:21.010-0700 I  ELECTION [replexec-2] Scheduling priority takeover at 2020-05-08T12:17:23.094-0700
2020-05-08T12:17:21.953-0700 I  NETWORK  [conn204] end connection 192.168.122.11:59532 (47 connections now open)
2020-05-08T12:17:21.972-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:57548 #210 (48 connections now open)
2020-05-08T12:17:21.973-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:57556 #211 (49 connections now open)
2020-05-08T12:17:21.973-0700 I  NETWORK  [conn210] received client metadata from 192.168.122.1:57548 conn210: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:21.973-0700 I  NETWORK  [conn211] received client metadata from 192.168.122.1:57556 conn211: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:21.976-0700 I  NETWORK  [conn210] end connection 192.168.122.1:57548 (48 connections now open)
2020-05-08T12:17:21.976-0700 I  NETWORK  [conn211] end connection 192.168.122.1:57556 (47 connections now open)
2020-05-08T12:17:22.011-0700 I  REPL     [rsBackgroundSync] sync source candidate: n3:27019
2020-05-08T12:17:22.018-0700 I  REPL     [replexec-4] Member n1:27019 is now in state SECONDARY
2020-05-08T12:17:22.523-0700 I  REPL     [replexec-3] Canceling priority takeover callback
2020-05-08T12:17:22.523-0700 I  ELECTION [replexec-3] Starting an election, since we've seen no PRIMARY in the past 1000ms
2020-05-08T12:17:22.523-0700 I  ELECTION [replexec-3] conducting a dry run election to see if we could be elected. current term: 25
2020-05-08T12:17:22.523-0700 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 1186 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 25, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965434, 11), t: 23 } }
2020-05-08T12:17:22.523-0700 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 1187 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 25, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965434, 11), t: 23 } }
2020-05-08T12:17:22.523-0700 I  CONNPOOL [Replication] Connecting to n3:27019
2020-05-08T12:17:22.524-0700 I  ELECTION [replexec-2] VoteRequester(term 25 dry run) received a no vote from n1:27019 with reason "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965434, 11), t: 23 }, my last applied OpTime: { ts: Timestamp(1588965438, 3), t: 23 }"; response message: { term: 25, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965434, 11), t: 23 }, my last applied OpTime: { ts: Timesta...", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000017') }, lastCommittedOpTime: Timestamp(1588965434, 11), $clusterTime: { clusterTime: Timestamp(1588965442, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965438, 3) }
2020-05-08T12:17:22.657-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:59754 #214 (48 connections now open)
2020-05-08T12:17:22.657-0700 I  NETWORK  [conn214] received client metadata from 192.168.122.11:59754 conn214: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:23.011-0700 I  REPL     [replexec-0] Member n3:27019 is now in state RS_DOWN - Request 1184 timed out, deadline was 2020-05-08T12:17:23.011-0700, op was RemoteCommand 1184 -- target:[n3:27019] db:admin expDate:2020-05-08T12:17:23.011-0700 cmd:{ replSetHeartbeat: "rs_config", configVersion: 1, hbv: 1, from: "n2:27019", fromId: 1, term: 25 }
2020-05-08T12:17:23.011-0700 I  CONNPOOL [Replication] Ending connection to host n3:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:17:23.523-0700 I  ELECTION [replexec-4] VoteRequester(term 25 dry run) failed to receive response from n3:27019: NetworkInterfaceExceededTimeLimit: Couldn't get a connection within the time limit
2020-05-08T12:17:23.523-0700 I  ELECTION [replexec-4] not running for primary, we received insufficient votes
2020-05-08T12:17:23.523-0700 I  ELECTION [replexec-4] Lost dry run election due to internal error
2020-05-08T12:17:23.528-0700 I  ELECTION [replexec-3] Starting an election, since we've seen no PRIMARY in the past 1000ms
2020-05-08T12:17:23.528-0700 I  ELECTION [replexec-3] conducting a dry run election to see if we could be elected. current term: 25
2020-05-08T12:17:23.528-0700 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 1188 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 25, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965434, 11), t: 23 } }
2020-05-08T12:17:23.528-0700 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 1189 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 25, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965434, 11), t: 23 } }
2020-05-08T12:17:23.529-0700 I  ELECTION [replexec-2] VoteRequester(term 25 dry run) received a no vote from n1:27019 with reason "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965434, 11), t: 23 }, my last applied OpTime: { ts: Timestamp(1588965438, 3), t: 23 }"; response message: { term: 25, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965434, 11), t: 23 }, my last applied OpTime: { ts: Timesta...", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000017') }, lastCommittedOpTime: Timestamp(1588965434, 11), $clusterTime: { clusterTime: Timestamp(1588965442, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965438, 3) }
2020-05-08T12:17:23.627-0700 I  ELECTION [conn214] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 25, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965438, 3), t: 23 } }
2020-05-08T12:17:23.627-0700 I  ELECTION [conn214] Sending vote response: { term: 25, voteGranted: true, reason: "" }
2020-05-08T12:17:23.632-0700 I  ELECTION [conn214] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 26, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965438, 3), t: 23 } }
2020-05-08T12:17:23.632-0700 I  ELECTION [conn214] Sending vote response: { term: 26, voteGranted: true, reason: "" }
2020-05-08T12:17:24.019-0700 I  REPL     [replexec-4] Member n1:27019 is now in state PRIMARY
2020-05-08T12:17:24.528-0700 I  ELECTION [replexec-2] VoteRequester(term 25 dry run) failed to receive response from n3:27019: NetworkInterfaceExceededTimeLimit: Couldn't get a connection within the time limit
2020-05-08T12:17:24.528-0700 I  ELECTION [replexec-2] not running for primary, we have been superseded already during dry run. original term: 25, current term: 26
2020-05-08T12:17:24.528-0700 I  ELECTION [replexec-2] Lost dry run election due to internal error
2020-05-08T12:17:25.117-0700 I  ELECTION [replexec-3] Starting an election, since we've seen no PRIMARY in the past 1000ms
2020-05-08T12:17:25.117-0700 I  ELECTION [replexec-3] conducting a dry run election to see if we could be elected. current term: 26
2020-05-08T12:17:25.117-0700 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 1192 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 26, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965434, 11), t: 23 } }
2020-05-08T12:17:25.117-0700 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 1193 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 26, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965434, 11), t: 23 } }
2020-05-08T12:17:25.117-0700 I  CONNPOOL [Replication] Connecting to n3:27019
2020-05-08T12:17:25.118-0700 I  ELECTION [replexec-4] VoteRequester(term 26 dry run) received a no vote from n1:27019 with reason "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965434, 11), t: 23 }, my last applied OpTime: { ts: Timestamp(1588965444, 5), t: 26 }"; response message: { term: 26, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965434, 11), t: 23 }, my last applied OpTime: { ts: Timesta...", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000001a') }, lastCommittedOpTime: Timestamp(1588965434, 11), $clusterTime: { clusterTime: Timestamp(1588965444, 5), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965444, 5) }
2020-05-08T12:17:25.120-0700 I  REPL     [replexec-0] Member n3:27019 is now in state SECONDARY
2020-05-08T12:17:25.120-0700 I  ELECTION [replexec-2] VoteRequester(term 26 dry run) received a no vote from n3:27019 with reason "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965434, 11), t: 23 }, my last applied OpTime: { ts: Timestamp(1588965441, 26), t: 25 }"; response message: { term: 26, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965434, 11), t: 23 }, my last applied OpTime: { ts: Timesta...", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000019') }, lastCommittedOpTime: Timestamp(1588965434, 11), $clusterTime: { clusterTime: Timestamp(1588965444, 5), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965441, 26) }
2020-05-08T12:17:25.120-0700 I  ELECTION [replexec-2] not running for primary, we received insufficient votes
2020-05-08T12:17:25.120-0700 I  ELECTION [replexec-2] Lost dry run election due to internal error
2020-05-08T12:17:25.156-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:57714 #216 (49 connections now open)
2020-05-08T12:17:25.157-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:57720 #217 (50 connections now open)
2020-05-08T12:17:25.157-0700 I  NETWORK  [conn216] received client metadata from 192.168.122.1:57714 conn216: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:25.157-0700 I  NETWORK  [conn217] received client metadata from 192.168.122.1:57720 conn217: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:25.158-0700 I  NETWORK  [conn216] end connection 192.168.122.1:57714 (49 connections now open)
2020-05-08T12:17:25.158-0700 I  NETWORK  [conn217] end connection 192.168.122.1:57720 (48 connections now open)
2020-05-08T12:17:25.311-0700 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to n1:27019: InvalidSyncSource: Sync source was cleared. Was n1:27019
2020-05-08T12:17:25.311-0700 I  NETWORK  [conn79] end connection 192.168.122.13:45048 (47 connections now open)
2020-05-08T12:17:25.346-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n3:27019
2020-05-08T12:17:25.578-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:57782 #219 (48 connections now open)
2020-05-08T12:17:25.579-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:57788 #220 (49 connections now open)
2020-05-08T12:17:25.579-0700 I  NETWORK  [conn219] received client metadata from 192.168.122.1:57782 conn219: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:25.579-0700 I  NETWORK  [conn220] received client metadata from 192.168.122.1:57788 conn220: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:25.582-0700 I  NETWORK  [conn219] end connection 192.168.122.1:57782 (48 connections now open)
2020-05-08T12:17:25.582-0700 I  NETWORK  [conn220] end connection 192.168.122.1:57788 (47 connections now open)
2020-05-08T12:17:25.851-0700 I  REPL     [replication-0] Choosing new sync source. Our current sync source is not primary and does not have a sync source, so we require that it is ahead of us. Current sync source: n3:27019, my last fetched oplog optime: { ts: Timestamp(1588965441, 26), t: 25 }, latest oplog optime of sync source: { ts: Timestamp(1588965441, 26), t: 25 } (sync source does not know the primary)
2020-05-08T12:17:25.851-0700 I  REPL     [replication-0] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: n3:27019, OpTime { ts: Timestamp(1588965441, 26), t: 25 }, its sync source index:-1
2020-05-08T12:17:25.851-0700 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source n3:27019 (config version: 1; last applied optime: { ts: Timestamp(1588965441, 26), t: 25 }; sync source index: -1; primary index: -1) is no longer valid
2020-05-08T12:17:25.851-0700 I  REPL     [rsBackgroundSync] Clearing sync source n3:27019 to choose a new one.
2020-05-08T12:17:25.851-0700 I  REPL     [rsBackgroundSync] could not find member to sync from
2020-05-08T12:17:25.852-0700 I  REPL     [replexec-0] Member n1:27019 is now in state SECONDARY
2020-05-08T12:17:25.854-0700 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to n3:27019: InvalidSyncSource: Sync source was cleared. Was n3:27019
2020-05-08T12:17:25.933-0700 I  NETWORK  [conn19] end connection 192.168.122.13:43448 (46 connections now open)
2020-05-08T12:17:26.053-0700 I  NETWORK  [conn203] end connection 192.168.122.13:48718 (45 connections now open)
2020-05-08T12:17:26.097-0700 I  ELECTION [conn195] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 26, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965441, 26), t: 25 } }
2020-05-08T12:17:26.097-0700 I  ELECTION [conn195] Sending vote response: { term: 26, voteGranted: true, reason: "" }
2020-05-08T12:17:26.101-0700 I  ELECTION [conn195] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 27, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965441, 26), t: 25 } }
2020-05-08T12:17:26.101-0700 I  ELECTION [conn195] Sending vote response: { term: 27, voteGranted: true, reason: "" }
2020-05-08T12:17:26.148-0700 I  ELECTION [conn214] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 26, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965444, 5), t: 26 } }
2020-05-08T12:17:26.149-0700 I  ELECTION [conn214] Sending vote response: { term: 27, voteGranted: false, reason: "candidate's term (26) is lower than mine (27)" }
2020-05-08T12:17:26.299-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:57800 #221 (46 connections now open)
2020-05-08T12:17:26.300-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:57806 #222 (47 connections now open)
2020-05-08T12:17:26.300-0700 I  NETWORK  [conn221] received client metadata from 192.168.122.1:57800 conn221: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:26.300-0700 I  NETWORK  [conn222] received client metadata from 192.168.122.1:57806 conn222: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:26.303-0700 I  NETWORK  [conn221] end connection 192.168.122.1:57800 (46 connections now open)
2020-05-08T12:17:26.303-0700 I  NETWORK  [conn222] end connection 192.168.122.1:57806 (45 connections now open)
2020-05-08T12:17:26.557-0700 I  REPL     [replexec-2] Member n3:27019 is now in state PRIMARY
2020-05-08T12:17:26.557-0700 I  ELECTION [replexec-2] Scheduling priority takeover at 2020-05-08T12:17:28.698-0700
2020-05-08T12:17:26.722-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:57824 #223 (46 connections now open)
2020-05-08T12:17:26.723-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:57830 #224 (47 connections now open)
2020-05-08T12:17:26.723-0700 I  NETWORK  [conn223] received client metadata from 192.168.122.1:57824 conn223: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:26.723-0700 I  NETWORK  [conn224] received client metadata from 192.168.122.1:57830 conn224: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:26.726-0700 I  NETWORK  [conn223] end connection 192.168.122.1:57824 (46 connections now open)
2020-05-08T12:17:26.726-0700 I  NETWORK  [conn224] end connection 192.168.122.1:57830 (45 connections now open)
2020-05-08T12:17:26.852-0700 I  REPL     [rsBackgroundSync] sync source candidate: n1:27019
2020-05-08T12:17:26.854-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n1:27019
2020-05-08T12:17:26.854-0700 I  CONNPOOL [RS] Connecting to n1:27019
2020-05-08T12:17:26.856-0700 I  REPL     [rsBackgroundSync] Starting rollback due to OplogStartMissing: Our last optime fetched: { ts: Timestamp(1588965441, 26), t: 25 }. source's GTE: { ts: Timestamp(1588965444, 1), t: 26 }
2020-05-08T12:17:26.856-0700 I  REPL     [rsBackgroundSync] Replication commit point: { ts: Timestamp(1588965434, 11), t: 23 }
2020-05-08T12:17:26.856-0700 I  REPL     [rsBackgroundSync] Rollback using 'recoverToStableTimestamp' method.
2020-05-08T12:17:26.856-0700 I  REPL     [rsBackgroundSync] Scheduling rollback (sync source: n1:27019)
2020-05-08T12:17:26.856-0700 I  ROLLBACK [rsBackgroundSync] transition to ROLLBACK
2020-05-08T12:17:26.856-0700 I  REPL     [rsBackgroundSync] State transition ops metrics: { lastStateTransition: "rollback", userOpsKilled: 0, userOpsRunning: 46 }
2020-05-08T12:17:26.856-0700 I  REPL     [rsBackgroundSync] Canceling priority takeover callback
2020-05-08T12:17:26.856-0700 I  REPL     [rsBackgroundSync] transition to ROLLBACK from SECONDARY
2020-05-08T12:17:26.856-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 214
2020-05-08T12:17:26.856-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 205
2020-05-08T12:17:26.856-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 199
2020-05-08T12:17:26.856-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 198
2020-05-08T12:17:26.856-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 195
2020-05-08T12:17:26.856-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 190
2020-05-08T12:17:26.856-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 184
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 148
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 130
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 122
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 114
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 113
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 91
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 78
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 73
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 72
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 71
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 70
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 57
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 54
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 53
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 48
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 47
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 42
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 41
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 40
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 39
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 38
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 37
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 36
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 35
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 34
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 32
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 30
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 29
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 28
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 27
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 26
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 25
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 23
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 22
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 21
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 20
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 18
2020-05-08T12:17:26.857-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 17
2020-05-08T12:17:26.857-0700 I  ROLLBACK [rsBackgroundSync] Waiting for all background operations to complete before starting rollback
2020-05-08T12:17:26.857-0700 I  ROLLBACK [rsBackgroundSync] Finished waiting for background operations to complete before rollback
2020-05-08T12:17:26.857-0700 I  ROLLBACK [rsBackgroundSync] finding common point
2020-05-08T12:17:26.865-0700 I  ROLLBACK [rsBackgroundSync] Rollback common point is { ts: Timestamp(1588965434, 11), t: 23 }
2020-05-08T12:17:26.871-0700 I  REPL     [rsBackgroundSync] Incremented the rollback ID to 5
2020-05-08T12:17:26.871-0700 I  ROLLBACK [rsBackgroundSync] finding record store counts
2020-05-08T12:17:26.871-0700 I  ROLLBACK [rsBackgroundSync] Preparing to write deleted documents to a rollback file for collection config.lockpings with uuid 81acf486-601b-4e2b-8ee1-bbf74a1edd96 to /var/lib/mongodb/rollback/config.lockpings/removed.2020-05-08T19-17-26.3.bson
2020-05-08T12:17:26.872-0700 I  STORAGE  [rsBackgroundSync] closeCatalog: closing all databases
2020-05-08T12:17:26.872-0700 I  STORAGE  [rsBackgroundSync] closeCatalog: closing storage engine catalog
2020-05-08T12:17:26.872-0700 I  STORAGE  [rsBackgroundSync] Deregistering all the collections
2020-05-08T12:17:26.872-0700 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2020-05-08T12:17:26.949-0700 I  ROLLBACK [rsBackgroundSync] Rolling back to the stable timestamp. StableTimestamp: Timestamp(1588965434, 11) Initial Data Timestamp: Timestamp(1588965338, 1)
2020-05-08T12:17:26.950-0700 I  STORAGE  [rsBackgroundSync] openCatalog: loading storage engine catalog
2020-05-08T12:17:26.962-0700 I  STORAGE  [rsBackgroundSync] OplogTruncaterThread local.oplog.rs already started
2020-05-08T12:17:26.962-0700 I  STORAGE  [rsBackgroundSync] The size storer reports that the oplog contains 294 records totaling to 62541 bytes
2020-05-08T12:17:26.962-0700 I  STORAGE  [rsBackgroundSync] Scanning the oplog to determine where to place markers for truncation
2020-05-08T12:17:26.962-0700 I  STORAGE  [rsBackgroundSync] WiredTiger record store oplog processing took 0ms
2020-05-08T12:17:26.966-0700 I  STORAGE  [rsBackgroundSync] openCatalog: reconciling catalog and idents
2020-05-08T12:17:26.966-0700 I  STORAGE  [rsBackgroundSync] openCatalog: reopening all databases
2020-05-08T12:17:26.982-0700 I  STORAGE  [rsBackgroundSync] openCatalog: updating cached oplog pointer
2020-05-08T12:17:26.982-0700 I  STORAGE  [rsBackgroundSync] openCatalog: finished reloading collection catalog
2020-05-08T12:17:26.982-0700 I  STORAGE  [rsBackgroundSync] recoverToStableTimestamp successful. Stable Timestamp: Timestamp(1588965434, 11)
2020-05-08T12:17:26.982-0700 I  ROLLBACK [rsBackgroundSync] Rollback reverted 0 insert operations, 6 update operations and 0 delete operations.
2020-05-08T12:17:26.982-0700 I  ROLLBACK [rsBackgroundSync] Marking to truncate all oplog entries with timestamps greater than or equal to { ts: Timestamp(1588965441, 1), t: 25 }
2020-05-08T12:17:26.983-0700 I  REPL     [rsBackgroundSync] Removing unapplied entries starting at: { : Timestamp(1588965441, 1) }
2020-05-08T12:17:26.983-0700 I  REPL     [rsBackgroundSync] Replication recovery oplog truncation finished in: 0ms
2020-05-08T12:17:26.986-0700 I  REPL     [rsBackgroundSync] Recovering from stable timestamp: Timestamp(1588965434, 11) (top of oplog: { ts: Timestamp(1588965434, 11), t: 23 }, appliedThrough: { ts: Timestamp(1588965434, 11), t: 23 }, TruncateAfter: Timestamp(0, 0))
2020-05-08T12:17:26.986-0700 I  REPL     [rsBackgroundSync] Starting recovery oplog application at the stable timestamp: Timestamp(1588965434, 11)
2020-05-08T12:17:26.986-0700 I  REPL     [rsBackgroundSync] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2020-05-08T12:17:26.986-0700 I  REPL     [rsBackgroundSync] Not updating committed snapshot because we are in rollback
2020-05-08T12:17:26.986-0700 I  ROLLBACK [rsBackgroundSync] Triggering the rollback op observer
2020-05-08T12:17:26.986-0700 I  REPL     [rsBackgroundSync] Resetting key manager cache
2020-05-08T12:17:26.986-0700 I  ROLLBACK [rsBackgroundSync] Rollback complete
2020-05-08T12:17:26.986-0700 I  ROLLBACK [rsBackgroundSync] Rollback summary:
2020-05-08T12:17:26.986-0700 I  ROLLBACK [rsBackgroundSync] 	start time: 2020-05-08T12:17:26.856-0700
2020-05-08T12:17:26.986-0700 I  ROLLBACK [rsBackgroundSync] 	end time: 2020-05-08T12:17:26.986-0700
2020-05-08T12:17:26.986-0700 I  ROLLBACK [rsBackgroundSync] 	sync source: n1:27019
2020-05-08T12:17:26.986-0700 I  ROLLBACK [rsBackgroundSync] 	rollback data file directory: /var/lib/mongodb/rollback/config.lockpings
2020-05-08T12:17:26.986-0700 I  ROLLBACK [rsBackgroundSync] 	rollback id: 5
2020-05-08T12:17:26.986-0700 I  ROLLBACK [rsBackgroundSync] 	last optime on branch of history rolled back: { ts: Timestamp(1588965441, 26), t: 25 }
2020-05-08T12:17:26.986-0700 I  ROLLBACK [rsBackgroundSync] 	common point optime: { ts: Timestamp(1588965434, 11), t: 23 }
2020-05-08T12:17:26.987-0700 I  ROLLBACK [rsBackgroundSync] 	last wall clock time on the branch of history rolled back: 2020-05-08T12:17:21.364-0700
2020-05-08T12:17:26.987-0700 I  ROLLBACK [rsBackgroundSync] 	wall clock time of the first operation after the common point: 2020-05-08T12:17:14.907-0700
2020-05-08T12:17:26.987-0700 I  ROLLBACK [rsBackgroundSync] 	difference in wall clock times: 6 second(s)
2020-05-08T12:17:26.987-0700 I  ROLLBACK [rsBackgroundSync] 	truncate timestamp: Timestamp(1588965441, 1)
2020-05-08T12:17:26.987-0700 I  ROLLBACK [rsBackgroundSync] 	stable timestamp: Timestamp(1588965434, 11)
2020-05-08T12:17:26.987-0700 I  ROLLBACK [rsBackgroundSync] 	shard identity document rolled back: false
2020-05-08T12:17:26.987-0700 I  ROLLBACK [rsBackgroundSync] 	config server config version document rolled back: false
2020-05-08T12:17:26.987-0700 I  ROLLBACK [rsBackgroundSync] 	affected sessions: none
2020-05-08T12:17:26.987-0700 I  ROLLBACK [rsBackgroundSync] 	affected namespaces: 
2020-05-08T12:17:26.987-0700 I  ROLLBACK [rsBackgroundSync] 		config.lockpings
2020-05-08T12:17:26.987-0700 I  ROLLBACK [rsBackgroundSync] 	counts of interesting commands rolled back: 
2020-05-08T12:17:26.987-0700 I  ROLLBACK [rsBackgroundSync] 		insert: 0
2020-05-08T12:17:26.987-0700 I  ROLLBACK [rsBackgroundSync] 		delete: 0
2020-05-08T12:17:26.987-0700 I  ROLLBACK [rsBackgroundSync] 		update: 6
2020-05-08T12:17:26.987-0700 I  ROLLBACK [rsBackgroundSync] 	total number of entries rolled back (including no-ops): 7
2020-05-08T12:17:26.987-0700 I  ROLLBACK [rsBackgroundSync] transition to SECONDARY
2020-05-08T12:17:26.987-0700 I  REPL     [rsBackgroundSync] transition to SECONDARY from ROLLBACK
2020-05-08T12:17:26.987-0700 I  REPL     [rsBackgroundSync] Resetting sync source to empty, which was n1:27019
2020-05-08T12:17:26.987-0700 I  REPL     [rsBackgroundSync] Rollback successful.
2020-05-08T12:17:26.987-0700 I  REPL     [rsBackgroundSync] Stopping replication producer
2020-05-08T12:17:26.987-0700 I  REPL     [rsBackgroundSync] Resetting last fetched optimes in bgsync
2020-05-08T12:17:26.987-0700 I  REPL     [rsBackgroundSync] sync source candidate: n1:27019
2020-05-08T12:17:26.988-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n1:27019
2020-05-08T12:17:26.991-0700 I  REPL     [replication-1] Choosing new sync source. Our current sync source is not primary and does not have a sync source, so we require that it is ahead of us. Current sync source: n1:27019, my last fetched oplog optime: { ts: Timestamp(1588965444, 5), t: 26 }, latest oplog optime of sync source: { ts: Timestamp(1588965444, 5), t: 26 } (n3:27019 is)
2020-05-08T12:17:26.991-0700 I  REPL     [replication-1] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: n1:27019, OpTime { ts: Timestamp(1588965444, 5), t: 26 }, its sync source index:-1
2020-05-08T12:17:26.992-0700 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source n1:27019 (config version: 1; last applied optime: { ts: Timestamp(1588965444, 5), t: 26 }; sync source index: -1; primary index: 2) is no longer valid
2020-05-08T12:17:26.992-0700 I  REPL     [rsBackgroundSync] Clearing sync source n1:27019 to choose a new one.
2020-05-08T12:17:26.992-0700 I  REPL     [rsBackgroundSync] could not find member to sync from
2020-05-08T12:17:26.998-0700 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to n1:27019: InvalidSyncSource: Sync source was cleared. Was n1:27019
2020-05-08T12:17:27.750-0700 I  ELECTION [conn214] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 27, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965444, 5), t: 26 } }
2020-05-08T12:17:27.751-0700 I  ELECTION [conn214] Sending vote response: { term: 27, voteGranted: true, reason: "" }
2020-05-08T12:17:27.755-0700 I  ELECTION [conn214] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 28, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965444, 5), t: 26 } }
2020-05-08T12:17:27.756-0700 I  ELECTION [conn214] Sending vote response: { term: 28, voteGranted: true, reason: "" }
2020-05-08T12:17:27.992-0700 I  REPL     [replexec-5] Member n3:27019 is now in state RS_DOWN - Request 1219 timed out, deadline was 2020-05-08T12:17:27.992-0700, op was RemoteCommand 1219 -- target:[n3:27019] db:admin expDate:2020-05-08T12:17:27.992-0700 cmd:{ replSetHeartbeat: "rs_config", configVersion: 1, hbv: 1, from: "n2:27019", fromId: 1, term: 27 }
2020-05-08T12:17:27.992-0700 I  CONNPOOL [Replication] Ending connection to host n3:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:17:27.994-0700 I  REPL     [replexec-4] Member n1:27019 is now in state PRIMARY
2020-05-08T12:17:28.493-0700 I  REPL     [replexec-3] Member n3:27019 is now in state SECONDARY
2020-05-08T12:17:28.993-0700 I  REPL     [rsBackgroundSync] sync source candidate: n1:27019
2020-05-08T12:17:28.996-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n1:27019
2020-05-08T12:17:29.006-0700 I  COMMAND  [conn47] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp(1588965434, 11), t: 23 } }, limit: 1, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965448, 10), signature: { hash: BinData(0, 757BE20189694B764CE2F7BC81A0D7963927BD0E), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965434, 11), t: 23 } }, $db: "config" } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:571 locks:{ ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { r: 1 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 2 } }, Mutex: { acquireCount: { r: 1 } } } protocol:op_msg 644ms
2020-05-08T12:17:29.565-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:49694 #227 (46 connections now open)
2020-05-08T12:17:29.566-0700 I  NETWORK  [conn227] received client metadata from 192.168.122.13:49694 conn227: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:29.568-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:49696 #228 (47 connections now open)
2020-05-08T12:17:29.569-0700 I  NETWORK  [conn228] received client metadata from 192.168.122.13:49696 conn228: { driver: { name: "MongoDB Internal Client", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:29.667-0700 I  NETWORK  [conn228] end connection 192.168.122.13:49696 (46 connections now open)
2020-05-08T12:17:32.482-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:58134 #229 (47 connections now open)
2020-05-08T12:17:32.482-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:58140 #230 (48 connections now open)
2020-05-08T12:17:32.483-0700 I  NETWORK  [conn229] received client metadata from 192.168.122.1:58134 conn229: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:32.483-0700 I  NETWORK  [conn230] received client metadata from 192.168.122.1:58140 conn230: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:32.486-0700 I  NETWORK  [conn229] end connection 192.168.122.1:58134 (47 connections now open)
2020-05-08T12:17:32.486-0700 I  NETWORK  [conn230] end connection 192.168.122.1:58140 (46 connections now open)
2020-05-08T12:17:33.121-0700 I  ELECTION [replexec-3] Starting an election, since we've seen no PRIMARY in the past 1000ms
2020-05-08T12:17:33.121-0700 I  ELECTION [replexec-3] conducting a dry run election to see if we could be elected. current term: 28
2020-05-08T12:17:33.121-0700 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 1259 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 28, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965448, 18), t: 28 } }
2020-05-08T12:17:33.121-0700 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 1260 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 28, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965448, 18), t: 28 } }
2020-05-08T12:17:33.122-0700 I  ELECTION [replexec-5] VoteRequester(term 28 dry run) received a yes vote from n3:27019; response message: { term: 28, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000001b') }, lastCommittedOpTime: Timestamp(1588965448, 18), $clusterTime: { clusterTime: Timestamp(1588965452, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965448, 18) }
2020-05-08T12:17:33.122-0700 I  ELECTION [replexec-5] dry election run succeeded, running for election in term 29
2020-05-08T12:17:33.122-0700 I  CONNPOOL [Replication] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:17:33.125-0700 I  REPL     [replexec-5] Scheduling remote command request for vote request: RemoteCommand 1261 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 29, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965448, 18), t: 28 } }
2020-05-08T12:17:33.125-0700 I  REPL     [replexec-5] Scheduling remote command request for vote request: RemoteCommand 1262 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 29, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965448, 18), t: 28 } }
2020-05-08T12:17:33.125-0700 I  CONNPOOL [Replication] Connecting to n1:27019
2020-05-08T12:17:33.128-0700 I  ELECTION [replexec-2] VoteRequester(term 29) received a yes vote from n3:27019; response message: { term: 29, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000001b') }, lastCommittedOpTime: Timestamp(1588965448, 18), $clusterTime: { clusterTime: Timestamp(1588965452, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965448, 18) }
2020-05-08T12:17:33.128-0700 I  ELECTION [replexec-2] election succeeded, assuming primary role in term 29
2020-05-08T12:17:33.128-0700 I  REPL     [replexec-2] transition to PRIMARY from SECONDARY
2020-05-08T12:17:33.128-0700 I  REPL     [replexec-2] Resetting sync source to empty, which was n1:27019
2020-05-08T12:17:33.128-0700 I  CONNPOOL [Replication] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:17:33.128-0700 I  REPL     [replexec-2] Entering primary catch-up mode.
2020-05-08T12:17:33.995-0700 I  REPL     [replexec-2] Member n1:27019 is now in state RS_DOWN - Couldn't get a connection within the time limit
2020-05-08T12:17:33.995-0700 I  REPL     [replexec-2] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1588965448, 18), t: 28 }. My Last Applied: { ts: Timestamp(1588965448, 18), t: 28 }
2020-05-08T12:17:33.995-0700 I  REPL     [replexec-2] Exited primary catch-up mode.
2020-05-08T12:17:33.995-0700 I  REPL     [replexec-2] Stopping replication producer
2020-05-08T12:17:33.995-0700 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 29
2020-05-08T12:17:33.995-0700 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2020-05-08T12:17:33.995-0700 I  CONNPOOL [RS] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:17:33.996-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:17:33.996-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:17:33.996-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 1 }
2020-05-08T12:17:33.998-0700 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-05-08T12:17:33.998-0700 I  SHARDING [Balancer] CSRS balancer is starting
2020-05-08T12:17:33.999-0700 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-05-08T12:17:34.000-0700 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-05-08T12:17:34.000-0700 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-05-08T12:17:34.000-0700 I  NETWORK  [Balancer] Starting new replica set monitor for rs_shard1/n4:27018,n5:27018,n6:27018
2020-05-08T12:17:34.000-0700 I  NETWORK  [Balancer] Starting new replica set monitor for rs_shard2/n7:27018,n8:27018,n9:27018
2020-05-08T12:17:34.002-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:34.002-0700 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs_shard2 is rs_shard2/n7:27018,n8:27018,n9:27018
2020-05-08T12:17:34.005-0700 I  TXN      [TransactionCoordinator] Need to resume coordinating commit for 0 transactions
2020-05-08T12:17:34.005-0700 I  TXN      [TransactionCoordinator] Incoming coordinateCommit requests are now enabled
2020-05-08T12:17:34.232-0700 I  NETWORK  [listener] connection accepted from 192.168.122.19:35154 #231 (47 connections now open)
2020-05-08T12:17:34.232-0700 I  NETWORK  [conn231] received client metadata from 192.168.122.19:35154 conn231: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:34.410-0700 I  NETWORK  [listener] connection accepted from 192.168.122.17:50630 #232 (48 connections now open)
2020-05-08T12:17:34.411-0700 I  NETWORK  [conn232] received client metadata from 192.168.122.17:50630 conn232: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:34.457-0700 I  NETWORK  [listener] connection accepted from 192.168.122.14:54640 #233 (49 connections now open)
2020-05-08T12:17:34.457-0700 I  NETWORK  [conn233] received client metadata from 192.168.122.14:54640 conn233: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:34.502-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:34.732-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:58224 #234 (50 connections now open)
2020-05-08T12:17:34.732-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:58232 #235 (51 connections now open)
2020-05-08T12:17:34.733-0700 I  NETWORK  [conn234] received client metadata from 192.168.122.1:58224 conn234: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:34.733-0700 I  NETWORK  [conn235] received client metadata from 192.168.122.1:58232 conn235: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:34.734-0700 I  NETWORK  [conn234] end connection 192.168.122.1:58224 (50 connections now open)
2020-05-08T12:17:34.734-0700 I  NETWORK  [conn235] end connection 192.168.122.1:58232 (49 connections now open)
2020-05-08T12:17:34.869-0700 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to n1:27019: InvalidSyncSource: Sync source was cleared. Was n1:27019
2020-05-08T12:17:35.001-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:35.501-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:35.505-0700 I  REPL     [replexec-3] Member n3:27019 is now in state RS_DOWN - no response within election timeout period
2020-05-08T12:17:35.505-0700 I  REPL     [replexec-3] can't see a majority of the set, relinquishing primary
2020-05-08T12:17:35.505-0700 I  REPL     [replexec-3] Stepping down from primary in response to heartbeat
2020-05-08T12:17:35.505-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:17:35.505-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:17:35.505-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 0, userOpsRunning: 0 }
2020-05-08T12:17:35.505-0700 I  REPL     [replexec-3] transition to SECONDARY from PRIMARY
2020-05-08T12:17:35.505-0700 I  SHARDING [Balancer] caught exception while doing balance: operation was interrupted
2020-05-08T12:17:35.505-0700 I  SHARDING [Balancer] couldn't create config.actionlog collection: :: caused by :: InterruptedDueToReplStateChange: operation was interrupted
2020-05-08T12:17:35.505-0700 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-05-08T12:17:35.857-0700 I  ELECTION [conn195] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 29, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965453, 11), t: 29 } }
2020-05-08T12:17:35.857-0700 I  ELECTION [conn195] Sending vote response: { term: 29, voteGranted: true, reason: "" }
2020-05-08T12:17:35.857-0700 I  NETWORK  [conn195] end connection 192.168.122.13:48846 (48 connections now open)
2020-05-08T12:17:35.909-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:58366 #236 (49 connections now open)
2020-05-08T12:17:35.909-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:58372 #237 (50 connections now open)
2020-05-08T12:17:35.910-0700 I  NETWORK  [conn236] received client metadata from 192.168.122.1:58366 conn236: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:35.910-0700 I  NETWORK  [conn237] received client metadata from 192.168.122.1:58372 conn237: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:35.913-0700 I  NETWORK  [conn236] end connection 192.168.122.1:58366 (49 connections now open)
2020-05-08T12:17:35.913-0700 I  NETWORK  [conn237] end connection 192.168.122.1:58372 (48 connections now open)
2020-05-08T12:17:36.001-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:36.129-0700 I  CONNPOOL [Replication] Ending connection to host n3:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-05-08T12:17:36.129-0700 I  CONNPOOL [Replication] Connecting to n3:27019
2020-05-08T12:17:36.502-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:36.544-0700 I  ELECTION [replexec-5] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:17:37.001-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:37.501-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:37.596-0700 I  ELECTION [replexec-5] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:17:38.001-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:38.501-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:38.720-0700 I  ELECTION [replexec-0] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:17:39.002-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:39.502-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:39.720-0700 I  ELECTION [replexec-4] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:17:39.946-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:58560 #238 (49 connections now open)
2020-05-08T12:17:39.946-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:58566 #239 (50 connections now open)
2020-05-08T12:17:39.946-0700 I  NETWORK  [conn238] received client metadata from 192.168.122.1:58560 conn238: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:39.946-0700 I  NETWORK  [conn239] received client metadata from 192.168.122.1:58566 conn239: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:39.949-0700 I  NETWORK  [conn238] end connection 192.168.122.1:58560 (49 connections now open)
2020-05-08T12:17:39.949-0700 I  NETWORK  [conn239] end connection 192.168.122.1:58566 (48 connections now open)
2020-05-08T12:17:40.001-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:40.143-0700 I  NETWORK  [conn198] end connection 192.168.122.13:48874 (47 connections now open)
2020-05-08T12:17:40.143-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:50336 #240 (48 connections now open)
2020-05-08T12:17:40.144-0700 I  NETWORK  [conn240] received client metadata from 192.168.122.13:50336 conn240: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:40.496-0700 I  REPL     [replexec-3] Member n1:27019 is now in state SECONDARY
2020-05-08T12:17:40.501-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:40.508-0700 I  REPL     [rsBackgroundSync] sync source candidate: n1:27019
2020-05-08T12:17:40.509-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n1:27019
2020-05-08T12:17:40.510-0700 I  CONNPOOL [RS] Connecting to n1:27019
2020-05-08T12:17:41.001-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:41.014-0700 I  REPL     [replication-0] Choosing new sync source. Our current sync source is not primary and does not have a sync source, so we require that it is ahead of us. Current sync source: n1:27019, my last fetched oplog optime: { ts: Timestamp(1588965456, 1), t: 30 }, latest oplog optime of sync source: { ts: Timestamp(1588965456, 1), t: 30 } (sync source does not know the primary)
2020-05-08T12:17:41.014-0700 I  REPL     [replication-0] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: n1:27019, OpTime { ts: Timestamp(1588965456, 1), t: 30 }, its sync source index:-1
2020-05-08T12:17:41.014-0700 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source n1:27019 (config version: 1; last applied optime: { ts: Timestamp(1588965456, 1), t: 30 }; sync source index: -1; primary index: -1) is no longer valid
2020-05-08T12:17:41.014-0700 I  REPL     [rsBackgroundSync] Clearing sync source n1:27019 to choose a new one.
2020-05-08T12:17:41.014-0700 I  REPL     [rsBackgroundSync] could not find member to sync from
2020-05-08T12:17:41.021-0700 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to n1:27019: InvalidSyncSource: Sync source was cleared. Was n1:27019
2020-05-08T12:17:41.121-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:60686 #244 (49 connections now open)
2020-05-08T12:17:41.121-0700 I  NETWORK  [conn244] received client metadata from 192.168.122.11:60686 conn244: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:41.501-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:41.733-0700 I  NETWORK  [conn214] end connection 192.168.122.11:59754 (48 connections now open)
2020-05-08T12:17:42.001-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:42.050-0700 I  ELECTION [replexec-0] Starting an election, since we've seen no PRIMARY in the past 1000ms
2020-05-08T12:17:42.050-0700 I  ELECTION [replexec-0] conducting a dry run election to see if we could be elected. current term: 31
2020-05-08T12:17:42.050-0700 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 1340 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 31, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965456, 1), t: 30 } }
2020-05-08T12:17:42.050-0700 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 1341 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 31, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965456, 1), t: 30 } }
2020-05-08T12:17:42.051-0700 I  ELECTION [replexec-4] VoteRequester(term 31 dry run) received a yes vote from n1:27019; response message: { term: 31, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000001c') }, lastCommittedOpTime: Timestamp(1588965453, 11), $clusterTime: { clusterTime: Timestamp(1588965461, 46), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965456, 1) }
2020-05-08T12:17:42.051-0700 I  ELECTION [replexec-4] dry election run succeeded, running for election in term 32
2020-05-08T12:17:42.056-0700 I  REPL     [replexec-2] Scheduling remote command request for vote request: RemoteCommand 1342 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 32, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965456, 1), t: 30 } }
2020-05-08T12:17:42.056-0700 I  REPL     [replexec-2] Scheduling remote command request for vote request: RemoteCommand 1343 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 32, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965456, 1), t: 30 } }
2020-05-08T12:17:42.056-0700 I  CONNPOOL [Replication] Connecting to n3:27019
2020-05-08T12:17:42.059-0700 I  ELECTION [replexec-3] VoteRequester(term 32) received a yes vote from n1:27019; response message: { term: 32, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000001c') }, lastCommittedOpTime: Timestamp(1588965453, 11), $clusterTime: { clusterTime: Timestamp(1588965461, 99), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965456, 1) }
2020-05-08T12:17:42.060-0700 I  ELECTION [replexec-3] election succeeded, assuming primary role in term 32
2020-05-08T12:17:42.060-0700 I  REPL     [replexec-3] transition to PRIMARY from SECONDARY
2020-05-08T12:17:42.060-0700 I  CONNPOOL [Replication] Ending connection to host n3:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:17:42.060-0700 I  REPL     [replexec-3] Resetting sync source to empty, which was :27017
2020-05-08T12:17:42.061-0700 I  REPL     [replexec-3] Entering primary catch-up mode.
2020-05-08T12:17:42.501-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:42.551-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:58724 #246 (49 connections now open)
2020-05-08T12:17:42.551-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:58730 #247 (50 connections now open)
2020-05-08T12:17:42.552-0700 I  NETWORK  [conn246] received client metadata from 192.168.122.1:58724 conn246: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:42.552-0700 I  NETWORK  [conn247] received client metadata from 192.168.122.1:58730 conn247: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:42.555-0700 I  NETWORK  [conn246] end connection 192.168.122.1:58724 (49 connections now open)
2020-05-08T12:17:42.555-0700 I  NETWORK  [conn247] end connection 192.168.122.1:58730 (48 connections now open)
2020-05-08T12:17:42.753-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:50092 #248 (49 connections now open)
2020-05-08T12:17:42.753-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:50090 #249 (50 connections now open)
2020-05-08T12:17:42.753-0700 I  NETWORK  [conn249] received client metadata from 192.168.122.13:50090 conn249: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:42.754-0700 I  NETWORK  [conn248] received client metadata from 192.168.122.13:50092 conn248: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:43.001-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:43.061-0700 I  REPL     [replexec-3] Catchup timed out after becoming primary.
2020-05-08T12:17:43.061-0700 I  REPL     [replexec-3] Exited primary catch-up mode.
2020-05-08T12:17:43.061-0700 I  REPL     [replexec-3] Stopping replication producer
2020-05-08T12:17:43.061-0700 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 32
2020-05-08T12:17:43.061-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:17:43.062-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:17:43.062-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 0 }
2020-05-08T12:17:43.064-0700 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-05-08T12:17:43.064-0700 I  SHARDING [Balancer] CSRS balancer is starting
2020-05-08T12:17:43.065-0700 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-05-08T12:17:43.066-0700 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-05-08T12:17:43.066-0700 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-05-08T12:17:43.067-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:43.077-0700 I  NETWORK  [listener] connection accepted from 192.168.122.16:42828 #250 (51 connections now open)
2020-05-08T12:17:43.077-0700 I  NETWORK  [listener] connection accepted from 192.168.122.16:42830 #251 (52 connections now open)
2020-05-08T12:17:43.077-0700 I  NETWORK  [conn250] received client metadata from 192.168.122.16:42828 conn250: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:43.078-0700 I  NETWORK  [conn251] received client metadata from 192.168.122.16:42830 conn251: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:43.079-0700 I  NETWORK  [listener] connection accepted from 192.168.122.16:42832 #252 (53 connections now open)
2020-05-08T12:17:43.079-0700 I  NETWORK  [conn252] received client metadata from 192.168.122.16:42832 conn252: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:43.097-0700 I  ELECTION [conn249] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 32, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965461, 59), t: 31 } }
2020-05-08T12:17:43.097-0700 I  ELECTION [conn249] Sending vote response: { term: 32, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965461, 59), t: 31 }, my last applied OpTime: { ts: Timesta..." }
2020-05-08T12:17:43.100-0700 I  NETWORK  [listener] connection accepted from 192.168.122.18:45740 #253 (54 connections now open)
2020-05-08T12:17:43.100-0700 I  NETWORK  [listener] connection accepted from 192.168.122.12:52614 #254 (55 connections now open)
2020-05-08T12:17:43.100-0700 I  NETWORK  [conn253] received client metadata from 192.168.122.18:45740 conn253: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:43.100-0700 I  NETWORK  [listener] connection accepted from 192.168.122.18:45744 #255 (56 connections now open)
2020-05-08T12:17:43.100-0700 I  NETWORK  [conn254] received client metadata from 192.168.122.12:52614 conn254: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:43.100-0700 I  NETWORK  [listener] connection accepted from 192.168.122.15:39478 #256 (57 connections now open)
2020-05-08T12:17:43.100-0700 I  NETWORK  [conn255] received client metadata from 192.168.122.18:45744 conn255: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:43.101-0700 I  NETWORK  [listener] connection accepted from 192.168.122.15:39480 #257 (58 connections now open)
2020-05-08T12:17:43.101-0700 I  NETWORK  [conn256] received client metadata from 192.168.122.15:39478 conn256: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:43.101-0700 I  NETWORK  [conn257] received client metadata from 192.168.122.15:39480 conn257: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:43.101-0700 I  NETWORK  [listener] connection accepted from 192.168.122.18:45750 #258 (59 connections now open)
2020-05-08T12:17:43.101-0700 I  NETWORK  [listener] connection accepted from 192.168.122.18:45752 #259 (60 connections now open)
2020-05-08T12:17:43.101-0700 I  NETWORK  [conn258] received client metadata from 192.168.122.18:45750 conn258: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:43.102-0700 I  NETWORK  [conn259] received client metadata from 192.168.122.18:45752 conn259: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:43.102-0700 I  NETWORK  [listener] connection accepted from 192.168.122.15:39486 #260 (61 connections now open)
2020-05-08T12:17:43.102-0700 I  NETWORK  [conn260] received client metadata from 192.168.122.15:39486 conn260: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:43.146-0700 I  ELECTION [conn244] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 32, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965461, 59), t: 31 } }
2020-05-08T12:17:43.146-0700 I  ELECTION [conn244] Sending vote response: { term: 32, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965461, 59), t: 31 }, my last applied OpTime: { ts: Timesta..." }
2020-05-08T12:17:43.265-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:60630 #262 (62 connections now open)
2020-05-08T12:17:43.265-0700 I  NETWORK  [conn262] received client metadata from 192.168.122.11:60630 conn262: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:43.501-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:43.606-0700 I  NETWORK  [listener] connection accepted from 192.168.122.12:52656 #263 (63 connections now open)
2020-05-08T12:17:43.606-0700 I  NETWORK  [listener] connection accepted from 192.168.122.19:35752 #264 (64 connections now open)
2020-05-08T12:17:43.607-0700 I  NETWORK  [conn263] received client metadata from 192.168.122.12:52656 conn263: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:43.607-0700 I  NETWORK  [listener] connection accepted from 192.168.122.15:39520 #265 (65 connections now open)
2020-05-08T12:17:43.607-0700 I  NETWORK  [conn264] received client metadata from 192.168.122.19:35752 conn264: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:43.607-0700 I  NETWORK  [conn265] received client metadata from 192.168.122.15:39520 conn265: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:43.624-0700 I  REPL     [replexec-5] Member n1:27019 is now in state RS_DOWN - no response within election timeout period
2020-05-08T12:17:43.624-0700 I  REPL     [replexec-5] can't see a majority of the set, relinquishing primary
2020-05-08T12:17:43.624-0700 I  REPL     [replexec-5] Stepping down from primary in response to heartbeat
2020-05-08T12:17:43.624-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:17:43.624-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:17:43.624-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 28, userOpsRunning: 0 }
2020-05-08T12:17:43.625-0700 W  COMMAND  [conn251] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.625-0700 I  COMMAND  [conn251] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965462, 14), signature: { hash: BinData(0, 5375CCD6A36689B998E36578EDE19985EEFADCE5), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 546ms
2020-05-08T12:17:43.626-0700 W  COMMAND  [conn232] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.626-0700 I  COMMAND  [conn232] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n7:27017" }, u: { $set: { _id: "n7:27017", ping: new Date(1588965459010), up: 115, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965461, 99), signature: { hash: BinData(0, 80D28330E549DF51511CF82BC0240A7A660EF724), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 559ms
2020-05-08T12:17:43.626-0700 W  COMMAND  [conn73] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.626-0700 I  COMMAND  [conn73] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n9:27017" }, u: { $set: { _id: "n9:27017", ping: new Date(1588965459011), up: 115, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965461, 99), signature: { hash: BinData(0, 80D28330E549DF51511CF82BC0240A7A660EF724), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 527ms
2020-05-08T12:17:43.626-0700 W  COMMAND  [conn54] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.627-0700 I  COMMAND  [conn54] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n8:27017:1588965341:-4138072281809841771" }, update: { $set: { ping: new Date(1588965463097) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965462, 169), signature: { hash: BinData(0, 5375CCD6A36689B998E36578EDE19985EEFADCE5), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:746 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 527ms
2020-05-08T12:17:43.627-0700 W  COMMAND  [conn57] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.627-0700 I  COMMAND  [conn57] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n7:27017:1588965341:6871791939861018853" }, update: { $set: { ping: new Date(1588965463075) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965461, 99), signature: { hash: BinData(0, 80D28330E549DF51511CF82BC0240A7A660EF724), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:745 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 551ms
2020-05-08T12:17:43.628-0700 W  COMMAND  [conn205] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.628-0700 I  COMMAND  [conn205] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965461, 99), signature: { hash: BinData(0, 80D28330E549DF51511CF82BC0240A7A660EF724), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 561ms
2020-05-08T12:17:43.628-0700 W  COMMAND  [conn254] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.628-0700 I  COMMAND  [conn254] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n2:27017:1588965341:-7408192429411933944" }, update: { $set: { ping: new Date(1588965463098) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965462, 14), signature: { hash: BinData(0, 5375CCD6A36689B998E36578EDE19985EEFADCE5), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:746 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 527ms
2020-05-08T12:17:43.629-0700 W  COMMAND  [conn255] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.629-0700 I  COMMAND  [conn255] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965462, 169), signature: { hash: BinData(0, 5375CCD6A36689B998E36578EDE19985EEFADCE5), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 527ms
2020-05-08T12:17:43.629-0700 W  COMMAND  [conn258] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.629-0700 W  COMMAND  [conn256] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.629-0700 I  COMMAND  [conn258] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965462, 169), signature: { hash: BinData(0, 5375CCD6A36689B998E36578EDE19985EEFADCE5), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 527ms
2020-05-08T12:17:43.629-0700 I  COMMAND  [conn256] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965462, 167), signature: { hash: BinData(0, 5375CCD6A36689B998E36578EDE19985EEFADCE5), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 528ms
2020-05-08T12:17:43.629-0700 W  COMMAND  [conn32] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.630-0700 I  COMMAND  [conn32] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965462, 167), signature: { hash: BinData(0, 5375CCD6A36689B998E36578EDE19985EEFADCE5), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 530ms
2020-05-08T12:17:43.630-0700 W  COMMAND  [conn253] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.630-0700 W  COMMAND  [conn23] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.630-0700 I  COMMAND  [conn253] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965462, 169), signature: { hash: BinData(0, 5375CCD6A36689B998E36578EDE19985EEFADCE5), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 529ms
2020-05-08T12:17:43.630-0700 W  COMMAND  [conn190] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.630-0700 W  COMMAND  [conn231] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.630-0700 I  COMMAND  [conn23] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n6:27017:1588965341:-562014436095676681" }, update: { $set: { ping: new Date(1588965463075) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965462, 14), signature: { hash: BinData(0, 5375CCD6A36689B998E36578EDE19985EEFADCE5), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:745 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 553ms
2020-05-08T12:17:43.630-0700 I  COMMAND  [conn190] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n3:27017" }, u: { $set: { _id: "n3:27017", ping: new Date(1588965459570), up: 115, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965461, 99), signature: { hash: BinData(0, 80D28330E549DF51511CF82BC0240A7A660EF724), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 563ms
2020-05-08T12:17:43.630-0700 I  COMMAND  [conn231] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n9:27017:1588965341:2106346409928220643" }, update: { $set: { ping: new Date(1588965463097) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965461, 99), signature: { hash: BinData(0, 80D28330E549DF51511CF82BC0240A7A660EF724), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:745 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 531ms
2020-05-08T12:17:43.630-0700 W  COMMAND  [conn252] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.630-0700 W  COMMAND  [conn22] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.630-0700 I  COMMAND  [conn252] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n6:27017" }, u: { $set: { _id: "n6:27017", ping: new Date(1588965459570), up: 115, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965462, 14), signature: { hash: BinData(0, 5375CCD6A36689B998E36578EDE19985EEFADCE5), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 550ms
2020-05-08T12:17:43.631-0700 I  COMMAND  [conn22] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n3:27017:1588965341:-8033712167525645570" }, update: { $set: { ping: new Date(1588965463097) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965461, 99), signature: { hash: BinData(0, 80D28330E549DF51511CF82BC0240A7A660EF724), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:746 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 532ms
2020-05-08T12:17:43.631-0700 W  COMMAND  [conn70] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.631-0700 W  COMMAND  [conn53] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.631-0700 I  COMMAND  [conn70] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n2:27017" }, u: { $set: { _id: "n2:27017", ping: new Date(1588965459011), up: 115, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965462, 14), signature: { hash: BinData(0, 5375CCD6A36689B998E36578EDE19985EEFADCE5), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 531ms
2020-05-08T12:17:43.631-0700 I  COMMAND  [conn53] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n4:27017" }, u: { $set: { _id: "n4:27017", ping: new Date(1588965459570), up: 115, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965462, 12), signature: { hash: BinData(0, 5375CCD6A36689B998E36578EDE19985EEFADCE5), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 556ms
2020-05-08T12:17:43.631-0700 W  COMMAND  [conn78] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.631-0700 W  COMMAND  [conn233] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.631-0700 I  COMMAND  [conn78] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965461, 99), signature: { hash: BinData(0, 80D28330E549DF51511CF82BC0240A7A660EF724), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 564ms
2020-05-08T12:17:43.631-0700 W  COMMAND  [conn250] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.631-0700 I  COMMAND  [conn233] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n4:27017:1588965341:3187934225528271170" }, update: { $set: { ping: new Date(1588965463075) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965462, 12), signature: { hash: BinData(0, 5375CCD6A36689B998E36578EDE19985EEFADCE5), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:745 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 555ms
2020-05-08T12:17:43.631-0700 I  COMMAND  [conn250] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965462, 14), signature: { hash: BinData(0, 5375CCD6A36689B998E36578EDE19985EEFADCE5), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 552ms
2020-05-08T12:17:43.631-0700 W  COMMAND  [conn259] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.631-0700 I  REPL     [replexec-0] transition to SECONDARY from PRIMARY
2020-05-08T12:17:43.631-0700 W  COMMAND  [conn257] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.631-0700 W  COMMAND  [conn260] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:43.632-0700 I  COMMAND  [conn259] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n8:27017" }, u: { $set: { _id: "n8:27017", ping: new Date(1588965459009), up: 115, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965462, 169), signature: { hash: BinData(0, 5375CCD6A36689B998E36578EDE19985EEFADCE5), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 529ms
2020-05-08T12:17:43.632-0700 I  COMMAND  [conn260] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n5:27017:1588965341:-4005854753316312821" }, update: { $set: { ping: new Date(1588965463098) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965462, 167), signature: { hash: BinData(0, 5375CCD6A36689B998E36578EDE19985EEFADCE5), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:746 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 528ms
2020-05-08T12:17:43.632-0700 W  TXN      [TransactionCoordinator] Coordinator recovery failed and coordinateCommit requests will not be allowed :: caused by :: PrimarySteppedDown: Primary stepped down while waiting for replication
2020-05-08T12:17:43.632-0700 I  COMMAND  [conn257] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n5:27017" }, u: { $set: { _id: "n5:27017", ping: new Date(1588965459010), up: 115, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965462, 167), signature: { hash: BinData(0, 5375CCD6A36689B998E36578EDE19985EEFADCE5), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 529ms
2020-05-08T12:17:43.632-0700 I  SHARDING [Balancer] caught exception while doing balance: operation was interrupted
2020-05-08T12:17:43.632-0700 I  SHARDING [Balancer] couldn't create config.actionlog collection: :: caused by :: InterruptedDueToReplStateChange: operation was interrupted
2020-05-08T12:17:43.632-0700 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-05-08T12:17:44.001-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:44.062-0700 I  REPL     [replexec-2] Member n1:27019 is now in state SECONDARY
2020-05-08T12:17:44.076-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:50562 #266 (66 connections now open)
2020-05-08T12:17:44.076-0700 I  NETWORK  [conn266] received client metadata from 192.168.122.13:50562 conn266: { driver: { name: "MongoDB Internal Client", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:44.191-0700 I  NETWORK  [conn266] end connection 192.168.122.13:50562 (65 connections now open)
2020-05-08T12:17:44.217-0700 I  ELECTION [conn262] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 32, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965461, 59), t: 31 } }
2020-05-08T12:17:44.217-0700 I  ELECTION [conn262] Sending vote response: { term: 32, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965461, 59), t: 31 }, my last applied OpTime: { ts: Timesta..." }
2020-05-08T12:17:44.501-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:44.684-0700 I  ELECTION [replexec-0] Starting an election, since we've seen no PRIMARY in the past 1000ms
2020-05-08T12:17:44.684-0700 I  ELECTION [replexec-0] conducting a dry run election to see if we could be elected. current term: 32
2020-05-08T12:17:44.684-0700 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 1367 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 32, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965463, 18), t: 32 } }
2020-05-08T12:17:44.684-0700 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 1368 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 32, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965463, 18), t: 32 } }
2020-05-08T12:17:44.685-0700 I  ELECTION [replexec-6] VoteRequester(term 32 dry run) received a yes vote from n1:27019; response message: { term: 32, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000001c') }, lastCommittedOpTime: Timestamp(1588965453, 11), $clusterTime: { clusterTime: Timestamp(1588965463, 22), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965461, 59) }
2020-05-08T12:17:44.685-0700 I  ELECTION [replexec-6] dry election run succeeded, running for election in term 33
2020-05-08T12:17:44.688-0700 I  REPL     [replexec-6] Scheduling remote command request for vote request: RemoteCommand 1369 -- target:n1:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 33, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965463, 18), t: 32 } }
2020-05-08T12:17:44.688-0700 I  REPL     [replexec-6] Scheduling remote command request for vote request: RemoteCommand 1370 -- target:n3:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 33, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965463, 18), t: 32 } }
2020-05-08T12:17:44.691-0700 I  ELECTION [replexec-0] VoteRequester(term 33) received a yes vote from n1:27019; response message: { term: 33, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000001c') }, lastCommittedOpTime: Timestamp(1588965453, 11), $clusterTime: { clusterTime: Timestamp(1588965463, 22), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1588965461, 59) }
2020-05-08T12:17:44.691-0700 I  ELECTION [replexec-0] election succeeded, assuming primary role in term 33
2020-05-08T12:17:44.691-0700 I  REPL     [replexec-0] transition to PRIMARY from SECONDARY
2020-05-08T12:17:44.691-0700 I  REPL     [replexec-0] Resetting sync source to empty, which was :27017
2020-05-08T12:17:44.692-0700 I  CONNPOOL [Replication] Ending connection to host n3:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-05-08T12:17:44.692-0700 I  REPL     [replexec-0] Entering primary catch-up mode.
2020-05-08T12:17:44.692-0700 I  CONNPOOL [Replication] Connecting to n3:27019
2020-05-08T12:17:44.694-0700 I  REPL     [replexec-0] Member n3:27019 is now in state SECONDARY
2020-05-08T12:17:44.694-0700 I  REPL     [replexec-0] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1588965463, 18), t: 32 }. My Last Applied: { ts: Timestamp(1588965463, 18), t: 32 }
2020-05-08T12:17:44.694-0700 I  REPL     [replexec-0] Exited primary catch-up mode.
2020-05-08T12:17:44.694-0700 I  REPL     [replexec-0] Stopping replication producer
2020-05-08T12:17:44.695-0700 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 33
2020-05-08T12:17:44.695-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:17:44.695-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:17:44.695-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 0 }
2020-05-08T12:17:44.698-0700 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-05-08T12:17:44.698-0700 I  SHARDING [Balancer] CSRS balancer is starting
2020-05-08T12:17:44.698-0700 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-05-08T12:17:44.700-0700 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-05-08T12:17:44.700-0700 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-05-08T12:17:44.701-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:45.001-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:45.276-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:58920 #268 (66 connections now open)
2020-05-08T12:17:45.277-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:58924 #269 (67 connections now open)
2020-05-08T12:17:45.277-0700 I  NETWORK  [conn268] received client metadata from 192.168.122.1:58920 conn268: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:45.277-0700 I  NETWORK  [conn269] received client metadata from 192.168.122.1:58924 conn269: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:45.280-0700 I  NETWORK  [conn268] end connection 192.168.122.1:58920 (66 connections now open)
2020-05-08T12:17:45.280-0700 I  NETWORK  [conn269] end connection 192.168.122.1:58924 (65 connections now open)
2020-05-08T12:17:45.501-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:45.693-0700 I  REPL     [replexec-6] Member n1:27019 is now in state RS_DOWN - no response within election timeout period
2020-05-08T12:17:46.001-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:46.197-0700 I  REPL     [replexec-3] Member n3:27019 is now in state RS_DOWN - no response within election timeout period
2020-05-08T12:17:46.197-0700 I  REPL     [replexec-3] can't see a majority of the set, relinquishing primary
2020-05-08T12:17:46.197-0700 I  REPL     [replexec-3] Stepping down from primary in response to heartbeat
2020-05-08T12:17:46.197-0700 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-05-08T12:17:46.197-0700 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-05-08T12:17:46.197-0700 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 26, userOpsRunning: 0 }
2020-05-08T12:17:46.198-0700 W  COMMAND  [conn231] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.198-0700 I  COMMAND  [conn231] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n9:27017:1588965341:2106346409928220643" }, update: { $set: { ping: new Date(1588965463097) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965463, 18), signature: { hash: BinData(0, 1AF78C4F46F048CBC91EAE7778F119A0BD5F3C5F), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:0 numYields:0 reslen:745 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 1065ms
2020-05-08T12:17:46.198-0700 W  COMMAND  [conn78] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.198-0700 I  COMMAND  [conn78] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n3:27017" }, u: { $set: { _id: "n3:27017", ping: new Date(1588965459570), up: 115, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965464, 3), signature: { hash: BinData(0, F136610D19C9AD9DF7E1C0B15B5C7B6C095149CA), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 1065ms
2020-05-08T12:17:46.198-0700 W  COMMAND  [conn252] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.198-0700 I  COMMAND  [conn252] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n6:27017:1588965341:-562014436095676681" }, update: { $set: { ping: new Date(1588965463075) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965463, 21), signature: { hash: BinData(0, 1AF78C4F46F048CBC91EAE7778F119A0BD5F3C5F), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:0 numYields:0 reslen:745 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 1070ms
2020-05-08T12:17:46.198-0700 W  COMMAND  [conn256] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.198-0700 I  COMMAND  [conn256] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965464, 8), signature: { hash: BinData(0, F136610D19C9AD9DF7E1C0B15B5C7B6C095149CA), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 1066ms
2020-05-08T12:17:46.198-0700 W  COMMAND  [conn251] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.198-0700 I  COMMAND  [conn251] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n6:27017" }, u: { $set: { _id: "n6:27017", ping: new Date(1588965459570), up: 115, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965463, 21), signature: { hash: BinData(0, 1AF78C4F46F048CBC91EAE7778F119A0BD5F3C5F), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 1071ms
2020-05-08T12:17:46.198-0700 W  COMMAND  [conn260] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.198-0700 I  COMMAND  [conn260] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965464, 8), signature: { hash: BinData(0, F136610D19C9AD9DF7E1C0B15B5C7B6C095149CA), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 1066ms
2020-05-08T12:17:46.199-0700 W  COMMAND  [conn32] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.199-0700 I  COMMAND  [conn32] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n5:27017:1588965341:-4005854753316312821" }, update: { $set: { ping: new Date(1588965463098) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965464, 8), signature: { hash: BinData(0, F136610D19C9AD9DF7E1C0B15B5C7B6C095149CA), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:0 numYields:0 reslen:746 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 1066ms
2020-05-08T12:17:46.199-0700 W  COMMAND  [conn254] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.199-0700 I  COMMAND  [conn254] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965463, 21), signature: { hash: BinData(0, 1AF78C4F46F048CBC91EAE7778F119A0BD5F3C5F), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 1069ms
2020-05-08T12:17:46.200-0700 W  COMMAND  [conn130] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.200-0700 I  COMMAND  [conn130] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965464, 4), signature: { hash: BinData(0, F136610D19C9AD9DF7E1C0B15B5C7B6C095149CA), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 1135ms
2020-05-08T12:17:46.200-0700 W  COMMAND  [conn255] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.200-0700 W  COMMAND  [conn23] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.200-0700 I  COMMAND  [conn255] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n8:27017" }, u: { $set: { _id: "n8:27017", ping: new Date(1588965459009), up: 115, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965463, 19), signature: { hash: BinData(0, 1AF78C4F46F048CBC91EAE7778F119A0BD5F3C5F), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 1070ms
2020-05-08T12:17:46.200-0700 I  COMMAND  [conn23] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965463, 21), signature: { hash: BinData(0, 1AF78C4F46F048CBC91EAE7778F119A0BD5F3C5F), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 1072ms
2020-05-08T12:17:46.200-0700 W  COMMAND  [conn22] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.200-0700 I  COMMAND  [conn22] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965464, 3), signature: { hash: BinData(0, F136610D19C9AD9DF7E1C0B15B5C7B6C095149CA), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 1068ms
2020-05-08T12:17:46.201-0700 W  COMMAND  [conn20] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.201-0700 W  COMMAND  [conn253] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.201-0700 W  COMMAND  [conn53] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.201-0700 I  COMMAND  [conn20] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965464, 4), signature: { hash: BinData(0, F136610D19C9AD9DF7E1C0B15B5C7B6C095149CA), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 1136ms
2020-05-08T12:17:46.201-0700 W  COMMAND  [conn250] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.201-0700 I  COMMAND  [conn53] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n4:27017:1588965341:3187934225528271170" }, update: { $set: { ping: new Date(1588965463075) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965464, 4), signature: { hash: BinData(0, F136610D19C9AD9DF7E1C0B15B5C7B6C095149CA), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:0 numYields:0 reslen:745 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 1068ms
2020-05-08T12:17:46.201-0700 I  COMMAND  [conn253] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n8:27017:1588965341:-4138072281809841771" }, update: { $set: { ping: new Date(1588965463097) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965463, 19), signature: { hash: BinData(0, 1AF78C4F46F048CBC91EAE7778F119A0BD5F3C5F), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:0 numYields:0 reslen:746 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 1072ms
2020-05-08T12:17:46.201-0700 I  COMMAND  [conn250] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965463, 21), signature: { hash: BinData(0, 1AF78C4F46F048CBC91EAE7778F119A0BD5F3C5F), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 1073ms
2020-05-08T12:17:46.201-0700 W  COMMAND  [conn257] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.201-0700 W  COMMAND  [conn258] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.201-0700 I  COMMAND  [conn257] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965464, 8), signature: { hash: BinData(0, F136610D19C9AD9DF7E1C0B15B5C7B6C095149CA), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } protocol:op_msg 1069ms
2020-05-08T12:17:46.201-0700 W  COMMAND  [conn233] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.201-0700 I  COMMAND  [conn258] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965463, 19), signature: { hash: BinData(0, 1AF78C4F46F048CBC91EAE7778F119A0BD5F3C5F), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 1072ms
2020-05-08T12:17:46.201-0700 I  COMMAND  [conn233] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965464, 4), signature: { hash: BinData(0, F136610D19C9AD9DF7E1C0B15B5C7B6C095149CA), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 1068ms
2020-05-08T12:17:46.201-0700 W  COMMAND  [conn70] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.201-0700 I  COMMAND  [conn70] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n2:27017:1588965341:-7408192429411933944" }, update: { $set: { ping: new Date(1588965463098) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965463, 21), signature: { hash: BinData(0, 1AF78C4F46F048CBC91EAE7778F119A0BD5F3C5F), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:0 numYields:0 reslen:746 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 1071ms
2020-05-08T12:17:46.202-0700 W  COMMAND  [conn205] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.202-0700 I  COMMAND  [conn205] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n3:27017:1588965341:-8033712167525645570" }, update: { $set: { ping: new Date(1588965463097) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965464, 3), signature: { hash: BinData(0, F136610D19C9AD9DF7E1C0B15B5C7B6C095149CA), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:0 numYields:0 reslen:746 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 1069ms
2020-05-08T12:17:46.202-0700 W  COMMAND  [conn265] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.202-0700 W  COMMAND  [conn190] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.202-0700 W  COMMAND  [conn57] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.202-0700 W  COMMAND  [conn54] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.202-0700 I  COMMAND  [conn190] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965464, 3), signature: { hash: BinData(0, F136610D19C9AD9DF7E1C0B15B5C7B6C095149CA), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 1069ms
2020-05-08T12:17:46.202-0700 I  COMMAND  [conn57] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "n7:27017:1588965341:6871791939861018853" }, update: { $set: { ping: new Date(1588965463075) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965463, 18), signature: { hash: BinData(0, 1AF78C4F46F048CBC91EAE7778F119A0BD5F3C5F), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:0 numYields:0 reslen:745 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 1071ms
2020-05-08T12:17:46.202-0700 I  REPL     [replexec-3] transition to SECONDARY from PRIMARY
2020-05-08T12:17:46.202-0700 I  COMMAND  [conn265] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "n5:27017" }, u: { $set: { _id: "n5:27017", ping: new Date(1588965459010), up: 115, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965464, 8), signature: { hash: BinData(0, F136610D19C9AD9DF7E1C0B15B5C7B6C095149CA), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 1069ms
2020-05-08T12:17:46.202-0700 I  COMMAND  [conn54] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965463, 22), signature: { hash: BinData(0, 1AF78C4F46F048CBC91EAE7778F119A0BD5F3C5F), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 795ms
2020-05-08T12:17:46.202-0700 W  COMMAND  [conn259] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-05-08T12:17:46.202-0700 I  COMMAND  [conn259] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard2" }, u: { $set: { host: "rs_shard2/n7:27018,n8:27018,n9:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1588965463, 19), signature: { hash: BinData(0, 1AF78C4F46F048CBC91EAE7778F119A0BD5F3C5F), keyId: 6824554174072487943 } }, $configServerState: { opTime: { ts: Timestamp(1588965453, 11), t: 29 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 1073ms
2020-05-08T12:17:46.202-0700 I  SHARDING [Balancer] caught exception while doing balance: operation was interrupted
2020-05-08T12:17:46.202-0700 W  TXN      [TransactionCoordinator] Coordinator recovery failed and coordinateCommit requests will not be allowed :: caused by :: TransactionCoordinatorSteppingDown: operation was interrupted
2020-05-08T12:17:46.203-0700 I  SHARDING [Balancer] couldn't create config.actionlog collection: :: caused by :: InterruptedDueToReplStateChange: operation was interrupted
2020-05-08T12:17:46.203-0700 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-05-08T12:17:46.473-0700 I  ELECTION [conn248] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 33, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965463, 18), t: 32 } }
2020-05-08T12:17:46.473-0700 I  ELECTION [conn248] Sending vote response: { term: 33, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965463, 18), t: 32 }, my last applied OpTime: { ts: Timesta..." }
2020-05-08T12:17:46.473-0700 I  NETWORK  [conn248] end connection 192.168.122.13:50092 (64 connections now open)
2020-05-08T12:17:46.501-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:46.529-0700 I  NETWORK  [conn249] end connection 192.168.122.13:50090 (63 connections now open)
2020-05-08T12:17:46.590-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:58970 #270 (64 connections now open)
2020-05-08T12:17:46.590-0700 I  NETWORK  [listener] connection accepted from 192.168.122.1:58976 #271 (65 connections now open)
2020-05-08T12:17:46.590-0700 I  NETWORK  [conn270] received client metadata from 192.168.122.1:58970 conn270: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:46.591-0700 I  NETWORK  [conn271] received client metadata from 192.168.122.1:58976 conn271: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.6.0-1-amd64" }, platform: "Java/Oracle Corporation/1.8.0_40-b25" }
2020-05-08T12:17:46.595-0700 I  NETWORK  [conn270] end connection 192.168.122.1:58970 (64 connections now open)
2020-05-08T12:17:46.595-0700 I  NETWORK  [conn271] end connection 192.168.122.1:58976 (63 connections now open)
2020-05-08T12:17:47.001-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:47.501-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:47.693-0700 I  CONNPOOL [Replication] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-05-08T12:17:47.694-0700 I  CONNPOOL [Replication] Ending connection to host n3:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-05-08T12:17:47.694-0700 I  CONNPOOL [Replication] Connecting to n3:27019
2020-05-08T12:17:47.714-0700 I  ELECTION [replexec-3] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:17:48.001-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:48.501-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:48.783-0700 I  ELECTION [replexec-4] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:17:49.002-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:49.193-0700 I  CONNPOOL [Replication] Ending connection to host n1:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-05-08T12:17:49.193-0700 I  CONNPOOL [Replication] Connecting to n1:27019
2020-05-08T12:17:49.501-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:49.881-0700 I  ELECTION [replexec-0] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:17:50.001-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:50.337-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:33014 #272 (64 connections now open)
2020-05-08T12:17:50.337-0700 I  NETWORK  [conn272] received client metadata from 192.168.122.11:33014 conn272: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:50.501-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:50.565-0700 I  NETWORK  [conn199] end connection 192.168.122.11:59394 (63 connections now open)
2020-05-08T12:17:50.565-0700 I  NETWORK  [listener] connection accepted from 192.168.122.11:33120 #273 (64 connections now open)
2020-05-08T12:17:50.566-0700 I  NETWORK  [conn273] received client metadata from 192.168.122.11:33120 conn273: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:50.658-0700 I  ELECTION [conn272] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 34, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965461, 59), t: 31 } }
2020-05-08T12:17:50.658-0700 I  ELECTION [conn272] Sending vote response: { term: 34, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1588965461, 59), t: 31 }, my last applied OpTime: { ts: Timesta..." }
2020-05-08T12:17:50.972-0700 I  ELECTION [replexec-2] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-05-08T12:17:51.001-0700 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-05-08T12:17:51.194-0700 I  REPL     [replexec-0] Member n3:27019 is now in state SECONDARY
2020-05-08T12:17:51.204-0700 I  REPL     [rsBackgroundSync] sync source candidate: n3:27019
2020-05-08T12:17:51.206-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n3:27019
2020-05-08T12:17:51.207-0700 I  REPL     [rsBackgroundSync] Starting rollback due to OplogStartMissing: Our last optime fetched: { ts: Timestamp(1588965464, 2), t: 33 }. source's GTE: { ts: Timestamp(1588965466, 2), t: 34 }
2020-05-08T12:17:51.208-0700 I  REPL     [rsBackgroundSync] Replication commit point: { ts: Timestamp(1588965453, 11), t: 29 }
2020-05-08T12:17:51.208-0700 I  REPL     [rsBackgroundSync] Rollback using 'recoverToStableTimestamp' method.
2020-05-08T12:17:51.208-0700 I  REPL     [rsBackgroundSync] Scheduling rollback (sync source: n3:27019)
2020-05-08T12:17:51.208-0700 I  ROLLBACK [rsBackgroundSync] transition to ROLLBACK
2020-05-08T12:17:51.208-0700 I  REPL     [rsBackgroundSync] State transition ops metrics: { lastStateTransition: "rollback", userOpsKilled: 0, userOpsRunning: 65 }
2020-05-08T12:17:51.208-0700 I  REPL     [rsBackgroundSync] transition to ROLLBACK from SECONDARY
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 273
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 272
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 265
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 264
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 263
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 262
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 260
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 259
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 258
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 257
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 256
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 255
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 254
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 253
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 252
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 251
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 250
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 244
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 240
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 233
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 232
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 231
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 227
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 205
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 190
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 184
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 148
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 130
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 122
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 114
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 113
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 91
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 78
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 73
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 72
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 71
2020-05-08T12:17:51.208-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 70
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 57
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 54
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 53
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 48
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 47
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 42
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 41
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 40
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 39
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 38
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 37
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 36
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 35
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 34
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 32
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 30
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 29
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 28
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 27
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 26
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 25
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 23
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 22
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 21
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 20
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 18
2020-05-08T12:17:51.209-0700 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 17
2020-05-08T12:17:51.209-0700 I  ROLLBACK [rsBackgroundSync] Waiting for all background operations to complete before starting rollback
2020-05-08T12:17:51.209-0700 I  ROLLBACK [rsBackgroundSync] Finished waiting for background operations to complete before rollback
2020-05-08T12:17:51.209-0700 I  ROLLBACK [rsBackgroundSync] finding common point
2020-05-08T12:17:51.213-0700 I  ROLLBACK [rsBackgroundSync] Rollback common point is { ts: Timestamp(1588965463, 18), t: 32 }
2020-05-08T12:17:51.218-0700 I  REPL     [rsBackgroundSync] Incremented the rollback ID to 6
2020-05-08T12:17:51.219-0700 I  ROLLBACK [rsBackgroundSync] finding record store counts
2020-05-08T12:17:51.219-0700 I  STORAGE  [rsBackgroundSync] closeCatalog: closing all databases
2020-05-08T12:17:51.219-0700 I  STORAGE  [rsBackgroundSync] closeCatalog: closing storage engine catalog
2020-05-08T12:17:51.219-0700 I  STORAGE  [rsBackgroundSync] Deregistering all the collections
2020-05-08T12:17:51.219-0700 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2020-05-08T12:17:51.282-0700 I  ROLLBACK [rsBackgroundSync] Rolling back to the stable timestamp. StableTimestamp: Timestamp(1588965453, 11) Initial Data Timestamp: Timestamp(1588965338, 1)
2020-05-08T12:17:51.283-0700 I  STORAGE  [rsBackgroundSync] openCatalog: loading storage engine catalog
2020-05-08T12:17:51.294-0700 I  STORAGE  [rsBackgroundSync] OplogTruncaterThread local.oplog.rs already started
2020-05-08T12:17:51.294-0700 I  STORAGE  [rsBackgroundSync] The size storer reports that the oplog contains 326 records totaling to 68602 bytes
2020-05-08T12:17:51.294-0700 I  STORAGE  [rsBackgroundSync] Scanning the oplog to determine where to place markers for truncation
2020-05-08T12:17:51.294-0700 I  STORAGE  [rsBackgroundSync] WiredTiger record store oplog processing took 0ms
2020-05-08T12:17:51.298-0700 I  STORAGE  [rsBackgroundSync] openCatalog: reconciling catalog and idents
2020-05-08T12:17:51.298-0700 I  STORAGE  [rsBackgroundSync] openCatalog: reopening all databases
2020-05-08T12:17:51.314-0700 I  STORAGE  [rsBackgroundSync] openCatalog: updating cached oplog pointer
2020-05-08T12:17:51.314-0700 I  STORAGE  [rsBackgroundSync] openCatalog: finished reloading collection catalog
2020-05-08T12:17:51.314-0700 I  STORAGE  [rsBackgroundSync] recoverToStableTimestamp successful. Stable Timestamp: Timestamp(1588965453, 11)
2020-05-08T12:17:51.314-0700 I  ROLLBACK [rsBackgroundSync] Rollback reverted 0 insert operations, 0 update operations and 0 delete operations.
2020-05-08T12:17:51.314-0700 I  ROLLBACK [rsBackgroundSync] Marking to truncate all oplog entries with timestamps greater than or equal to { ts: Timestamp(1588965464, 2), t: 33 }
2020-05-08T12:17:51.315-0700 I  REPL     [rsBackgroundSync] Removing unapplied entries starting at: { : Timestamp(1588965464, 2) }
2020-05-08T12:17:51.315-0700 I  REPL     [rsBackgroundSync] Replication recovery oplog truncation finished in: 0ms
2020-05-08T12:17:51.317-0700 I  REPL     [rsBackgroundSync] Recovering from stable timestamp: Timestamp(1588965453, 11) (top of oplog: { ts: Timestamp(1588965463, 18), t: 32 }, appliedThrough: { ts: Timestamp(1588965453, 11), t: 29 }, TruncateAfter: Timestamp(0, 0))
2020-05-08T12:17:51.317-0700 I  REPL     [rsBackgroundSync] Starting recovery oplog application at the stable timestamp: Timestamp(1588965453, 11)
2020-05-08T12:17:51.317-0700 I  REPL     [rsBackgroundSync] Replaying stored operations from Timestamp(1588965453, 11) (inclusive) to Timestamp(1588965463, 18) (inclusive).
2020-05-08T12:17:51.320-0700 I  REPL     [rsBackgroundSync] Applied 19 operations in 1 batches. Last operation applied with optime: { ts: Timestamp(1588965463, 18), t: 32 }
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] Triggering the rollback op observer
2020-05-08T12:17:51.321-0700 I  REPL     [rsBackgroundSync] Resetting key manager cache
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] Rollback complete
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] Rollback summary:
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 	start time: 2020-05-08T12:17:51.208-0700
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 	end time: 2020-05-08T12:17:51.321-0700
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 	sync source: n3:27019
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 	rollback data file directory: none; no files written
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 	rollback id: 6
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 	last optime on branch of history rolled back: { ts: Timestamp(1588965464, 2), t: 33 }
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 	common point optime: { ts: Timestamp(1588965463, 18), t: 32 }
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 	last wall clock time on the branch of history rolled back: 2020-05-08T12:17:44.696-0700
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 	wall clock time of the first operation after the common point: 2020-05-08T12:17:44.696-0700
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 	difference in wall clock times: 0 second(s)
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 	truncate timestamp: Timestamp(1588965464, 2)
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 	stable timestamp: Timestamp(1588965453, 11)
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 	shard identity document rolled back: false
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 	config server config version document rolled back: false
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 	affected sessions: none
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 	affected namespaces: none
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 	counts of interesting commands rolled back: 
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 		update: 0
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 		delete: 0
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 		insert: 0
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] 	total number of entries rolled back (including no-ops): 1
2020-05-08T12:17:51.321-0700 I  ROLLBACK [rsBackgroundSync] transition to SECONDARY
2020-05-08T12:17:51.321-0700 I  REPL     [rsBackgroundSync] transition to SECONDARY from ROLLBACK
2020-05-08T12:17:51.321-0700 I  REPL     [rsBackgroundSync] Resetting sync source to empty, which was n3:27019
2020-05-08T12:17:51.321-0700 I  REPL     [rsBackgroundSync] Rollback successful.
2020-05-08T12:17:51.321-0700 I  REPL     [rsBackgroundSync] Stopping replication producer
2020-05-08T12:17:51.321-0700 I  REPL     [rsBackgroundSync] Resetting last fetched optimes in bgsync
2020-05-08T12:17:51.321-0700 I  REPL     [rsBackgroundSync] sync source candidate: n3:27019
2020-05-08T12:17:51.322-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n3:27019
2020-05-08T12:17:51.325-0700 I  REPL     [replication-0] Choosing new sync source. Our current sync source is not primary and does not have a sync source, so we require that it is ahead of us. Current sync source: n3:27019, my last fetched oplog optime: { ts: Timestamp(1588965466, 2), t: 34 }, latest oplog optime of sync source: { ts: Timestamp(1588965466, 2), t: 34 } (sync source does not know the primary)
2020-05-08T12:17:51.325-0700 I  REPL     [replication-0] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: n3:27019, OpTime { ts: Timestamp(1588965466, 2), t: 34 }, its sync source index:-1
2020-05-08T12:17:51.325-0700 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source n3:27019 (config version: 1; last applied optime: { ts: Timestamp(1588965466, 2), t: 34 }; sync source index: -1; primary index: -1) is no longer valid
2020-05-08T12:17:51.325-0700 I  REPL     [rsBackgroundSync] Clearing sync source n3:27019 to choose a new one.
2020-05-08T12:17:51.325-0700 I  REPL     [rsBackgroundSync] could not find member to sync from
2020-05-08T12:17:51.325-0700 I  CONNPOOL [Replication] Connecting to n1:27019
2020-05-08T12:17:51.328-0700 I  REPL     [replexec-4] Member n1:27019 is now in state SECONDARY
2020-05-08T12:17:51.328-0700 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to n3:27019: InvalidSyncSource: Sync source was cleared. Was n3:27019
2020-05-08T12:17:51.910-0700 I  NETWORK  [conn240] end connection 192.168.122.13:50336 (63 connections now open)
2020-05-08T12:17:51.911-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:50860 #277 (64 connections now open)
2020-05-08T12:17:51.911-0700 I  NETWORK  [conn277] received client metadata from 192.168.122.13:50860 conn277: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:51.961-0700 I  ELECTION [conn272] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 34, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965466, 2), t: 34 } }
2020-05-08T12:17:51.961-0700 I  ELECTION [conn272] Sending vote response: { term: 34, voteGranted: true, reason: "" }
2020-05-08T12:17:51.965-0700 I  ELECTION [conn272] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 35, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965466, 2), t: 34 } }
2020-05-08T12:17:51.966-0700 I  ELECTION [conn272] Sending vote response: { term: 35, voteGranted: true, reason: "" }
2020-05-08T12:17:52.328-0700 I  REPL     [replexec-3] Member n1:27019 is now in state PRIMARY
2020-05-08T12:17:52.481-0700 I  ELECTION [conn262] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 33, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1588965461, 59), t: 31 } }
2020-05-08T12:17:52.481-0700 I  ELECTION [conn262] Sending vote response: { term: 35, voteGranted: false, reason: "candidate's term (33) is lower than mine (35)" }
2020-05-08T12:17:52.481-0700 I  NETWORK  [conn262] end connection 192.168.122.11:60630 (63 connections now open)
2020-05-08T12:17:53.249-0700 I  NETWORK  [conn184] end connection 192.168.122.13:48360 (62 connections now open)
2020-05-08T12:17:53.249-0700 I  NETWORK  [conn244] end connection 192.168.122.11:60686 (61 connections now open)
2020-05-08T12:17:53.326-0700 I  REPL     [rsBackgroundSync] sync source candidate: n1:27019
2020-05-08T12:17:53.328-0700 I  REPL     [rsBackgroundSync] Changed sync source from empty to n1:27019
2020-05-08T12:17:53.505-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:50700 #279 (62 connections now open)
2020-05-08T12:17:53.505-0700 I  NETWORK  [listener] connection accepted from 192.168.122.13:50698 #280 (63 connections now open)
2020-05-08T12:17:53.505-0700 I  NETWORK  [conn279] received client metadata from 192.168.122.13:50700 conn279: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
2020-05-08T12:17:53.505-0700 I  NETWORK  [conn280] received client metadata from 192.168.122.13:50698 conn280: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "PRETTY_NAME="Debian GNU/Linux 9 (stretch)"", architecture: "x86_64", version: "Kernel 5.6.0-1-amd64" } }
